{
  "papers": [
    {
      "id": "6159a7d8-e767-49d6-a740-4fab9a545b57",
      "title": "FuXi-\u03b2: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model",
      "authors": [
        "Yufei Ye",
        "Wei Guo",
        "Hao Wang",
        "Hong Zhu",
        "Yuyang Ye",
        "Yong Liu",
        "Huifeng Guo",
        "Ruiming Tang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "Scaling laws for autoregressive generative recommenders reveal potential for larger, more versatile systems but mean greater latency and training costs. To accelerate training and inference, we investigated the recent generative recommendation models HSTU and FuXi-$\u03b1$, identifying two efficiency bottlenecks: the indexing operations in relative temporal attention bias and the computation of the query-key attention map. Additionally, we observed that relative attention bias in self-attention mechanisms can also serve as attention maps. Previous works like Synthesizer have shown that alternative forms of attention maps can achieve similar performance, naturally raising the question of whether some attention maps are redundant. Through empirical experiments, we discovered that using the query-key attention map might degrade the model's performance in recommendation tasks. To address these bottlenecks, we propose a new framework applicable to Transformer-like recommendation models. On one hand, we introduce Functional Relative Attention Bias, which avoids the time-consuming operations of the original relative attention bias, thereby accelerating the process. On the other hand, we remove the query-key attention map from the original self-attention layer and design a new Attention-Free Token Mixer module. Furthermore, by applying this framework to FuXi-$\u03b1$, we introduce a new model, FuXi-$\u03b2$. Experiments across multiple datasets demonstrate that FuXi-$\u03b2$ outperforms previous state-of-the-art models and achieves significant acceleration compared to FuXi-$\u03b1$, while also adhering to the scaling law. Notably, FuXi-$\u03b2$ shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets compared to FuXi-$\u03b1$. Our code is available in a public repository: https://github.com/USTC-StarTeam/FuXi-beta",
      "year": 2025,
      "arxiv_id": "2508.10615",
      "arxiv_url": "https://arxiv.org/abs/2508.10615",
      "category": "recsys",
      "tags": [
        {
          "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
          "name": "Industrial"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-08-14",
      "created_at": "2026-01-29T18:16:45.255432",
      "updated_at": "2026-01-29T18:16:45.255432",
      "conference": null
    },
    {
      "id": "0bc487da-337c-4aaa-a377-55b82880620a",
      "title": "FuXi-$\u03b1$: Scaling Recommendation Model with Feature Interaction Enhanced Transformer",
      "authors": [
        "Yufei Ye",
        "Wei Guo",
        "Jin Yao Chin",
        "Hao Wang",
        "Hong Zhu",
        "Xi Lin",
        "Yuyang Ye",
        "Yong Liu",
        "Ruiming Tang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "Inspired by scaling laws and large language models, research on large-scale recommendation models has gained significant attention. Recent advancements have shown that expanding sequential recommendation models to large-scale recommendation models can be an effective strategy. Current state-of-the-art sequential recommendation models primarily use self-attention mechanisms for explicit feature interactions among items, while implicit interactions are managed through Feed-Forward Networks (FFNs). However, these models often inadequately integrate temporal and positional information, either by adding them to attention weights or by blending them with latent representations, which limits their expressive power. A recent model, HSTU, further reduces the focus on implicit feature interactions, constraining its performance. We propose a new model called FuXi-$\u03b1$ to address these issues. This model introduces an Adaptive Multi-channel Self-attention mechanism that distinctly models temporal, positional, and semantic features, along with a Multi-stage FFN to enhance implicit feature interactions. Our offline experiments demonstrate that our model outperforms existing models, with its performance continuously improving as the model size increases. Additionally, we conducted an online A/B test within the Huawei Music app, which showed a $4.76\\%$ increase in the average number of songs played per user and a $5.10\\%$ increase in the average listening duration per user. Our code has been released at https://github.com/USTC-StarTeam/FuXi-alpha.",
      "year": 2025,
      "arxiv_id": "2502.03036",
      "arxiv_url": "https://arxiv.org/abs/2502.03036",
      "category": "recsys",
      "tags": [
        {
          "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
          "name": "Industrial"
        },
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-02-05",
      "created_at": "2026-01-29T18:17:13.221185",
      "updated_at": "2026-01-30T15:37:02.245625",
      "conference": "WWW'25",
      "summary": {
        "one_line": "FuXi-$\u03b1$ introduces an adaptive multi-channel self-attention mechanism to effectively scale recommendation models by integrating temporal, positional, and semantic features, leading to improved performance.",
        "contribution": "This paper proposes FuXi-$\u03b1$, a novel recommendation model that distinguishes between temporal, positional, and semantic features through an adaptive multi-channel self-attention mechanism. Furthermore, it utilizes a multi-stage FFN to enhance implicit feature interactions, addressing limitations in existing sequential recommendation models.",
        "methodology": "The model employs an adaptive multi-channel self-attention mechanism to capture diverse feature interactions, allowing for independent modeling of temporal, positional, and semantic information. A multi-stage FFN is integrated to refine implicit feature interactions, improving the model's ability to capture nuanced relationships between items.",
        "results": "Offline experiments demonstrate FuXi-$\u03b1$'s superior performance across increasing model sizes, while an online A/B test within Huawei Music resulted in significant improvements: a 4.76% increase in songs played and a 5.10% increase in listening duration."
      },
      "full_translation": [
        {
          "name": "Abstract",
          "original": "FuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction\nEnhanced Transformer\nYufei Ye\u2217\naboluo2003@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, China\nWei Guo\u2217\nguowei67@huawei.com\nHuawei Noah\u2019s Ark Lab\nSingapore, Singapore\nJin Yao Chin\nchin.jin.yao@huawei.com\nHuawei Noah\u2019s Ark Lab\nSingapore, Singapore\nHao Wang\u2020\nwanghao3@ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, China\nHong Zhu\nzhuhong8@huawei.com\nConsumer Business Group, Huawei\nShenzhen, China\nXi Lin\nlinxi16@huawei.com\nConsumer Business Group, Huawei\nShenzhen, China\nYuyang Ye\nyeyuyang@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, China\nYong Liu\nliu.yong6@huawei.com\nHuawei Noah\u2019s Ark Lab\nSingapore, Singapore\nRuiming Tang\ntangruiming@huawei.com\nHuawei Noah\u2019s Ark Lab\nShenzhen, China\nDefu Lian\u2020\nliandefu@ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, China\nEnhong Chen\ncheneh@ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, China",
          "translated": "FuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction\nEnhanced Transformer\n\nWe propose FuXi-\ud835\udefc, a novel recommendation model that scales effectively by incorporating feature interaction.  Our approach leverages an Enhanced Transformer architecture, which allows us to capture complex relationships between items and users. Specifically, we introduce a new attention mechanism that explicitly models feature interactions, leading to improved recommendation performance.  We evaluate FuXi-\ud835\udefc on several benchmark datasets and demonstrate significant gains compared to state-of-the-art models.  The key innovation lies in our ability to scale the model efficiently while maintaining high accuracy.  We further analyze the model\u2019s behavior and provide insights into the importance of feature interaction for recommendation systems.  Our results highlight the potential of FuXi-\ud835\udefc as a robust and scalable solution for real-world recommendation applications.\n\nThe core of our model is based on the Enhanced Transformer, a powerful architecture designed for sequence modeling.  This architecture utilizes self-attention mechanisms to learn contextual representations of input sequences.  We extend this architecture by adding a new interaction module that explicitly captures feature correlations.  This module is integrated into the attention mechanism, allowing the model to attend to relevant features based on their interaction strength.\n\nMathematically, the attention weights are calculated as follows:\n\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n\nwhere $Q$, $K$, and $V$ represent the query, key, and value matrices, respectively, and $d_k$ is the dimension of the key vectors.\n\nWe also explore different scaling strategies to optimize the model\u2019s performance.  These strategies include adjusting the model size and the number of attention heads.  Our experiments show that scaling the model size linearly with the dataset size leads to a significant improvement in recommendation accuracy."
        },
        {
          "name": "Abstract",
          "original": "Inspired by scaling laws and large language models, research on\nlarge-scale recommendation models has gained significant atten-\ntion. Recent advancements have shown that expanding sequential\nrecommendation models to large-scale recommendation models can\nbe an effective strategy. Current state-of-the-art sequential recom-\nmendation models primarily use self-attention mechanisms for ex-\nplicit feature interactions among items, while implicit interactions\nare managed through Feed-Forward Networks (FFNs). However,\nthese models often inadequately integrate temporal and positional\ninformation, either by adding them to attention weights or by blend-\ning them with latent representations, which limits their expressive\npower. A recent model, HSTU, further reduces the focus on implicit\nfeature interactions, constraining its performance. We propose a\nnew model called FuXi-\ud835\udefc to address these issues. This model intro-\nduces an Adaptive Multi-channel Self-attention mechanism that\ndistinctly models temporal, positional, and semantic features, along\nwith a Multi-stage FFN to enhance implicit feature interactions.\nOur offline experiments demonstrate that our model outperforms\nexisting models, with its performance continuously improving as\nthe model size increases. Additionally, we conducted an online A/B\ntest within the Huawei Music app, which showed a 4.76% increase\nin the average number of songs played per user and a 5.10% in-\ncrease in the average listening duration per user. Our code has been\nreleased at https://github.com/USTC-StarTeam/FuXi-alpha.\n\u2217Both authors contributed equally to this research.\n\u2020Corresponding authors.",
          "translated": "\uc601\ud5a5\uc744 \ubc1b\ub294 Scaling Laws \ubc0f \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378 \uc5f0\uad6c\uc5d0 \ud798\uc785\uc5b4 \ub300\uaddc\ubaa8 \ucd94\ucc9c \ubaa8\ub378\uc5d0 \ub300\ud55c \uc5f0\uad6c\uac00 \uc0c1\ub2f9\ud55c \uad00\uc2ec\uc744 \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \ucd5c\uadfc\uc758 \ubc1c\uc804\uc740 \uc21c\ucc28 \ucd94\ucc9c \ubaa8\ub378\uc744 \ub300\uaddc\ubaa8 \ucd94\ucc9c \ubaa8\ub378\ub85c \ud655\uc7a5\ud558\ub294 \uac83\uc774 \ud6a8\uacfc\uc801\uc778 \uc804\ub7b5\uc784\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \ud604\uc7ac \ucd5c\ucca8\ub2e8 \uc21c\ucc28 \ucd94\ucc9c \ubaa8\ub378\uc740 \uc8fc\ub85c \ud56d\ubaa9 \uac04 \uba85\uc2dc\uc801 \ud2b9\uc9d5 \uc0c1\ud638 \uc791\uc6a9\uc744 \uc704\ud574 Self-Attention \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uba70, Implicit \uc0c1\ud638 \uc791\uc6a9\uc740 Feed-Forward Networks (FFNs)\ub97c \ud1b5\ud574 \uad00\ub9ac\ub429\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc774\ub7ec\ud55c \ubaa8\ub378\uc740 \uc8fc\uc758 \uac00\uc911\uce58\uc5d0 \ucd94\uac00\ud558\uac70\ub098 \uc7a0\uc7ac \ud45c\ud604\uacfc \ud63c\ud569\ud558\uc5ec \uc2dc\uac04\uc801 \ubc0f \uc704\uce58\uc801 \uc815\ubcf4\ub97c \uc81c\ub300\ub85c \ud1b5\ud569\ud558\uc9c0 \ubabb\ud558\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc544 \ud45c\ud604\ub825\uc774 \uc81c\ud55c\ub429\ub2c8\ub2e4. \ucd5c\uadfc HSTU \ubaa8\ub378\uc740 Implicit \ud2b9\uc9d5 \uc0c1\ud638 \uc791\uc6a9\uc5d0 \ub300\ud55c \uc9d1\uc911\uc744 \ub354\uc6b1 \uc904\uc5ec \uc131\ub2a5\uc744 \uc81c\ud55c\ud588\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \ubb38\uc81c\uc810\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 Adaptive Multi-channel Self-attention \uba54\ucee4\ub2c8\uc998\uacfc Multi-stage FFN\uc744 \ub3c4\uc785\ud55c \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc778 FuXi-\ud835\udefc\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4.  Adaptive Multi-channel Self-attention \uba54\ucee4\ub2c8\uc998\uc740 \uc2dc\uac04\uc801, \uc704\uce58\uc801, \uc758\ubbf8\uc801 \ud2b9\uc9d5\uc744 \uba85\ud655\ud558\uac8c \ubaa8\ub378\ub9c1\ud558\uc5ec Implicit \ud2b9\uc9d5 \uc0c1\ud638 \uc791\uc6a9\uc744 \uac15\ud654\ud569\ub2c8\ub2e4.  \uc624\ud504\ub77c\uc778 \uc2e4\ud5d8 \uacb0\uacfc, \ubaa8\ub378 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c \ud5a5\uc0c1\ub418\uc5b4 \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4. \ub610\ud55c, Huawei Music \uc571 \ub0b4\uc5d0\uc11c \uc628\ub77c\uc778 A/B \ud14c\uc2a4\ud2b8\ub97c \uc218\ud589\ud558\uc5ec \uc0ac\uc6a9\uc790\ub2f9 \ud3c9\uade0 \uc7ac\uc0dd\ud558\ub294 \ub178\ub798 \uc218\uc5d0\uc11c 4.76% \uc99d\uac00\uc640 \uc0ac\uc6a9\uc790\ub2f9 \ud3c9\uade0 \uccad\ucde8 \uc2dc\uac04\uc5d0\uc11c 5.10% \uc99d\uac00\ub97c \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4.  \ucf54\ub4dc\ub294 https://github.com/USTC-StarTeam/FuXi-alpha \uc5d0\uc11c \uacf5\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4."
        },
        {
          "name": "Introduction",
          "original": "Recent advancements [1, 3, 14, 24] in scaling laws have revealed that\nthe performance of Large Language Models (LLMs) systematically\nimproves predictably as the number of model parameters, the vol-\nume of training data, and computational resources increase. These\nfindings are crucial as they provide researchers and practitioners\nwith a framework for efficiently allocating limited computational\nresources to optimize model performance. Building on this founda-\ntion, we propose to investigate whether recommendation models\nalso conform to scaling laws. By identifying such models, scaling\nlaws can be utilized to guide the training of larger models, thus\nenhancing their performance.\nBesides the scaling laws found in LLMs such as GPTs [1, 3], LLa-\nMAs [10, 53], autoregressive sequential models have been shown to\nadhere to scaling laws across various domains, including generative\nimage modeling, video modeling, etc [19]. The expansion of Vision\nTransformers (ViT) has also achieved significant success in the field\nof computer vision [9, 73]. This revolutionary innovation has also\nbeen extended to recommendation models. Recent studies [47, 76]\ndemonstrate that autoregressive sequence recommendation models\nalso follow these scaling laws. The success of projects like HSTU\n[4, 60, 71] indicates that scaling up sequential recommendation\nmodels in accordance with these laws is an effective strategy for\ndeveloping large-scale recommendation systems.\nSequential recommendation models have been a focal point of\nresearch in the field of recommender systems, characterized by a\nwide array of architectural innovations [36, 48, 55, 57, 62, 63, 68].\n1\narXiv:2502.03036v1  [cs.IR]  5 Feb 2025\n\nWWW \u201925, 28 April - 2 May, 2025, Sydney, Australia Ye and Guo, et al.\nInitially, pooling operations were employed to manage interaction\nsequences [8]. With the development of deep learning, more sophis-\nticated models emerged, including CNN-based architectures such\nas Caser [52], GNN-based models like SR-GNN [ 61], RNN-based\nframeworks like GRU4Rec [ 20]. Inspired by the huge success of\nTransformers in NLP, models based on self-attention mechanisms\nwere proposed, leading to notable architectures such as SASRec\n[23] and Bert4Rec [51].\nBesides sequential recommendation models, traditional Deep\nLearning Recommendation Models (DLRMs), such as DCN [ 58]\nand xDeepFM [33], also play a crucial role in recommender sys-\ntems. A fundamental concept in these DLRMs is feature interaction,\nwhich is pivotal for enhancing model performance. Feature interac-\ntions are categorized into two types: explicit and implicit. Explicit\ninteractions model feature relationships directly through various\noperators, such as the dot product [43, 58], bilinear functions [33],\nand attention mechanisms [50]. Conversely, implicit interactions\nare facilitated by applying deep neural networks (DNNs). Although\nsuch an approach lacks interpretability, it is extensively used in\nstate-of-the-art DLRMs such as DCN [58], DCNv2 [59], DeepFM\n[13], and PNN [40]. In fact, the integrated DNNs in such models\nare a key driver of their superior performance. However, previous\nstudies [2, 15] have indicated that DLRMs do not necessarily exhibit\nsignificant performance improvements with increased model size.\nNonetheless, the concept of feature interaction can still guide us in\ndesigning models.\nFrom the perspective of feature interaction, sequential recom-\nmendation models can be conceptualized as exploring the interplay\nbetween various features over time. Pooling methods [8] have lim-\nited expressive capabilities because they overlook the semantic\nrichness of interaction sequences. CNN-based methods [ 52] are\nconstrained by a fixed window size, limiting their ability to capture\nlong-range dependencies. RNN-based models [20] interact directly\nwith the previous timestep\u2019s hidden state, which can restrict their\ncapacity to model complex interactions. GNN-based approaches\n[61] limit feature interactions to directly connected items, thereby\nnarrowing their scope. In contrast, attention-based models, includ-\ning SASRec [23], BERT4Rec [51], TiSASRec [29], and HSTU [72],\nenable comprehensive item interactions. Consequently, these mod-\nels are more effective at capturing dynamic user interests through\ninteraction sequences. TiSASRec [29] further improves on SASRec\nby incorporating time intervals and relative position information,\nenhancing its performance. HSTU [72] advances this by utilizing\npositional and temporal information alongside element-wise mul-\ntiplication to model explicit interactions between items, thereby\ndemonstrating superiority over its predecessors.\nDespite the significant advancements made in the aforemen-\ntioned work, there remain several shortcomings that need to be\naddressed. Firstly, previous studies fail to fully leverage temporal\nand positional information in explicit interactions. They integrate\nthis information by simply adding embeddings to input sequences\n[23], incorporating them into the query and key matrices used in\nself-attention layers [29], or adjusting attention weights [72]. Com-\npared to various methods that facilitate feature interactions, this\nsimple addition lacks expressive capacity. Understanding positional\nand temporal information is crucial for sequential recommendation\nbecause different cues can lead to varying results, as illustrated\nItem sequence 2\nItem sequence 3\ntime\nItem sequence 1\ntime\nFigure 1: Different temporal intervals or orders between ob-\njects may lead to varying subsequent interacted items.\nin Figure 1. However, existing models have limited feature inter-\naction with temporal and positional information, hence severely\nrestricting their ability to effectively convey the corresponding\ntemporal and positional cues. Secondly, while HSTU emphasizes\nexplicit interactions, it underemphasizes implicit feature interac-\ntions, potentially leading to a loss of nuanced learning processes\npost-interaction and thus constraining the model\u2019s expressiveness.\nTo address the aforementioned challenges, we propose a novel\nattention-based model named FuXi-\ud835\udefc. Our approach introduces\nan Adaptive Multi-channel Self-attention (AMS) layer, which re-\nsolves the issue of insufficient feature interactions by modeling the\ntemporal and positional information separately. Furthermore, we\nintegrate a multi-stage feedforward neural network (MFFN) layer to\nfacilitate implicit feature interactions, thereby boosting the model\u2019s\nexpressiveness. The proposed method outperforms state-of-the-art\nsequential recommendation techniques across several benchmark\ndatasets. We also evaluate the model\u2019s adherence to scaling laws\nusing a large-scale industrial dataset. The results indicate that per-\nformance consistently improves with increased model complexity,\nhighlighting its potential for large-scale recommendation systems.\nOur contributions are summarized as follows:\n\u2022 We propose a novel model, FuXi-\ud835\udefc, which adheres to the scaling\nlaw by leveraging the perspective of feature interactions.\n\u2022 We design an Adaptive Multi-channel Self-attention (AMS) layer\nthat disentangles the modeling of temporal, positional, and se-\nmantic information. We demonstrate that it permits a more ex-\npressive representation of temporal and positional information.\nAdditionally, we introduce a Multi-stage Feedforward Network\n(MFFN) to enhance implicit feature interactions.\n\u2022 We conducted extensive experiments on multiple real-world\ndatasets and online A/B tests on Huawei Music, demonstrating\nour model\u2019s strong performance. Specifically, the online deploy-\nment led to an increase of 4.76% in the average number of song\nplays per user and a 5.10% enhancement in the average duration\nof song playback per user.",
          "translated": "**\uc11c\ub860**\n\n\ucd5c\uadfc \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59 [1, 3, 14, 24]\uc758 \ubc1c\uc804\uc73c\ub85c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \uc131\ub2a5\uc774 \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130 \uc218, \ud559\uc2b5 \ub370\uc774\ud130\uc758 \uc591, \uadf8\ub9ac\uace0 \ucef4\ud4e8\ud305 \uc790\uc6d0 \uc99d\uac00\uc5d0 \ub530\ub77c \uc608\uce21 \uac00\ub2a5\ud558\uac8c \uc77c\uad00\uc131 \uc788\uac8c \ud5a5\uc0c1\ub428\uc774 \ubc1d\ud600\uc84c\ub2e4. \uc774\ub7ec\ud55c \ubc1c\uacac\uc740 \uc5f0\uad6c\uc790\uc640 \uc2e4\ubb34\uc790\uc5d0\uac8c \uc81c\ud55c\ub41c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ubc30\ubd84\ud558\uc5ec \ubaa8\ub378 \uc131\ub2a5\uc744 \ucd5c\uc801\ud654\ud558\ub294 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uacf5\ud568\uc73c\ub85c\uc368 \ub9e4\uc6b0 \uc911\uc694\ud558\ub2e4. \uc774\ub7ec\ud55c \uae30\ucd08\ub97c \ubc14\ud0d5\uc73c\ub85c, \ucd94\ucc9c \ubaa8\ub378 \ub610\ud55c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc744 \ub530\ub974\ub294\uc9c0 \uc870\uc0ac\ud558\uace0\uc790 \ud55c\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc744 \uc2dd\ubcc4\ud558\uba74, \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc744 \ud65c\uc6a9\ud558\uc5ec \ub354 \ud070 \ubaa8\ub378\uc758 \ud6c8\ub828\uc744 \uc548\ub0b4\ud558\uace0, \uadf8 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\ub2e4.\n\nGPT [1, 3], LLaMA [10, 53]\uc640 \uac19\uc740 LLM\uc758 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59 \uc678\uc5d0\ub3c4, \uc0dd\uc131 \uc774\ubbf8\uc9c0 \ubaa8\ub378\ub9c1, \ube44\ub514\uc624 \ubaa8\ub378\ub9c1 \ub4f1 \ub2e4\uc591\ud55c \ubd84\uc57c\uc5d0\uc11c \uc790\uae30 \ud68c\uadc0 \uc2dc\ud000\uc15c \ubaa8\ub378\uc774 \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc744 \ub530\ub974\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\ub2e4 [19]. \ucef4\ud4e8\ud130 \ube44\uc804 \ubd84\uc57c\uc5d0\uc11c \ud68d\uae30\uc801\uc778 \uc131\uacf5\uc744 \uac70\ub454 \ube44\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38(ViT)\uc758 \ud655\uc7a5 \ub610\ud55c \uc8fc\ubaa9\ud560 \ub9cc\ud558\ub2e4 [9, 73]. \uc774\ub7ec\ud55c \ud601\uc2e0\uc801\uc778 \uae30\uc220 \ub610\ud55c \ucd94\ucc9c \ubaa8\ub378\uc5d0 \uc801\uc6a9\ub418\uc5c8\ub2e4. \ucd5c\uadfc \uc5f0\uad6c [47, 76]\ub294 \uc790\uae30 \ud68c\uadc0 \uc2dc\ud000\uc2a4 \ucd94\ucc9c \ubaa8\ub378 \ub610\ud55c \uc774\ub7ec\ud55c \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc744 \ub530\ub978\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. HSTU [4, 60, 71]\uc640 \uac19\uc740 \ud504\ub85c\uc81d\ud2b8\uc758 \uc131\uacf5\uc740 \uc774\ub7ec\ud55c \ubc95\uce59\uc5d0 \ub530\ub77c \uc2dc\ud000\uc2a4 \ucd94\ucc9c \ubaa8\ub378\uc744 \ud655\uc7a5\ud558\ub294 \uac83\uc774 \ub300\uaddc\ubaa8 \ucd94\ucc9c \uc2dc\uc2a4\ud15c \uac1c\ubc1c\uc744 \uc704\ud55c \ud6a8\uacfc\uc801\uc778 \uc804\ub7b5\uc784\uc744 \uc2dc\uc0ac\ud55c\ub2e4.\n\n\ucd94\ucc9c \uc2dc\uc2a4\ud15c \ubd84\uc57c\uc5d0\uc11c \uc2dc\ud000\uc2a4 \ucd94\ucc9c \ubaa8\ub378\uc740 \uad11\ubc94\uc704\ud55c \uc544\ud0a4\ud14d\ucc98 \ud601\uc2e0 [36, 48, 55, 57, 62, 63, 68]\ub85c \ud2b9\uc9d5\uc9c0\uc5b4\uc9c0\ub294 \ud575\uc2ec \uc5f0\uad6c \ubd84\uc57c\uc774\ub2e4.\n\n\ucd08\uae30\uc5d0\ub294 \uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4\ub97c \uad00\ub9ac\ud558\uae30 \uc704\ud574 \ud480\ub9c1 \uc5f0\uc0b0\uc774 \uc0ac\uc6a9\ub418\uc5c8\ub2e4 [8]. \ub525\ub7ec\ub2dd\uc758 \ubc1c\uc804\uacfc \ud568\uaed8 CNN \uae30\ubc18 \uc544\ud0a4\ud14d\ucc98(\uc608: Caser [52], GNN \uae30\ubc18 \ubaa8\ub378\uc778 SR-GNN [61], RNN \uae30\ubc18 \ud504\ub808\uc784\uc6cc\ud06c\uc778 GRU4Rec [20])\ub97c \ud3ec\ud568\ud55c \ub354\uc6b1 \uc815\uad50\ud55c \ubaa8\ub378\uc774 \ub4f1\uc7a5\ud588\ub2e4. \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \uc5c4\uccad\ub09c \uc131\uacf5\uc5d0 \uc601\uac10\uc744 \ubc1b\uc544, \uc790\uae30-\uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \uae30\ubc18\uc73c\ub85c \ud558\ub294 \ubaa8\ub378\uc774 \uc81c\uc548\ub418\uc5c8\uc73c\uba70, SASRec [23] \ubc0f BERT4Rec [51]\uc640 \uac19\uc740 \uc8fc\ubaa9\ud560 \ub9cc\ud55c \uc544\ud0a4\ud14d\ucc98\ub97c \uc774\ub04c\uc5c8\ub2e4.\n\n\uc2dc\ud000\uc2a4 \ucd94\ucc9c \ubaa8\ub378 \uc678\uc5d0\ub3c4, DCN [58] \ubc0f xDeepFM [33]\uc640 \uac19\uc740 \ub525\ub7ec\ub2dd \ucd94\ucc9c \ubaa8\ub378(DLRM) \ub610\ud55c \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud55c\ub2e4. \uc774\ub7ec\ud55c DLRM\uc758 \ud575\uc2ec \uac1c\ub150\uc740 \ud2b9\uc9d5 \uac04 \uc0c1\ud638 \uc791\uc6a9\uc774\uba70, \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ub9e4\uc6b0 \uc911\uc694\ud558\ub2e4. \ud2b9\uc9d5 \uac04 \uc0c1\ud638 \uc791\uc6a9\uc740 \ub450 \uac00\uc9c0 \uc720\ud615\uc73c\ub85c \ubd84\ub958\ud560 \uc218 \uc788\ub2e4: \uba85\uc2dc\uc801 \ubc0f \uc554\uc2dc\uc801. \uba85\uc2dc\uc801 \uc0c1\ud638 \uc791\uc6a9\uc740 \ub2e4\uc591\ud55c \uc5f0\uc0b0\uc790(\uc608: \ub0b4\uc801 [43, 58], \uc774\uc9c4 \ud568\uc218 [33], \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998 [50])\ub97c \ud1b5\ud574 \ud2b9\uc9d5 \uac04 \uad00\uacc4\ub97c \uc9c1\uc811 \ubaa8\ub378\ub9c1\ud55c\ub2e4. \ubc18\ub300\ub85c, \uc554\uc2dc\uc801 \uc0c1\ud638 \uc791\uc6a9\uc740 \ub525 \ub274\ub7f4 \ub124\ud2b8\uc6cc\ud06c(DNN)\ub97c \uc801\uc6a9\ud558\uc5ec \ucd09\uc9c4\ub41c\ub2e4. \uc774\ub7ec\ud55c \uc811\uadfc \ubc29\uc2dd\uc740 \ud574\uc11d \uac00\ub2a5\uc131\uc774 \ubd80\uc871\ud558\uc9c0\ub9cc, DCN [58], DCNv2 [59], DeepFM [13], PNN [40]\uc640 \uac19\uc740 \ucd5c\ucca8\ub2e8 DLRM\uc5d0\uc11c \ub110\ub9ac \uc0ac\uc6a9\ub41c\ub2e4. \uc2e4\uc81c\ub85c \uc774\ub7ec\ud55c \ubaa8\ub378\uc5d0\uc11c \ud1b5\ud569\ub41c DNN\uc740 \uadf8 \uc6b0\uc218\ud55c \uc131\ub2a5\uc758 \ud575\uc2ec \ub3d9\uc778\uc774\ub2e4. \uadf8\ub7ec\ub098 \uc774\uc804 \uc5f0\uad6c [2, 15]\ub294 DLRM\uc774 \ubaa8\ub378 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ubc18\ub4dc\uc2dc \uc0c1\ub2f9\ud55c \uc131\ub2a5 \uac1c\uc120\uc744 \ubcf4\uc774\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud55c\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \ud2b9\uc9d5 \uac04 \uc0c1\ud638 \uc791\uc6a9\uc758 \uac1c\ub150\uc740 \uc5ec\uc804\ud788 \ubaa8\ub378 \uc124\uacc4\ub97c \uc548\ub0b4\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub420 \uc218 \uc788\ub2e4.\n\n\ud2b9\uc9d5 \uac04 \uc0c1\ud638 \uc791\uc6a9 \uad00\uc810\uc5d0\uc11c \ubcfc \ub54c, \uc2dc\ud000\uc2a4 \ucd94\ucc9c \ubaa8\ub378\uc740 \uc2dc\uac04\uc5d0 \ub530\ub978 \ub2e4\uc591\ud55c \ud2b9\uc9d5 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9\uc744 \ud0d0\uc0c9\ud558\ub294 \uac83\uc73c\ub85c \uac1c\ub150\ud654\ud560 \uc218 \uc788\ub2e4. \ud480\ub9c1 \ubc29\ubc95 [8]\uc740 \uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4\uc758 \uc758\ubbf8\uc801 \ud48d\ubd80\ud568\uc744 \uac04\uacfc\ud558\uae30 \ub54c\ubb38\uc5d0 \ud45c\ud604 \ub2a5\ub825\uc774 \uc81c\ud55c\ub41c\ub2e4. CNN \uae30\ubc18 \ubc29\ubc95 [52]\ub294 \uace0\uc815\ub41c \uc708\ub3c4\uc6b0 \ud06c\uae30\ub85c \uc778\ud574 \uc7a5\uac70\ub9ac \uc758\uc874\uc131\uc744 \ucea1\ucc98\ud558\ub294 \ub2a5\ub825\uc744 \uc81c\ud55c\ud55c\ub2e4. RNN \uae30\ubc18 \ubaa8\ub378 [20]\uc740 \uc774\uc804 \ud0c0\uc784\uc2a4\ud15d\uc758 \uc228\uaca8\uc9c4 \uc0c1\ud0dc\uc640 \uc9c1\uc811 \uc0c1\ud638 \uc791\uc6a9\ud560 \uc218 \uc788\uc5b4 \ubcf5\uc7a1\ud55c \uc0c1\ud638 \uc791\uc6a9\uc744 \ubaa8\ub378\ub9c1\ud558\ub294 \ub2a5\ub825\uc744 \uc81c\ud55c\ud560 \uc218 \uc788\ub2e4. GNN \uae30\ubc18 \uc811\uadfc \ubc29\uc2dd [61]\uc740 \uc9c1\uc811 \uc5f0\uacb0\ub41c \ud56d\ubaa9 \uac04\uc758 \ud2b9\uc9d5 \uac04 \uc0c1\ud638 \uc791\uc6a9\uc744 \uc81c\ud55c\ud558\uc5ec \ubc94\uc704\ub97c \uc881\ud78c\ub2e4. \ubc18\ub300\ub85c, \uc8fc\uc758 \uae30\ubc18 \ubaa8\ub378(\uc608: SASRec [23], BERT4Rec [51], TiSASRec [29], HSTU [72])\ub294 \ud3ec\uad04\uc801\uc778 \ud56d\ubaa9 \uac04 \uc0c1\ud638 \uc791\uc6a9\uc744 \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \uc774\ub7ec\ud55c \ubaa8\ub378\uc740 \uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790 \uad00\uc2ec\uc758 \ub3d9\uc801\uc744 \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \ucea1\ucc98\ud560 \uc218 \uc788\ub2e4. TiSASRec [29]\ub294 SASRec\uc744 \uac1c\uc120\ud558\uc5ec \uc2dc\uac04 \uac04\uaca9 \ubc0f \uc0c1\ub300\uc801 \uc704\uce58 \uc815\ubcf4\ub97c \ud1b5\ud569\ud568\uc73c\ub85c\uc368 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4. HSTU [72]\ub294 \uc704\uce58 \ubc0f \uc2dc\uac04 \uc815\ubcf4\ub97c \uc694\uc18c\ubcc4 \uacf1\uc148\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud558\uc5ec \ud56d\ubaa9 \uac04 \uba85\uc2dc\uc801 \uc0c1\ud638 \uc791\uc6a9\uc744 \ubaa8\ub378\ub9c1\ud568\uc73c\ub85c\uc368 \uc804\uc784\uc790\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90c\uc73c\ub85c\uc368 \uc774\ub97c \ubc1c\uc804\uc2dc\ucf30\ub2e4.\n\n\uc55e\uc11c \uc5b8\uae09\ud55c \uc791\uc5c5\uc758 \uc0c1\ub2f9\ud55c \uc9c4\uc804\uc744 \uace0\ub824\ud560 \ub54c, \uc5ec\uc804\ud788 \ud574\uacb0\ud574\uc57c \ud560 \uba87 \uac00\uc9c0 \ubb38\uc81c\uc810\uc774 \uc874\uc7ac\ud55c\ub2e4. \uccab\uc9f8, \uc774\uc804 \uc5f0\uad6c\ub294 \uba85\uc2dc\uc801 \uc0c1\ud638 \uc791\uc6a9\uc5d0\uc11c \uc2dc\uac04 \ubc0f \uc704\uce58 \uc815\ubcf4\ub97c \uc644\uc804\ud788 \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud588\ub2e4. \uc774 \uc815\ubcf4\ub97c \ub2e8\uc21c\ud788 \uc785\ub825 \uc2dc\ud000\uc2a4\uc5d0 \uc784\ubca0\ub529\uc744 \ucd94\uac00\ud558\uac70\ub098 \ud1b5\ud569\ud568\uc73c\ub85c\uc368 \ud1b5\ud569\ud588\uc73c\uba70...\n\n[... \uae38\uc774 \uc81c\ud55c\uc73c\ub85c \uc0dd\ub7b5 ...]"
        },
        {
          "name": "Related Work",
          "original": "2.1 Scaling Law\nScaling laws, prevalent in Natural Language Processing (NLP) [1, 3,\n24, 70], describe the relationship between a model\u2019s performance\nand its size, training data, and computational resources. These\nlaws extend beyond NLP to domains like autoregressive generative\n2\n\nFuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction Enhanced Transformer WWW \u201925, 28 April - 2 May, 2025, Sydney, Australia\nmodels [19] and visual processing [30, 39, 65, 66, 74]. In the recom-\nmendation domain, applying scaling laws is challenging. Studies\nshow that scaling benefits do not always apply to recommenda-\ntion models [2, 15]. Issues such as embedding collapse have been\nreported [15], and increasing non-embedding parameters in Deep\nLearning Recommendation Models (DLRMs) offers minimal gains\n[2].\nDespite these challenges, research into scaling laws for recom-\nmendation models persists. Studies have explored scaling in user ad\nactivity sequences with generative models [7] and efforts to scale\nuser representation models [ 49]. A sequential recommendation\nmodel with 0.8 billion parameters has been developed, highlighting\nscaling laws in this domain [ 76]. Additionally, it was found that\nincreasing computational resources benefits DLRM less than Gen-\nerative Recommendations (GR) [72]. This led to the development\nof HSTU, enhancing the GR paradigm in feature processing, model\narchitecture, and efficiency [72].\nOur study proposes a model designed to adhere to scaling laws,\nfacilitating its expansion into a large-scale recommendation model\nfor improved performance.\n2.2 Sequential Recommendation\nSequential recommendation focuses on predicting users\u2019 future in-\nterests based on past interactions [16, 17, 67, 69]. Early approaches\nused Markov Chain models [45]. With advancements in neural net-\nworks, various architectures have enhanced sequential modeling.\nGRU4Rec [20] uses Gated Recurrent Units to capture sequential\ndata, while Caser [ 52] employs CNNs for short-term preference\npatterns. To model long-term preferences, memory network-based\nmethods [5, 22, 79] were developed. Wu et al. [61] introduced graph-\nbased interaction modeling. SASRec [23] and BERT4Rec [51] lever-\nage self-attention mechanisms for improved recommendations.\nIn traditional recommendation systems, discriminative-based\nmodels typically rank items using a multi-level scoring approach.\nIn contrast, generative recommendation models can directly gen-\nerate the items to be recommended. Following the introduction of\nHSTU, it has become feasible for autoregressive sequence models\nthat adhere to scaling laws to evolve into generative recommenda-\ntion models by increasing their model size. HLLM [4] transforms the\ninput IDs into text information encoded by large language models\n(LLMs), and leverage another LLM for generative sequence rec-\nommendation. MBGen [37] incorporates behavior tokens into the\nsequence, thereby improving the model\u2019s multi-task capabilities.\nIn this study, we adopt the autoregressive sequence modeling\nparadigm to develop a new large-scale recommendation model.\n2.3 Feature Interactions\nFeature interactions play an important role in recommender sys-\ntems [55, 64, 77, 84] and can be divided into explicit and implicit\nmethods.\nExplicit interactions are categorized into four types based on\ntheir operations: dot product [ 13, 40, 43, 58], bilinear function\n[27, 33, 59], convolution [32, 34, 35, 56, 80\u201383], and attention mech-\nanisms [31, 50]. Dot product-based methods like Factorization Ma-\nchines (FM) and DeepFM extend logistic regression by capturing\npairwise interactions [13, 43]. DCN [58] models higher-order in-\nteractions through product-based cross networks, while DCNv2\n[59] enhances DCN with bilinear functions. DCNv3 [27] introduces\nthe Exponential Cross Network for more refined modeling. CCPM\n[35] and FGCNN [34] use CNNs for interactions, and Fi-GNN [32]\napplies GNNs. Attention-based methods like AutoInt [50] use at-\ntention mechanisms, and InterHAt [31] employs self-attention for\ninterpretable high-order interactions.\nImplicit interactions often use deep neural networks (DNNs)\n[78] to simultaneously engage all features. This approach is often\ncombined with explicit interaction structures to enhance overall\ninteraction capabilities. For example, dual-tower architectures like\nWide & Deep and DeepFM integrate low-order explicit interactions\nwith high-order implicit interactions [6, 13]. Models like xDeepFM,\nDCN, and DCNv2 use DNNs to compensate for certain limitations\nof explicit feature interactions. [33, 58, 59]. Single-tower structures\nimprove the expressiveness of explicitly crossed features by employ-\ning stacked DNNs after explicit interaction structures [18, 40, 41].\nInspired by successful feature interaction applications in recom-\nmendation models, our work aims to enhance large-scale recom-\nmendation models through improved feature interactions.\n3 PROBLEM STATEMENT\nIn the domain of sequential recommendation, the primary objective\nis to predict the next item a user is likely to interact with, based\non their historical interaction sequence. Formally, consider a set of\nusers U = {\ud835\udc621, \ud835\udc622, . . . , \ud835\udc62| U |} and a set of itemsI = {\ud835\udc561, \ud835\udc562, . . . , \ud835\udc56| I |}.\nFor each user \ud835\udc62 \u2208 U , we define an interaction sequence S\ud835\udc62 =\n[\ud835\udc56 (\ud835\udc62 )\n1 , \ud835\udc56(\ud835\udc62 )\n2 , . . . , \ud835\udc56(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 ], which is a chronologically ordered list of items.\nThe task of sequential recommendation is to predict the next\nitem \ud835\udc56 (\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 +1 that user \ud835\udc62 will interact with, given the sequence S\ud835\udc62.\nThis prediction can be formulated as estimating the probability\ndistribution over the item setI for the next interaction, conditioned\non the historical interactions:\ud835\udc43 (\ud835\udc56 (\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 +1 = \ud835\udc56 | S\ud835\udc62 ) for all\ud835\udc56 \u2208 I . During\ntraining, our objective is to predict the subsequent item \ud835\udc56 (\ud835\udc62 )\n\ud835\udc57+1 for\nevery prefix \ud835\udc57 of the sequence S\ud835\udc62. The desired output sequence is\n[\ud835\udc56 (\ud835\udc62 )\n2 , \ud835\udc56(\ud835\udc62 )\n3 , . . . , \ud835\udc56(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 +1] [23].",
          "translated": "2.1 Scaling Law\nScaling laws, prevalent in Natural Language Processing (NLP) [1, 3, 24, 70], describe the relationship between a model\u2019s performance and its size, training data, and computational resources. These laws extend beyond NLP to domains like autoregressive generative models [19] and visual processing [30, 39, 65, 66, 74]. In the recommendation domain, applying scaling laws is challenging. Studies show that scaling benefits do not always apply to recommendation models [2, 15]. Issues such as embedding collapse have been reported [15], and increasing non-embedding parameters in Deep Learning Recommendation Models (DLRMs) offers minimal gains [2].\n\nDespite these challenges, research into scaling laws for recommendation models persists. Studies have explored scaling in user ad activity sequences with generative models [7] and efforts to scale user representation models [ 49]. A sequential recommendation model with 0.8 billion parameters has been developed, highlighting scaling laws in this domain [ 76]. Additionally, it was found that increasing computational resources benefits DLRM less than Generative Recommendations (GR) [72]. This led to the development of HSTU, enhancing the GR paradigm in feature processing, model architecture, and efficiency [72].\n\nOur study proposes a model designed to adhere to scaling laws, facilitating its expansion into a large-scale recommendation model for improved performance.\n\n2.2 Sequential Recommendation\nSequential recommendation focuses on predicting users\u2019 future interests based on past interactions [16, 17, 67, 69]. Early approaches used Markov Chain models [45]. With advancements in neural networks, various architectures have enhanced sequential modeling. GRU4Rec [20] uses Gated Recurrent Units to capture sequential data, while Caser [ 52] employs CNNs for short-term preference patterns. To model long-term preferences, memory network-based methods [5, 22, 79] were developed. Wu et al. [61] introduced graph-based interaction modeling. SASRec [23] and BERT4Rec [51] leverage self-attention mechanisms for improved recommendations.\n\nIn traditional recommendation systems, discriminative-based models typically rank items using a multi-level scoring approach. In contrast, generative recommendation models can directly generate the items to be recommended. Following the introduction of HSTU, it has become feasible for autoregressive sequence models that adhere to scaling laws to evolve into generative recommendation models by increasing their model size. HLLM [4] transforms the input IDs into text information encoded by large language models (LLMs), and leverage another LLM for generative sequence recommendation. MBGen [37] incorporates behavior tokens into the sequence, thereby improving the model\u2019s multi-task capabilities.\n\nIn this study, we adopt the autoregressive sequence modeling paradigm to develop a new large-scale recommendation model.\n\n2.3 Feature Interactions\nFeature interactions play an important role in recommender systems [55, 64, 77, 84] and can be divided into explicit and implicit methods.\n\nExplicit interactions are categorized into four types based on their operations: dot product [ 13, 40, 43, 58], bilinear function [27, 33, 59], convolution [32, 34, 35, 56, 80\u201383], and attention mechanisms [31, 50]. Dot product-based methods like Factorization Machines (FM) and DeepFM extend logistic regression by capturing pairwise interactions [13, 43]. DCN [58] models higher-order interactions through product-based cross networks, while DCNv2 [59] enhances DCN with bilinear functions. DCNv3 [27] introduces the Exponential Cross Network for more refined modeling. CCPM [35] and FGCNN [34] use CNNs for interactions, and Fi-GNN [32] applies GNNs. Attention-based methods like AutoInt [50] use attention mechanisms, and InterHAt [31] employs self-attention for interpretable high-order interactions.\n\nImplicit interactions often use deep neural networks (DNNs) [78] to simultaneously engage all features. This approach is often combined with explicit interaction structures to enhance overall interaction capabilities. For example, dual-tower architectures like Wide & Deep and DeepFM integrate low-order explicit interactions with high-order implicit interactions [6, 13]. Models like xDeepFM, DCN, and DCNv2 use DNNs to compensate for certain limitations of explicit feature interactions. [33, 58, 59]. Single-tower structures improve the expressiveness of explicitly crossed features by employing stacked DNNs after explicit interaction structures [18, 40, 41].\n\nInspired by successful feature interaction applications in recommendation models, our work aims to enhance large-scale recommendation models through improved feature interactions.\n\n3 PROBLEM STATEMENT\nIn the domain of sequential recommendation, the primary objective"
        },
        {
          "name": "Methodology",
          "original": "The overview of our model architecture is depicted in Figure 2,\nwhich is composed of a stack of \ud835\udc4f FuXi Blocks. In the following\nsections, we will introduce each module individually. Finally, we\nwill discuss the optimization objectives.\n4.1 Embedding Layer\nWe convert each user\u2019s interaction sequence into a fixed-length\nsequence of length \ud835\udc5b through truncation or padding before the\nembedding layer. Sequences shorter than \ud835\udc5b are padded with a spe-\ncial \"padding item\". In the embedding layer, each item \ud835\udc56 \u2208 I is\nmapped to a \ud835\udc51-dimensional vector using a learnable embedding\nmatrix E \u2208 R| I | \u00d7\ud835\udc51 where \ud835\udc51 is the latent vector dimensionality.\nWe also employ learnable positional encodings [12], where \ud835\udc91\ud835\udc56 de-\nnotes the positional embedding of the \ud835\udc56-th position in the sequence.\nFor a user \ud835\udc62 with a sequence S\ud835\udc62 = [\ud835\udc56 (\ud835\udc62 )\n1 , . . . , \ud835\udc56(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 ], the output is\nx0 = [e(\ud835\udc62 )\n1 + \ud835\udc911, . . . , e(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 + \ud835\udc91\ud835\udc5b\ud835\udc62 , 0, \u00b7 \u00b7 \u00b7, 0], where the zero vectors\ndenote the padding items for positions beyond \ud835\udc5b\ud835\udc62 up to \ud835\udc5b.\n3\n\nWWW \u201925, 28 April - 2 May, 2025, Sydney, Australia Ye and Guo, et al.\nAdaptive Multi-channel Self-attention\nMulti-stage FFN\nInteraction sequence\nFuxi BlockFuxi Block\nFuxi BlockMLPOutput Predictions\nEmbedding Layer\nFigure 2: The overall architecture of the proposed FuXi-\ud835\udefc.\n4.2 FuXi Block\nThe core component of our model is composed of\ud835\udc4f stacked layers of\nFuXi block which are similar to the transformer decoder [54]. Each\nFuXi block consists of an Adaptive Multi-channel Self-attention\n(AMS) layer and a Multi-stage Feed-Forward Network (MFFN). The\nadaptive multi-channel self-attention is a variant of the multi-head\nself-attention [54], while the multi-stage FFN first combines the\nmulti-channel outputs of the AMS layer and then performs implicit\nfeature interactions. In this architecture, let x\ud835\udc59 \u22121 \u2208 R\ud835\udc5b\u00d7\ud835\udc51 denote\nthe input to the \ud835\udc59-th layer, and x\ud835\udc59 \u2208 R\ud835\udc5b\u00d7\ud835\udc51 denote the output of the\n\ud835\udc59-th layer. The initial input for the first layer is given by x0.\n4.2.1 Adaptive Multi-channel Self-attention The AMS layer is de-\nsigned to effectively capture and utilize the user interest patterns\ninherent in sequential data. Unlike conventional multi-head self-\nattention mechanisms, which typically integrate positional encod-\nings directly into the input embeddings, our FuXi self-attention\nseparates the processing of hidden states, positional information,\nand temporal signals into distinct attention heads. This separation\nallows each head to specialize in capturing different aspects of the\nsequence data, thereby enhancing the model\u2019s capacity to learn\ncomplex interest patterns.\nAs depicted in Figure 3, we define three types of channels: se-\nmantic, temporal, and positional channels. The attention weights\nin the temporal and positional channels depend only on the dif-\nference in relative timestamps and relative positions. Additionally,\nthere is no further need to calculate the query and key matrices\nin these two channels. To circumvent the intricacy of the model,\nwe opt not to employ extra value matrices for the temporal and\npositional heads. Instead, they will share the value matrices with\nthe semantics channel. The following approach is used to compute\nthese matrices which is similar to multi-head self-attention:\n\u02dcx\ud835\udc59 = RMSN(x\ud835\udc59 \u22121) (1)\nq\ud835\udc59 = \ud835\udf19 ( \u02dcx\ud835\udc59 W\ud835\udc59\n\ud835\udc5e), k\ud835\udc59 = \ud835\udf19 ( \u02dcx\ud835\udc59 W\ud835\udc59\n\ud835\udc58 ), v\ud835\udc59 = \ud835\udf19 ( \u02dcx\ud835\udc59 W\ud835\udc59\n\u210e) (2)\nwhere W\ud835\udc59\ud835\udc5e \u2208 R\ud835\udc51 \u00d7\ud835\udc51\u210e, W\ud835\udc59\n\ud835\udc58 \u2208 R\ud835\udc51 \u00d7\ud835\udc51\u210e, W\ud835\udc59\ud835\udc63 \u2208 R\ud835\udc51 \u00d7\ud835\udc51\u210e are the learn-\nable parameters. RMSN denotes the root mean square (RMS) layer\nnormalization operation [75]. \ud835\udf19 provides nonlinearity which we\nemploy SiLU [11] here, and \ud835\udc51\u210e represents the size of each head.\nRelativeEmbedding\nInput\n\ud835\udc49\nPositionTimestamp\n\ud835\udc34!\ud835\udc34\"\n\ud835\udc4a# \ud835\udc4a$ \ud835\udc4a%\nOutput\n\ud835\udc4a&\n\u00d7\n\ud835\udf19\ud835\udc44\ud835\udc3e\ud835\udc49Semantic Channel\ud835\udc34\ud835\udc49PositionalChannel\ud835\udc34\ud835\udc49TemporalChannel\nLayer Normalization\nFigure 3: Illustration of Adaptive Multi-channel Self-\nattention (AMS). In contrast to the conventional multi-head\nself-attention, AMS decouples the modeling of temporal and\npositional information from semantics information.\nThe following describes the method for calculating the attention\nweights for semantic, temporal, and positional channels separately:\na\ud835\udc59\n\u210e = 1\n\ud835\udc5b \ud835\udf19 (q\ud835\udc59 (k\ud835\udc59 )\ud835\udc47 ), (a\ud835\udc59\n\ud835\udc61 )\ud835\udc56,\ud835\udc57 = \ud835\udefc (\ud835\udc61 \ud835\udc57 \u2212 \ud835\udc61\ud835\udc56 ), (a\ud835\udc59\n\ud835\udc5d )\ud835\udc56,\ud835\udc57 = \ud835\udefd \ud835\udc57 \u2212\ud835\udc56 (3)\nwhere, \ud835\udf19 supplies nonlinearity, and we leverage SiLU once again.\nPrevious studies have demonstrated that the use of SiLU function\nin self-attention layers outperforms softmax in sequence recom-\nmendation tasks [72]. The term \ud835\udefc (\ud835\udc61 \ud835\udc57 \u2212 \ud835\udc61\ud835\udc56 ) represents the mapping\nof the difference in timestamps into buckets, where each bucket\nis associated with a learnable parameter [42]. On the other hand,\n\ud835\udefd \u2208 R\ud835\udc5b denotes a vector of learnable parameters.\nSubsequent to the computation of outputs from the channels,\nthese outputs are concatenated and subjected to RMS layer nor-\nmalization. Following this, the normalized result is element-wise\nmultiplied with the matrix\ud835\udc48 , which is derived from \u02dc\ud835\udc65\ud835\udc59 . The process\nis encapsulated by the following formula:\nh\ud835\udc59 = RMSN(concat(a\ud835\udc59\n\u210ev\ud835\udc59\n\u210e, a\ud835\udc59\n\ud835\udc5d v\ud835\udc59\n\ud835\udc5d, a\ud835\udc59\n\ud835\udc61 v\ud835\udc59\n\ud835\udc61 )) \u2297 \ud835\udf19 (x\ud835\udc59 W\ud835\udc59\n\ud835\udc62 ) (4)\nhere W\ud835\udc59\ud835\udc62 \u2208 R\ud835\udc51 \u00d73\ud835\udc51\u210e denotes learnable parameters and \ud835\udf19 denotes\nSiLU function. We adopted the design of the matrix \ud835\udc48 in our ar-\nchitecture following HSTU [ 72] to introduce explicit 2-order in-\nteractions. For simplicity and clarity, we describe the case with a\nsingle head in each channel here. However, this approach can be\neasily extended to multiple heads within each channel, similar to\nthe multi-head self-attention [54].\n4.2.2 Multi-stage Feed-Forward Network The MFFN encompasses\ntwo distinct stages as depicted in Figure 4. In the first stage, the\noutputs from different channels are fused with the original input of\nthe current layer. Subsequently, in the second stage, implicit feature\ninteractions are conducted.\u00b7\nIn the first stage, MFFN receives the outputs across different\nchannels from the AMS layer and applies a projection transfor-\nmation characterized by learnable parameters \ud835\udc4a\ud835\udc5c \u2208 R3\ud835\udc51\u210e \u00d7\ud835\udc51. The\noutput of this stage is obtained by combining the projected output\n4\n\nFuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction Enhanced Transformer WWW \u201925, 28 April - 2 May, 2025, Sydney, Australia\nStage 1\nStage 2\nAMS outputinput\ud835\udc4a!\n\ud835\udc4a\"\n+\n\ud835\udc4a#\n\ud835\udc4a$\n\u00d7\n+\nLayer Normalization\nMFFN output\nFigure 4: Diagram of MFFN: Stage 1 fuses outputs from differ-\nent channels; Stage 2 facilitates implicit feature interactions.\nwith the input of current layer x\ud835\udc59 .\no\ud835\udc59 = h\ud835\udc59 W\ud835\udc59\n\ud835\udc5c + x\ud835\udc59 \u22121 (5)\nIn the second stage, the primary objective of MFFN is to conduct\nimplicit interactions. Following LLaMa [53], we apply RMS layer\nnormalization to the output of the previous stage and followed by\na SwiGLU activation [ 46] to enhance feature learning and then\nadding the residual connection:\nx\ud835\udc59 = FFN\ud835\udc59 (RMSN(o\ud835\udc59 )) + o\ud835\udc59 (6)\nFFN\ud835\udc59 (x) = SwiGLU(x)W\ud835\udc59\n3 = (\ud835\udf19 (xW\ud835\udc59\n1) \u2297 ( xW\ud835\udc59\n2))W\ud835\udc59\n3 (7)\nwhere \ud835\udf19 represents SiLU, \u2297 denotes element-wise multiplication,\nand W\ud835\udc59\n1 \u2208 R\ud835\udc51 \u00d7\ud835\udc51\ud835\udc39 \ud835\udc39 \ud835\udc41 , W\ud835\udc59\n2 \u2208 R\ud835\udc51 \u00d7\ud835\udc51\ud835\udc39 \ud835\udc39 \ud835\udc41 , W\ud835\udc59\n3 \u2208 R\ud835\udc51\ud835\udc39 \ud835\udc39 \ud835\udc41 \u00d7\ud835\udc51 are learnable\nparameters. This configuration allows the network to effectively\ncapture complex interactions within the data while maintaining\nefficient gradient flow through the residual connections.\n4.3 Prediction Layer & Optimization objective\nAfter passing through \ud835\udc4f layers of FuXi blocks, each position has\nobtained sufficient information about the previously interacted\nitems. We employ a multiplication with the transpose of the input\nembedding matrix, followed by a softmax function to obtain a\nprobability distribution over predicted items. The transformation\ncan be mathematically represented as follows:\n\ud835\udc43\n\u0010\n\ud835\udc56 (\ud835\udc62 )\n\ud835\udc61 = \ud835\udc56 | \ud835\udc56 (\ud835\udc62 )\n1 , . . . , \ud835\udc56(\ud835\udc62 )\n\ud835\udc61 \u22121\n\u0011\n= \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65\n\u0010\nx\ud835\udc4f E\ud835\udc47\n\u0011\n\ud835\udc56\n(8)\nIn order to accelerate the training process, we adopt the sampled\nsoftmax loss with \ud835\udc41 randomly sampled negative samples [25].\n5 ANALYSIS\n5.1 Space and Time Complexity\nSpace Complexity Each FuXi block comprises an AMS layer and\nan MFFN. The AMS layer features four projection matrices totaling\n6\ud835\udc51 \u00d7 \ud835\udc51\ud835\udc3b parameters, alongside positional and temporal embeddings\nwith \ud835\udc42 (\ud835\udc5b + \ud835\udc5b\ud835\udc35) parameters, where \ud835\udc5b\ud835\udc35 is the number of buckets.\nThe MFFN includes four projection matrices, amounting to 3\ud835\udc51\u210e \u00d7\n\ud835\udc51 + 3\ud835\udc51\ud835\udc39 \ud835\udc39 \ud835\udc41 \u00d7 \ud835\udc51 parameters. The item embeddings have |I | \u00d7 \ud835\udc51\nparameters. Typically,\ud835\udc51\u210e and \ud835\udc51\ud835\udc39 \ud835\udc39 \ud835\udc41 are proportional to \ud835\udc51, and \ud835\udc5b is\ncomparable to \ud835\udc5b\ud835\udc35. Therefore, we assume \ud835\udc51\u210e = \ud835\udc42 (\ud835\udc51), \ud835\udc51\ud835\udc39 \ud835\udc39 \ud835\udc41 = \ud835\udc42 (\ud835\udc51),\nand \ud835\udc5b\ud835\udc35 = \ud835\udc42 (\ud835\udc5b). FuXi-\ud835\udefc is formed by stacking \ud835\udc4f FuXi layers, leading\nto a total space complexity of \ud835\udc42 (\ud835\udc4f (\ud835\udc512 + \ud835\udc5b) + |I |\ud835\udc51).\nTime Complexity The time complexity for computing attention\nweights in the semantics channel is \ud835\udc42 (\ud835\udc5b2\ud835\udc51), compared to \ud835\udc42 (\ud835\udc5b2) in\nother channels. Calculating the QKV matrices and the MFFN both\nrequire \ud835\udc42 (\ud835\udc5b\ud835\udc512). The cost for generating predictions is \ud835\udc42 (\ud835\udc5b|I |\ud835\udc51).\nThus, the overall time complexity is \ud835\udc42 (\ud835\udc4f\ud835\udc5b2\ud835\udc51 + \ud835\udc5b(\ud835\udc4f\ud835\udc51 2 + |I |\ud835\udc51)).\n5.2 Polynomial Approximation\nNext, we examine the properties of explicit inter-item interactions\nimplemented by FuXi-\ud835\udefc. To better analyze these interactions, we\nsimplify the \ud835\udc59-th layer of the FuXi Block by treating attention\nweights as constants and omitting the second stage of the MFFN,\nactivation functions, and most projection transformations. This\nsimplification yields:\n\ud835\udc53 (\ud835\udc59 )\n\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58 (\ud835\udc65\ud835\udc56 ; \ud835\udc651, \u00b7 \u00b7 \u00b7\ud835\udc65\ud835\udc5b) = \ud835\udc65\ud835\udc56 \u25e6 \u00a9\u00ad\n\u00ab\n\ud835\udc5b\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc4e (\ud835\udc59 )\n\ud835\udc56,\ud835\udc57 \ud835\udc65 \ud835\udc57\n\u00aa\u00ae\n\u00ac\n+ \ud835\udc65\ud835\udc56 (9)\nwhere the vectors \ud835\udc651, . . . , \ud835\udc65\ud835\udc5b are the latent representations input to\nthe \ud835\udc59-th layer of the FuXi block; \u25e6 denotes the interaction operator,\nsuch as element-wise multiplication; and \ud835\udc4e (\ud835\udc59 )\n\ud835\udc56,\ud835\udc57 are the attention\nweights in the \ud835\udc59-th layer. In this section, let \ud835\udc65\ud835\udc59,\ud835\udc56 denote the output\nlatent representation of the \ud835\udc56-th item after the \ud835\udc59-th layer. Let \ud835\udc39\ud835\udc5b\ndenote a polynomial of the form \u00cd\n\ud835\udf36 \ud835\udc64 \ud835\udf36\n\u00ce\n\ud835\udc56 \ud835\udc65\ud835\udefc\ud835\udc56\n0,\ud835\udc56, where the sum\nincludes all terms satisfying \u00cd \ud835\udefc\ud835\udc56 \u2264 \ud835\udc5b. We will use mathematical\ninduction to show that \ud835\udc65\ud835\udc4f,\ud835\udc56 = \ud835\udc65\ud835\udc56,0\ud835\udc392\ud835\udc4f \u22121.\n5.2.1 Base Case Consider \ud835\udc4f = 0. Here, \ud835\udc650,\ud835\udc56 = \ud835\udc650,\ud835\udc56 \u00b7 1 = \ud835\udc650,\ud835\udc56 \u00b7 \ud835\udc390,\nconfirming the equation holds.\n5.2.2 Inductive Step Assume the property holds for some integer\n\ud835\udc59 \u2265 0. Now consider \ud835\udc4f = \ud835\udc59 + 1:\n\ud835\udc65\ud835\udc59+1,\ud835\udc56 = \ud835\udc65\ud835\udc59,\ud835\udc56 \u25e6\n\ud835\udc5b\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc4e (\ud835\udc59+1)\n\ud835\udc56,\ud835\udc57 \ud835\udc65\ud835\udc59,\ud835\udc57 + \ud835\udc65\ud835\udc59,\ud835\udc56 (10)\n= \ud835\udc650,\ud835\udc56\ud835\udc392\ud835\udc59 \u22121 \u25e6 \u00a9\u00ad\n\u00ab\n\ud835\udc5b\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc4e (\ud835\udc59+1)\n\ud835\udc56,\ud835\udc57 \ud835\udc650,\ud835\udc57 \ud835\udc392\ud835\udc59 \u22121 + 1\u00aa\u00ae\n\u00ac\n(11)\nFor any term of the form\u00ce\n\ud835\udc57 \ud835\udc65\ud835\udefc\ud835\udc56\n0,\ud835\udc57 , where 1 \u2264 \u00cd \ud835\udefc\ud835\udc56 \u2264 2\ud835\udc59+1, it appears\nin the expression \u00cd\ud835\udc5b\n\ud835\udc57=1 \ud835\udc4e (\ud835\udc59+1)\n\ud835\udc56,\ud835\udc57 \ud835\udc650,\ud835\udc57 \ud835\udc392\ud835\udc59 \u22121. Thus, we have\n\ud835\udc5b\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc4e (\ud835\udc59+1)\n\ud835\udc56,\ud835\udc57 \ud835\udc650,\ud835\udc57 \ud835\udc392\ud835\udc59 \u22121 + 1 = \ud835\udc392\ud835\udc59 (12)\nTherefore, it follows that \ud835\udc65\ud835\udc59+1,\ud835\udc56 = \ud835\udc650,\ud835\udc56\ud835\udc392\ud835\udc59 +1 \u22121.\nConsequently, after progressing through \ud835\udc4f layers of the FuXi\nblocks, \ud835\udc65\ud835\udc4f,\ud835\udc56 incorporates the outcome of feature interaction between\n\ud835\udc650,\ud835\udc56 and the result of interactions among all the items being of any\ndegree up to 2\ud835\udc59 \u2212 1.\n5.3 Analysis of AMS\nThe formulation of relative positional embeddings in the T5 ar-\nchitecture [ 42] is delineated as follows. The attention weights\nA = (\ud835\udc4e\ud835\udc56,\ud835\udc57 )\ud835\udc5b\u00d7\ud835\udc5b can be computed by the process:\nA = \ud835\udf19\n\u0010\n(xW\ud835\udc5e) (xW\ud835\udc58 )\ud835\udc47 + B\n\u0011\n(13)\nwhere \ud835\udf19 denotes a non-linear function, such as softmax or SiLU,\nand B = (\ud835\udc4f\ud835\udc56,\ud835\udc57 )\ud835\udc5b\u00d7\ud835\udc5b denotes the matrix of the relative positional\nbias term. Let \ud835\udc5e\ud835\udc56 \u2208 R1\u00d7\ud835\udc5b denotes the query vector of the \ud835\udc56-th item,\nand \ud835\udc58\ud835\udc56, \ud835\udc63\ud835\udc56, \ud835\udc62\ud835\udc56 denotes the key vector, the value vector, the vector\nused for Hadamard product respectively. The output of multi-head\n5\n\nWWW \u201925, 28 April - 2 May, 2025, Sydney, Australia Ye and Guo, et al.\nTable 1: Dataset statistics.\nDataset User Item Interactions Avg. Len.\nMovieLens-1M 6,041 3,706 1,000,209 165.60\nMovieLens-20M 138,493 26,744 20,000,263 144.41\nKuaiRand 25,634 7,550 6,945,823 270.96\nIndustrial 19,252,028 234,488 1,023,711,774 53.17\nself-attention \ud835\udc5c\ud835\udc56 of the \ud835\udc56-th item is then computed as:\n\ud835\udc5c\ud835\udc56 = \ud835\udc4a\ud835\udc5c\n\u0010\u0010\u2211\ufe01\n\ud835\udc4e\ud835\udc56,\ud835\udc57 \ud835\udc63 \ud835\udc57\n\u0011\n\u2297 \ud835\udc62\ud835\udc56\n\u0011\n(14)\n\u2248 \ud835\udc4a\ud835\udc5c\n\u0010\u0010\u2211\ufe01\n\ud835\udf191 (\ud835\udc5e\ud835\udc56\ud835\udc58\ud835\udc47\n\ud835\udc57 )\ud835\udc49\ud835\udc57\n\u0011\n\u2297 \ud835\udc62\ud835\udc56\n\u0011\n+ \ud835\udc4a\ud835\udc5c\n\u0010\u0010\u2211\ufe01\n\ud835\udf192 (\ud835\udc4f\ud835\udc56,\ud835\udc57 )\ud835\udc63 \ud835\udc57\n\u0011\n\u2297 \ud835\udc62\ud835\udc56\n\u0011\n(15)\nOn the other hand, in the AMS layer, the calculation process is\nexpressed as:\n\ud835\udc5c\ud835\udc56 = \ud835\udc4a\ud835\udc5c1\n\u0010\u0010\u2211\ufe01\n\ud835\udf19 (\ud835\udc5e\ud835\udc56\ud835\udc58\ud835\udc47\n\ud835\udc57 )\ud835\udc49\ud835\udc57\n\u0011\n\u2297 \ud835\udc62 (1)\n\ud835\udc56\n\u0011\n+ \ud835\udc4a\ud835\udc5c2\n\u0010\u0010\u2211\ufe01\n\ud835\udc4f\ud835\udc56,\ud835\udc57 \ud835\udc49\ud835\udc57\n\u0011\n\u2297 \ud835\udc62 (2)\n\ud835\udc56\n\u0011\n(16)\nwhere\ud835\udc4a\ud835\udc5c1,\ud835\udc4a\ud835\udc5c2 denote the parameters in the first stage of the MFFN,\nand vectors \ud835\udc62 (1)\n\ud835\udc56 and \ud835\udc62 (2)\n\ud835\udc56 correspond to the \ud835\udc62\ud835\udc56 vectors within the\nsemantics and positional channels, respectively. This demonstrates\nthat the AMS layer facilitates a more expressive representation of\npositional and temporal information compared to the direct addition\nof attention weights, suggesting an enhancement in the model\u2019s\ncapacity to leverage the temporal and positional information.\n5.4 Relationship with Existing Models\nOur work shares structural similarities with three models: SAS-\nRec [23], LLaMa [10], and HSTU [72]. Here, we highlight the key\ndifferences between these models and our approach.\n5.4.1 SASRec and LLaMa Unlike SASRec and LLaMa, which em-\nploy standard NLP architectures for recommendation systems, our\nmodel introduces two major innovations. First, instead of the tra-\nditional multi-head self-attention layer, we use the AMS layer to\nindependently model temporal, positional, and semantic informa-\ntion, improving the model\u2019s feature utilization. Second, our model\nincorporates the MFFN, diverging from the FFN used in SASRec\nand LLaMa, by processing multi-channel information from the\nself-attention layer and enabling implicit feature interaction.\n5.4.2 HSTU HSTU incorporates relative temporal and positional\ndata by adding these features directly to attention weights, which\ncan dilute their impact. Moreover, HSTU lacks an FFN layer, relying\nsolely on self-attention and explicit feature interactions, limiting its\nability to capture complex item relationships. Our model overcomes\nthese limitations by decoupling temporal, positional, and semantic\ninformation within the self-attention layer and leveraging the MFFN\nto facilitate implicit interactions.",
          "translated": "**\ubc29\ubc95\ub860**\n\n\ubcf8 \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc758 \uac1c\uc694\ub294 \uadf8\ub9bc 2\uc5d0 \ub098\ud0c0\ub0b4\uc5b4\uc838 \uc788\uc73c\uba70, \uc774\ub294 \ud835\udc4f\uac1c\uc758 FuXi \ube14\ub85d\uc73c\ub85c \uad6c\uc131\ub41c \uc2a4\ud0dd\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uc139\uc158\uc5d0\uc11c\ub294 \uac01 \ubaa8\ub4c8\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \uc18c\uac1c\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ucd5c\uc801\ud654 \ubaa9\ud45c\uc5d0 \ub300\ud574 \ub17c\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n4.1 \uc784\ubca0\ub529 \ub808\uc774\uc5b4\n\uac01 \uc0ac\uc6a9\uc790\uc758 \uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4\ub97c \ud2b9\uc218 \"\ud328\ub529 \uc544\uc774\ud15c\"\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud328\ub529\ub41c \uc2dc\ud000\uc2a4 \uae38\uc774 \ud835\udc5b\uc758 \uace0\uc815\ub41c \uae38\uc774 \uc2dc\ud000\uc2a4\ub85c \ubcc0\ud658\ud558\uae30 \uc704\ud574 \uc784\ubca0\ub529 \ub808\uc774\uc5b4 \uc804\uc5d0 \uc790\ub974\uae30 \ub610\ub294 \ud328\ub529\ud569\ub2c8\ub2e4. \ud835\udc5b\ubcf4\ub2e4 \uc9e7\uc740 \uc2dc\ud000\uc2a4\ub294 \ud2b9\uc218 \"\ud328\ub529 \uc544\uc774\ud15c\"\uc73c\ub85c \ud328\ub529\ub429\ub2c8\ub2e4. \uc784\ubca0\ub529 \ub808\uc774\uc5b4\uc5d0\uc11c \uac01 \uc544\uc774\ud15c \ud835\udc56 \u2208 I\ub294 \ud835\udc51\ucc28\uc6d0 \ubca1\ud130\ub85c \ud559\uc2b5 \uac00\ub2a5\ud55c \uc784\ubca0\ub529 \ud589\ub82c E \u2208 R|I|\u00d7\ud835\udc51\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub9e4\ud551\ub429\ub2c8\ub2e4. \uc5ec\uae30\uc11c \ud835\udc51\ub294 \uc7a0\uc7ac \ubca1\ud130 \ucc28\uc6d0\uc785\ub2c8\ub2e4. \ub610\ud55c \ud559\uc2b5 \uac00\ub2a5\ud55c \uc704\uce58 \uc778\ucf54\ub529 [12]\ub97c \uc0ac\uc6a9\ud558\uba70, \ud835\udc5d\ud835\udc56\ub294 \ud835\udc59-\ubc88\uc9f8 \uc2dc\ud000\uc2a4 \uc704\uce58\uc758 \uc704\uce58 \uc778\ucf54\ub529\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud835\udc62 \uc0ac\uc6a9\uc790\uc5d0 \ub300\ud574 \ud835\udc46\ud835\udc62 = [\ud835\udc56 (\ud835\udc62 )\n1 , . . . , \ud835\udc56(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 ]\ub77c\ub294 \uc2dc\ud000\uc2a4\ub97c \uac16\ub294 \uacbd\uc6b0 \ucd9c\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\nx0 = [e(\ud835\udc62 )\n1 + \ud835\udc911, . . . , e(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 + \ud835\udc91\ud835\udc5b\ud835\udc62 , 0, \u00b7 \u00b7 \u00b7, 0], \uc5ec\uae30\uc11c 0 \ubca1\ud130\ub294 \ud835\udc5b\ud835\udc62\ubd80\ud130 \ud835\udc5b\uae4c\uc9c0\uc758 \uc704\uce58\ub97c \ucd08\uacfc\ud558\ub294 \uc704\uce58\uc5d0 \ub300\ud55c \ud328\ub529 \uc544\uc774\ud15c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\nWWW \u201925, 28 April - 2 May, 2025, Sydney, Australia Ye and Guo, et al.\n\uc801\uc751\ud615 \uba40\ud2f0 \ucc44\ub110 \uc790\uac00 \uc8fc\uc758\n\ub2e4\ub2e8\uacc4 FFN\n\uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4\nFuXi \ube14\ub85dFuXi \ube14\ub85d\nFuXi \ube14\ub85dMLP \ucd9c\ub825 \uc608\uce21\n\uc784\ubca0\ub529 \ub808\uc774\uc5b4\n\uadf8\ub9bc 2: \uc81c\uc548\ub41c FuXi-\ud835\udefc\uc758 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98.\n\n4.2 FuXi \ube14\ub85d\n\ubaa8\ub378\uc758 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\ub294 \ud835\udc4f\uac1c\uc758 FuXi \ube14\ub85d \ub808\uc774\uc5b4\uac00 \uc313\uc778 \uac83\uc73c\ub85c, \uc774\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ub514\ucf54\ub354 [54]\uc640 \uc720\uc0ac\ud569\ub2c8\ub2e4. \uac01 FuXi \ube14\ub85d\uc740 \uc801\uc751\ud615 \uba40\ud2f0 \ucc44\ub110 \uc790\uac00 \uc8fc\uc758 (AMS) \ub808\uc774\uc5b4\uc640 \ub2e4\ub2e8\uacc4 \uc804\uacb0\ud569 \uc2e0\uacbd\ub9dd (MFFN)\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc801\uc751\ud615 \uba40\ud2f0 \ucc44\ub110 \uc790\uac00 \uc8fc\uc758\ub294 \ub2e4\uc911 \ud5e4\ub4dc \uc790\uac00 \uc8fc\uc758 [54]\uc758 \ubcc0\ud615\uc774\uba70, \ub2e4\ub2e8\uacc4 FFN\uc740 \uba3c\uc800 AMS \ub808\uc774\uc5b4\uc758 \ub2e4\uc911 \ucc44\ub110 \ucd9c\ub825\uacfc \uacb0\ud569\ud55c \ub2e4\uc74c \ubd88\uba85\ud655\ud55c \ud2b9\uc9d5 \uc0c1\ud638 \uc791\uc6a9\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uc774 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c x\ud835\udc59 \u22121 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\ub294 \ud835\udc59-\ubc88\uc9f8 \ub808\uc774\uc5b4\uc758 \uc785\ub825\uc774\uace0, x\ud835\udc59 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\ub294 \ud835\udc59-\ubc88\uc9f8 \ub808\uc774\uc5b4\uc758 \ucd9c\ub825\uc785\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ub808\uc774\uc5b4\uc758 \ucd08\uae30 \uc785\ub825\uc740 x0\uc785\ub2c8\ub2e4.\n\n4.2.1 \uc801\uc751\ud615 \uba40\ud2f0 \ucc44\ub110 \uc790\uac00 \uc8fc\uc758 AMS \ub808\uc774\uc5b4\ub294 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc5d0 \ub0b4\uc7ac\ub41c \uc0ac\uc6a9\uc790 \uad00\uc2ec \ud328\ud134\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ucea1\ucc98\ud558\uace0 \ud65c\uc6a9\ud558\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uae30\uc874\uc758 \ub2e4\uc911 \ud5e4\ub4dc \uc790\uac00 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uacfc \ub2ec\ub9ac, \uc77c\ubc18\uc801\uc73c\ub85c \uc704\uce58 \uc778\ucf54\ub529\uc744 \uc785\ub825 \uc784\ubca0\ub529\uc5d0 \uc9c1\uc811 \ud1b5\ud569\ud558\ub294 \ub300\uc2e0, FuXi \uc790\uac00 \uc8fc\uc758\ub294 \uc228\uaca8\uc9c4 \uc0c1\ud0dc, \uc704\uce58 \uc815\ubcf4 \ubc0f \uc2dc\uac04 \uc2e0\ud638\ub97c \ubcc4\ub3c4\uc758 \uc8fc\uc758 \ud5e4\ub4dc\ub85c \ucc98\ub9ac\ud558\uc5ec \uc2dc\uac04 \ubc0f \uc704\uce58 \uc815\ubcf4\ub97c \ubd84\ub9ac\ud569\ub2c8\ub2e4. \uc774 \ubd84\ub9ac\ub294 \uac01 \ud5e4\ub4dc\uac00 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc758 \ub2e4\uc591\ud55c \uce21\uba74\uc744 \ucea1\ucc98\ud558\ub3c4\ub85d \uc804\ubb38\ud654\ud560 \uc218 \uc788\uac8c \ud558\uc5ec \ubaa8\ub378\uc774 \ubcf5\uc7a1\ud55c \uad00\uc2ec \ud328\ud134\uc744 \ud559\uc2b5\ud558\ub294 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.\n\n\uadf8\ub9bc 3\uc5d0 \ub098\ud0c0\ub0b4\ub4ef\uc774 \uc138 \uac00\uc9c0 \uc720\ud615\uc758 \ucc44\ub110\uc744 \uc815\uc758\ud569\ub2c8\ub2e4. \uc2dc\uac04 \ubc0f \uc704\uce58 \ucc44\ub110\uc758 \uc8fc\uc758 \uac00\uc911\uce58\ub294 \uc2dc\uac04 \ubc0f \uc704\uce58 \ucc28\uc774\uc5d0 \ub530\ub978 \ucc28\uc774\ub9cc \uc758\uc874\ud558\uba70, \uc774\ub7ec\ud55c \ucc44\ub110\uc5d0\uc11c \ucffc\ub9ac \ubc0f \ud0a4 \ud589\ub82c\uc744 \uacc4\uc0b0\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \ubaa8\ub378\uc758 \ubcf5\uc7a1\uc131\uc744 \uc6b0\ud68c\ud558\uae30 \uc704\ud574 \uc2dc\uac04 \ubc0f \uc704\uce58 \ud5e4\ub4dc\uc5d0 \ucd94\uac00 \uac12 \ud589\ub82c\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub3c4\ub85d \uc120\ud0dd\ud588\uc2b5\ub2c8\ub2e4. \ub300\uc2e0 \uc774\ub4e4\uc740 \uc758\ubbf8 \ucc44\ub110\uacfc \uacf5\uc720\ub429\ub2c8\ub2e4. \ub2e4\uc74c \uc811\uadfc \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c \ud589\ub82c\uc744 \uacc4\uc0b0\ud558\uba70, \uc774\ub294 \ub2e4\uc911 \ud5e4\ub4dc \uc790\uac00 \uc8fc\uc758\uc640 \uc720\uc0ac\ud569\ub2c8\ub2e4.\n\u02dcx\ud835\udc59 = RMSN(x\ud835\udc59 \u22121) (1)\nq\ud835\udc59 = \ud835\udf19 ( \u02dcx\ud835\udc59 W\ud835\udc59\n\ud835\udc5e), k\ud835\udc59 = \ud835\udf19 ( \u02dcx\ud835\udc59 W\ud835\udc59\n\ud835\udc58 ), v\ud835\udc59 = \ud835\udf19 ( \u02dcx\ud835\udc59 W\ud835\udc59\n\u210e) (2)\n\uc5ec\uae30\uc11c W\ud835\udc59\ud835\udc5e \u2208 R\ud835\udc51 \u00d7\ud835\udc51\u210e, W\ud835\udc59\n\ud835\udc58 \u2208 R\ud835\udc51 \u00d7\ud835\udc51\u210e, W\ud835\udc59\ud835\udc63 \u2208 R\ud835\udc51 \u00d7\ud835\udc51\u210e\ub294 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\uc785\ub2c8\ub2e4. RMSN\uc740 \ub8e8\ud2b8 \ud3c9\uade0 \uc81c\uacf1 (RMS) \ub808\uc774\uc5b4 \uc815\uaddc\ud654 \uc5f0\uc0b0 [75]\ub97c \ub098\ud0c0\ub0b4\uba70, \ud835\udf19\ub294 \uc5ec\uae30\uc5d0\uc11c SiLU [11]\ub97c \uc81c\uacf5\ud558\uace0, \ud835\udc51\u210e\ub294 \uac01 \ud5e4\ub4dc\uc758 \ud06c\uae30\uc785\ub2c8\ub2e4.\n\n\uc0c1\ub300\uc801 \uc784\ubca0\ub529\n\uc785\ub825\n\ud835\udc49\n\uc704\uce58 \ud0c0\uc784\uc2a4\ud0ec\ud504\n\ud835\udc34!\ud835\udc34\"\n\ud835\udc4a# \ud835\udc4a$ \ud835\udc4a%\n\ucd9c\ub825\n\ud835\udc4a&\n\u00d7\n\ud835\udf19\ud835\udc44\ud835\udc3e\ud835\udc49 \uc758\ubbf8 \ucc44\ub110\ud835\udc34\ud835\udc49 \uc704\uce58 \ucc44\ub110\ud835\udc34\ud835\udc49 \uc2dc\uac04 \ucc44\ub110\ud835\udc34\ud835\udc49\n\ub808\uc774\uc5b4 \uc815\uaddc\ud654\n\uadf8\ub9bc 3: \uc801\uc751\ud615 \uba40\ud2f0 \ucc44\ub110 \uc790\uac00 \uc8fc\uc758 (AMS)\uc758 \uc124\uba85. AMS\ub294 \uc2dc\uac04 \ubc0f \uc704\uce58 \uc815\ubcf4\ub97c \uc758\ubbf8 \uc815\ubcf4\uc640 \ubd84\ub9ac\ud558\uc5ec \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4.\n\n\ub2e4\uc74c\uc740 \ucc44\ub110\ubcc4\ub85c \uc8fc\uc758 \uac00\uc911\uce58\ub97c \uacc4\uc0b0\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\na\ud835\udc59\n\u210e = 1\n\ud835\udc5b \ud835\udf19 (q\ud835\udc59 (k\ud835\udc59 )\ud835\udc47 ), (a\ud835\udc59\n\ud835\udc61 )\ud835\udc56,\ud835\udc57 = \ud835\udefc (\ud835\udc61 \ud835\udc57 \u2212 \ud835\udc61\ud835\udc56 ), (a\ud835\udc59\n\ud835\udc5d )\ud835\udc56,\ud835\udc57 = \ud835\udefd \ud835\udc57 \u2212\ud835\udc56 (3)\n\uc5ec\uae30\uc11c \ud835\udf19\ub294 \ube44\uc120\ud615\uc131\uc744 \uc81c\uacf5\ud558\uace0 \ub2e4\uc2dc SiLU\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\uc804 \uc5f0\uad6c\uc5d0\uc11c SiLU \ud568\uc218\uac00 \uc2dc\ud000\uc2a4 \ucd94\ucc9c \uc791\uc5c5\uc5d0\uc11c \uc18c\ud504\ud2b8\ub9e5\uc2a4\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc88b\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4 [72]. \ud835\udefc (\ud835\udc61 \ud835\udc57 \u2212 \ud835\udc61\ud835\udc56 )\ub294 \uc2dc\uac04 \ubc0f \uc704\uce58 \ucc28\uc774\uc5d0 \ub530\ub978 BUCKET \ub9e4\ud551\uc744 \ub098\ud0c0\ub0b4\uba70, \uac01 BUCKET\ub294 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218 [42]\uc640 \uad00\ub828\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ud835\udefd \u2208 R\ud835\udc5b\uc740 \ud559\uc2b5 \uac00\ub2a5\ud55c \ubca1\ud130\uc785\ub2c8\ub2e4.\n\n\ucc44\ub110\uc5d0\uc11c \ucd9c\ub825 \uacc4\uc0b0 \ud6c4 \uc774\ub7ec\ud55c \ucd9c\ub825\uc740 \uc5f0\uacb0\ub418\uace0 RMS \ub808\uc774\uc5b4 \uc815\uaddc\ud654\ub429\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc815\uaddc\ud654\ub41c \uacb0\uacfc\ub97c x\ud835\udc59 W\ud835\udc59\n\ud835\udc62\ub85c \uc694\uc18c\ubcc4\ub85c \uacf1\ud569\ub2c8\ub2e4. \uc774 \ud504\ub85c\uc138\uc2a4\ub294 \ub2e4\uc74c \uacf5\uc2dd\uc744 \ud1b5\ud574 \ucea1\ucc98\ub429\ub2c8\ub2e4.\nh\ud835\udc59 = RMSN(concat(a\ud835\udc59\n\u210ev\ud835\udc59\n\u210e, a\ud835\udc59\n\ud835\udc5d v\ud835\udc59\n\ud835\udc5d, a\ud835\udc59\n\ud835\udc61 v\ud835\udc59\n\ud835\udc61 )) \u2297 \ud835\udf19 (x\ud835\udc59 W\ud835\udc59\n\ud835\udc62 ) (4)\n\uc5ec\uae30\uc11c W\ud835\udc59\ud835\udc62 \u2208 R\ud835\udc51 \u00d73\ud835\udc51\u210e\ub294 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\uc785\ub2c8\ub2e4.\n\n[... \uae38\uc774 \uc81c\ud55c\uc73c\ub85c \uc0dd\ub7b5 ...]"
        },
        {
          "name": "Experiments",
          "original": "6.1 Experiment Setup\n6.1.1 Datasets To evaluate the performance of the proposedFuXi-\ud835\udefc\narchitecture, we conduct extensive experiments on four real-world\ndatasets, including three public datasets and one private large-scale\ndataset, which are described as follows:\n\u2022 MovieLens-1M and MovieLens-20M1. The MovieLens dataset\nis a widely used movie recommendation dataset, which con-\ntains users\u2019 rating and tagging activities. It has multiple sub-\nsets of different sizes. We select two subsets, MovieLens-1M and\nMovieLens-20M for our experiments.\n\u2022 KuaiRand2. This dataset is collected with the user logs of a video-\nsharing app from kuaishou. Users in this platform are usualy very\nactive, with more than 200 interactions on average.\n\u2022 Industrial This dataset is constructed from user records of a\nmainstream music listening app, which has tens of millions active\nusers every month. We construct users\u2019 behavior sequence with\nover a month of positive behaviors, including collect, like, play\nand so on.\nFor the first two datasets (MovieLens-1M and MovieLens-20M),\nwe use the pre-processed train/validation/test set 3 as in HSTU\n[72] from Meta exactly. For the latter two datasets (KuaiRand and\nIndustrial), we process them using a similar manner to HSTU [72]\nby ourself. The statistics are shown in Table 1.\n6.1.2 Compared Baseline For a comprehensive comparison, we\ncompare FuXi-\ud835\udefc against two types of representative baselines: i)\nconventional models, including BPRMF [44], GRU4Rec [20], and\nNARM [28]; ii) autoregressive generative models, including SASRec\n[23], LLaMa [10], and HSTU [72].\n6.1.3 Evaluation Metrics We employ the widely used top-K Hit Ra-\ntio (HR@\ud835\udc3e), Normalized Discounted Cumulative Gain (NDCG@\ud835\udc3e)\nand Mean Reciprocal Rank (MRR) to evaluate the recall perfor-\nmances. For all metrics, higher value means better performance.\nWe rank the ground-truth item from full set of items and report the\nperformance of \ud835\udc3e = 10, 50 by default.\n6.1.4 Parameter Settings We implement our proposed FuXi-\ud835\udefc with\nPytorch [38]. To enable large-scale model training, we apply the\nmulti-machine and multi-card parallelism with the Accelerate li-\nbrary [26]. For a fair comparison, we maintain the same model\nparameters as HSTU [72] in the first two datasets, except for the\nnumber of layers. For the KuaiRand dataset, we set the hidden\ndimension as 50, and the number of negative samples as 128 by de-\nfault. All other parameters like optimizer, learning rate and weight\ndecay are consistent with HSTU [72]. For all the three datasets, the\nembedding dimensions and self-attention hidden vector dimensions\nare identical. For the basic modeling capacity comparison, we set\nthe number of layers as 2. We also extend these generative models\nto deeper layers by stacking 4x number of layers (8 layers) and\ndenoting it as \"XX-Large\" to analyze scaling effects.\n6.2 Performance Comparison (RQ1)\n6.2.1 Public Dataset Performance The overall performance com-\nparison of the proposed FuXi-\ud835\udefc and baseline models are shown in\nTable 2. Based on the results, we have the following observations:\n\u2022 Firstly, the generative models (i.e., SASRec, LLaMa, HSTU, and\nFuXi-\ud835\udefc) outperform conventional models (i.e., BPRMF, GRU4Rec\nand NARM), even when equipped with just two layers of pa-\nrameters. This demonstrates the generative models\u2019 superior\n1https://grouplens.org/datasets/movielens/\n2https://kuairand.com/\n3https://github.com/facebookresearch/generative-recommenders\n6\n\nFuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction Enhanced Transformer WWW \u201925, 28 April - 2 May, 2025, Sydney, Australia\nTable 2: The overall performance comparison. We use \u2605 to indicate a statistically significant result comparing FuXi- \ud835\udefc with the\nbest baseline which is indicated by underlined numbers.\nDataset MovieLens-1M MovieLens-20M KuaiRand\nModel NG@10 NG@50 HR@10 HR@50 MRR NG@10 NG@50 HR@10 HR@50 MRR NG@10 NG@50 HR@10 HR@50 MRR\nBPRMF 0.0607 0.1027 0.1185 0.3127 0.0556 0.0629 0.1074 0.1241 0.3300 0.0572 0.0248 0.0468 0.0520 0.1560 0.0235\nGRU4Rec 0.1015 0.1460 0.1816 0.3864 0.0895 0.0768 0.1155 0.1394 0.3177 0.0689 0.0289 0.0531 0.0597 0.1726 0.0275\nNARM 0.1350 0.1894 0.2445 0.4915 0.1165 0.1037 0.1552 0.1926 0.4281 0.0910 0.0411 0.0747 0.0836 0.2399 0.0387\nSASRec 0.1594 0.2187 0.2824 0.5500 0.1375 0.1553 0.2119 0.2781 0.5353 0.1330 0.0486 0.0877 0.0978 0.2801 0.0454\nLLaMa 0.1620 0.2207 0.2926 0.5591 0.1373 0.1640 0.2206 0.2915 0.5476 0.1402 0.0495 0.0878 0.0973 0.2752 0.0466\nHSTU 0.1639 0.2238 0.2969 0.5672 0.1390 0.1642 0.2225 0.2909 0.5553 0.1410 0.0491 0.0861 0.0992 0.2718 0.0451\nFuXi-\ud835\udefc 0.1835 0.2429 0.3254 0.5941 0.1557 0.1954 0.2533 0.3353 0.5969 0.1677 0.0537 0.0942 0.1067 0.2951 0.0497\nSASRec-Large 0.1186 0.1733 0.2183 0.4671 0.0186 0.0206 0.0379 0.0412 0.1209 0.0207 0.0285 0.0428 0.0544 0.1227 0.0258\nLLaMa-Large 0.1659 0.2257 0.2990 0.5692 0.1408 0.1842 0.2412 0.3202 0.5776 0.1576 0.0494 0.0878 0.0970 0.2754 0.0466\nHSTU-Large 0.1844 0.2437 0.3255 0.5929 0.1568 0.1995 0.2572 0.3407 0.6012 0.1714 0.0494 0.0883 0.0990 0.2799 0.0460\nFuXi-\ud835\udefc-Large 0.1934 0.2518 0.3359 0.5983 0.1651 0.2086 0.2658 0.3530 0.6113 0.1792 0.0555 0.0963 0.1105 0.2995 0.0510\nTable 3: Performance comparison on Industrial dataset.\nDataset Industrial\nModel NG@10 NG@50 HR@10 HR@50 MRR\nSASRec 0.1009 0.1580 0.1970 0.4581 0.0868\nLLaMa 0.1681 0.2238 0.2985 0.5498 0.1426\nHSTU 0.1733 0.2289 0.3057 0.5565 0.1472\nFuXi-\ud835\udefc 0.1875 0.2424 0.3230 0.5702 0.1601\nTable 4: Efficiency comparison on KuaiRand dataset with\ndifferent sequence length.\nDataset KuaiRand\nModel TPS@200 TPS@400 TPS@600 TPS@800\nSASRec 2481 2024 1672 1398\nLLaMa 2330 1972 1602 1326\nHSTU 2078 1183 680 436\nFuXi-\ud835\udefc 1971 1053 615 394\nTable 5: Performances of different FuXi- \ud835\udefc variants.\nDataset MovieLens-1M MovieLens-20M KuaiRand\nModel NG@10 HR@10 NG@10 HR@10 NG@10 HR@10\nBase 0.1454 0.2676 0.1452 0.2647 0.0476 0.0928\nw/o AMS 0.1563 0.2847 0.1612 0.2888 0.0470 0.0921\nw/o MFFN 0.1878 0.3304 0.2056 0.3488 0.0534 0.0947\nFuXi-\ud835\udefc 0.1934 0.3359 0.2086 0.3530 0.0555 0.1105\nability in capturing complex item relationships and diverse user\npreferences by their autoregressive modeling paradigm.\n\u2022 Secondly, as an early sequential model, SASRec fails to scale\nup to 8 layers across all three datasets, with a significant per-\nformance drop when the number of layers is increased to 8. In\ncontrast, the two recently proposed transformer-based architec-\ntures, LLaMa and HSTU, show substantial improvements in the\nfirst two datasets.\n\u2022 Finally, FuXi-\ud835\udefc consistently obtains the best results on all three\ndatasets with all evaluation metrics, no matter it\u2019s a shallow\nnetwork or a deep network. This demonstrates the outstand-\ning ability of our proposed FuXi-\ud835\udefc. Specifically, for shallow net-\nwork, it outperforms the strongest baseline HSTU by 13.24% in\nNDCG@10 (10.59% in NDCG@50, 10.81% in HR@10, 6.94% in\nHR@50, 13.72% in MRR) on average of the three datasets. For\ndeep network, it outperforms the strongest baseline HSTU-Large\n2 4 8 16 32\nNumber of layers\n0.170\n0.175\n0.180\n0.185\n0.190\n0.195\n0.200\n0.205NDCG@10\n0.31\n0.32\n0.33\n0.34\n0.35\nHR@10\nNDCG@10\nHR@10\nFigure 5: Scaling of FuXi-\ud835\udefc on Industrial Dataset.\nby 7.26% in NDCG@10 (5.24% in NDCG@50, 6.14% in HR@10,\n3.19% in HR@50, 6.90% in MRR) on average of the three datasets.\nThe excellent performance of FuXi-\ud835\udefc demonstrates the great util-\nity of introducing explicit and implicit feature interaction for\ndedicated user behavior modeling.\n6.2.2 Industrial Dataset Performance Table 3 presents the perfor-\nmance comparison of our proposed FuXi-\ud835\udefc against several baseline\nmodels on a private, large-scale industrial dataset. The current\nonline baseline in this scenario is a multi-channel recall system,\nwith SASRec as one of the channels that recalls items based on\nembedding similarity. The music recalled from multiple channels\nis mixed together and then passed through a cascaded pre-ranking\nand ranking process to obtain the final recommended music list.\nFrom Table 3, we have two key observations. Firstly, the newly\nproposed LLaMa and HSTU significantly outperform SASRec in\nthis music recommendation scenario, achieving gains of 64.82% and\n71.75% in NDCG@10, respectively. Secondly, our FuXi-\ud835\udefc outper-\nforms both LLaMa and HSTU by 11.54% and 8.19%, respectively.\nThese substantial improvements highlight the potential of scaling\nlaws, and the superiority of our proposed FuXi-\ud835\udefc.\n6.2.3 Scaling of FuXi- \ud835\udefc on Industrial Dataset Figure 5 presents\nthe performance of our proposed FuXi-\ud835\udefc on the industrial dataset\nwhen scaling up the number of layers while keeping all other hyper-\nparameters unchanged. Due to the memory limitation, we only scale\nup the layers to 32. We observe that FuXi-\ud835\udefc adheres to the scaling\nlaw, as the results show a positive relationship between the model\u2019s\nperformance and its size. This is a highly attractive property as the\n7\n\nWWW \u201925, 28 April - 2 May, 2025, Sydney, Australia Ye and Guo, et al.\n2 4 8 16\nNumber of layers\n0.182\n0.184\n0.186\n0.188\n0.190\n0.192\n0.194NDCG@10\n0.3250\n0.3275\n0.3300\n0.3325\n0.3350\n0.3375\nHR@10\nNDCG@10\nHR@10\n(a) MovieLens-1M\n2 4 8 16\nNumber of layers\n0.053\n0.054\n0.055\n0.056NDCG@10\n0.106\n0.108\n0.110\n0.112\n0.114\nHR@10\nNDCG@10\nHR@10 (b) KuaiRand\nFigure 6: Performances with different number of layers.\nperformance can be further improved by scaling up of the model\nsize, its training data, and the computational resources used.\n6.3 Efficiency Comparison (RQ2)\nWe assess the efficiency of theFuXi-\ud835\udefc architecture by comparing its\nThroughput Per Second (TPS) with generative baseline models. Ex-\nperiments were conducted on the KuaiRand dataset with sequence\nlengths ranging from 200 to 800. Each experiment involved three\ncomplete forward and backward propagations across the dataset,\ncalculating the average number of training samples processed per\nsecond. All hyperparameters, except sequence length, were consis-\ntent with previous experiments. Table 4 shows the TPS results. As\nsequence length increases, TPS for all models decreases. Notably,\nSASRec and LLaMa outperform HSTU andFuXi-\ud835\udefc in TPS, likely due\nto their exclusion of temporal information encoding, which, while\nperformance-enhancing, is time-intensive. Consequently, FuXi-\ud835\udefc\nachieves similar TPS to HSTU but significantly better overall per-\nformance, as seen in Tables 2 and 3.\n6.4 Ablation Study (RQ3)\nTo assess the effectiveness of sub-modules in our FuXi-\ud835\udefc architec-\nture, we analyze three model variants: (1) Base Model: Replaces\nthe AMS module with the vanilla self-attention layer from SASRec\nand substitutes the MFFN module with a single-stage MLP from\nHSTU. (2) w/o AMS : Replaces the AMS module with the vanilla\nself-attention layer. (3) w/o MFFN : Substitutes the MFFN module\nwith a single-stage MLP.\nTable 5 presents the ablation results, revealing the critical role\nof each component in model performance. Notably, removing the\nsecond stage of the MFFN results in a significant performance drop,\nemphasizing the importance of thorough implicit feature interac-\ntions. Despite this, the model still outperforms HSTU, demonstrat-\ning the effectiveness of our approach in capturing temporal and\npositional information. Additionally, replacing the AMS with the\nvanilla self-attention layer leads to a marked performance decline,\nhighlighting the necessity of explicit feature interactions and effec-\ntive use of temporal and positional data in recommendation tasks.\nThese results confirm the essential contributions of each module to\nthe model\u2019s predictive capability.\n6.5 Hyperparameter Study (RQ4)\nWe examine the effects of various hyper-parameters on FuXi-\ud835\udefc,\nfocusing on (1) the number of layers, (2) the hidden dimension,\nand (3) the number of negative samples for training. Due to space\nconstraints, we present only NDCG@10 and HR@10 results for\nthe MovieLens-1M and KuaiRand datasets. Results for other met-\nrics (NDCG@50, HR@50, MRR) and datasets (MovieLens-20M) are\n8 16 32 64\nHidden dimension\n0.10\n0.12\n0.14\n0.16\n0.18NDCG@10\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\nHR@10\nNDCG@10\nHR@10\n(a) MovieLens-1M\n8 16 32 64\nHidden dimension\n0.035\n0.040\n0.045\n0.050\n0.055NDCG@10\n0.07\n0.08\n0.09\n0.10\n0.11\nHR@10\nNDCG@10\nHR@10 (b) KuaiRand\nFigure 7: Performances with different hidden dimension.\n32 64 128 256\nNumber of negatives\n0.1800\n0.1825\n0.1850\n0.1875\n0.1900\n0.1925NDCG@10\n0.326\n0.328\n0.330\n0.332\n0.334\n0.336\nHR@10\nNDCG@10\nHR@10\n(a) MovieLens-1M\n32 64 128 256\nNumber of negatives\n0.0475\n0.0500\n0.0525\n0.0550\n0.0575\n0.0600NDCG@10\n0.085\n0.090\n0.095\n0.100\n0.105\n0.110\n0.115\nHR@10\nNDCG@10\nHR@10 (b) KuaiRand\nFigure 8: Diverse negative sample counts in performances.\nsimilar but omitted. We alter one hyper-parameter at a time while\nkeeping others constant to ensure fair comparisons.\n6.5.1 The number of layers Increasing layers is a rapid method\nto scale model parameters and enhance FuXi-\ud835\udefc\u2019s representational\ncapacity. We vary layers from 2 to 16, as shown in Figure 6. On\nMovieLens-1M, performance improves from 2 to 8 layers, but de-\nclines at 16 layers. Conversely, on KuaiRand, performance consis-\ntently increases from 2 to 16 layers. This may be due to MovieLens-\n1M\u2019s smaller size limiting parameter scaling.\n6.5.2 The hidden dimension Uniform embedding and self-attention\nhidden dimensions are used across datasets. Increasing hidden di-\nmensions enhances item representation and self-attention similarity\naccuracy. Adjusting dimensions from 8 to 64, Figure 7 shows perfor-\nmance on MovieLens-1M saturates at 32 dimensions, with minimal\ngains beyond. In contrast, KuaiRand performance steadily improves\nacross all dimensions.\n6.5.3 Negative Samples The influence of negative sampling on large\nrecommendation models has been overlooked in studies on LLM\nscaling laws [21, 24]. We vary negative samples from 32 to 256, with\nresults in Figure 8. Performance improves on both datasets even\nbeyond 64 negative samples, with gains from negative sampling\nsurpassing those from layer increases. This underscores the critical\nrole of negative sampling in enhancing models\u2019 performance.\n6.6 Online A/B Test\nIn a main scenario of Huawei Music, we conducted a 7-day online\nA/B test to evaluate the performance of our new model,FuXi-\ud835\udefc, uti-\nlizing 30% of the user traffic. The results demonstrated that FuXi-\ud835\udefc\nachieved significant improvements compared to a well-optimized\nmulti-channel retrieval baseline that has been refined over several\nyears. Specifically, the average number of songs played per user in-\ncreased by 4.67%, while the average listening duration per user rose\nby 5.10%. These findings indicate that FuXi-\ud835\udefc excels in enhancing\nuser interaction and engagement, particularly by improving user\nexperience and increasing platform usage time. After evaluation of\n8\n\nFuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction Enhanced Transformer WWW \u201925, 28 April - 2 May, 2025, Sydney, Australia\nseveral weeks, the FuXi-\ud835\udefc had become an inherent channel in this\nscenario to serve most of the online traffic.",
          "translated": "**6.1 \uc2e4\ud5d8 \uc124\uc815**\n\n6.1.1 \ub370\uc774\ud130\uc14b\n\uc81c\uc548\ub41c FuXi-\ud835\udefc \uc544\ud0a4\ud14d\ucc98\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574, \uc138 \uac1c\uc758 \uacf5\uac1c \ub370\uc774\ud130\uc14b\uacfc \ud558\ub098\uc758 \ub300\uaddc\ubaa8 \uac1c\uc778 \ub370\uc774\ud130\uc14b\uc744 \ud3ec\ud568\ud558\uc5ec \ub124 \uac1c\uc758 \uc2e4\uc81c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ub2e4\uc74c\uc73c\ub85c \uc124\uba85\ud558\ub294 \ub300\ub85c \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\u2022 MovieLens-1M \ubc0f MovieLens-20M: MovieLens \ub370\uc774\ud130\uc14b\uc740 \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 \uc601\ud654 \ucd94\ucc9c \ub370\uc774\ud130\uc14b\uc73c\ub85c, \uc0ac\uc6a9\uc790 \ud3c9\uc810 \ubc0f \ud0dc\uae45 \ud65c\ub3d9\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc5ec\ub7ec \ud06c\uae30\uc758 \ud558\uc704 \uc9d1\ud569\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc800\ud76c \uc2e4\ud5d8\uc744 \uc704\ud574 \ub450 \uac1c\uc758 \ud558\uc704 \uc9d1\ud569\uc778 MovieLens-1M \ubc0f MovieLens-20M\uc744 \uc120\ud0dd\ud588\uc2b5\ub2c8\ub2e4.\n\u2022 KuaiRand2: \uc774 \ub370\uc774\ud130\uc14b\uc740 kuaishou\uc758 \ube44\ub514\uc624 \uacf5\uc720 \uc571\uc758 \uc0ac\uc6a9\uc790 \ub85c\uadf8\ub97c \uc218\uc9d1\ud558\uc5ec \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ud50c\ub7ab\ud3fc\uc758 \uc0ac\uc6a9\uc790\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ub9e4\uc6b0 \ud65c\uc801\uc774\uba70 \ud3c9\uade0\uc801\uc73c\ub85c 200\uac74 \uc774\uc0c1\uc758 \uc0c1\ud638 \uc791\uc6a9\uc744 \ud569\ub2c8\ub2e4.\n\u2022 Industrial: \uc774 \ub370\uc774\ud130\uc14b\uc740 \uc218\ubc31\ub9cc \uba85\uc758 \uc6d4\uac04 \ud65c\uc131 \uc0ac\uc6a9\uc790\ub97c \uac00\uc9c4 \uc2a4\ud2b8\ub9ac\ubc0d \uc74c\uc545 \uc571\uc758 \uc0ac\uc6a9\uc790 \uae30\ub85d\uc5d0\uc11c \uad6c\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud55c \ub2ec \uc774\uc0c1\uc758 \uae0d\uc815\uc801\uc778 \ud589\ub3d9 \uc2dc\ud000\uc2a4\ub97c \uc0ac\uc6a9\uc790 \ud589\ub3d9\uc73c\ub85c \uad6c\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \uc218\uc9d1, \uc88b\uc544\uc694, \uc7ac\uc0dd \ub4f1\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n\uccab \ub450 \ub370\uc774\ud130\uc14b(MovieLens-1M \ubc0f MovieLens-20M)\uc758 \uacbd\uc6b0, Meta[72]\uc5d0\uc11c \uc0ac\uc6a9\ud55c pre-processed train/validation/test \uc138\ud2b8\ub97c \uadf8\ub300\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \ub4a4 \ub450 \ub370\uc774\ud130\uc14b(KuaiRand \ubc0f Industrial)\uc740 HSTU [72]\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc800\ud76c\uac00 \uc9c1\uc811 \ucc98\ub9ac\ud588\uc2b5\ub2c8\ub2e4. \ud1b5\uacc4\ub294 \ud45c 1\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.\n\n6.1.2 \uae30\ubcf8 \ubaa8\ub378 \ube44\uad50\n\uc885\ud569\uc801\uc778 \ube44\uad50\ub97c \uc704\ud574, FuXi-\ud835\udefc\ub97c \ub2e4\uc74c \ub450 \uac00\uc9c0 \uc720\ud615\uc758 \ub300\ud45c\uc801\uc778 \uae30\ubcf8 \ubaa8\ub378\uacfc \ube44\uad50\ud569\ub2c8\ub2e4. i) BPRMF [44], GRU4Rec [20], NARM [28]\ub97c \ud3ec\ud568\ud55c \uc804\ud1b5\uc801\uc778 \ubaa8\ub378 \ubc0f ii) SASRec [23], LLaMa [10], HSTU [72]\ub97c \ud3ec\ud568\ud55c \uc0dd\uc131 \ubaa8\ub378.\n\n6.1.3 \ud3c9\uac00 \uc9c0\ud45c\n\ub9ac\ucf5c \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574, \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 top-K Hit Ratio (HR@\ud835\udc3e), Normalized Discounted Cumulative Gain (NDCG@\ud835\udc3e) \ubc0f Mean Reciprocal Rank (MRR)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \uc9c0\ud45c\uc5d0\uc11c \ub354 \ub192\uc740 \uac12\uc740 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. Ground-truth item\uc744 \uc804\uccb4 \uc544\uc774\ud15c \uc9d1\ud569\uc5d0\uc11c \uc21c\uc704\ub97c \ub9e4\uaca8 K=10, 50\uc5d0 \ub300\ud55c \uc131\ub2a5\uc744 \uae30\ubcf8\uc801\uc73c\ub85c \ubcf4\uace0\ud569\ub2c8\ub2e4.\n\n6.1.4 \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\n\uc81c\uc548\ub41c FuXi-\ud835\udefc\ub97c Pytorch [38]\ub97c \uc0ac\uc6a9\ud558\uc5ec \uad6c\ud604\ud569\ub2c8\ub2e4. \ub300\uaddc\ubaa8 \ubaa8\ub378 \ud6c8\ub828\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uae30 \uc704\ud574, Accelerate \ub77c\uc774\ube0c\ub7ec\ub9ac [26]\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba40\ud2f0-\uba38\uc2e0 \ubc0f \uba40\ud2f0-\uce74\ub4dc \ubcd1\ub82c \ucc98\ub9ac\ub97c \uc801\uc6a9\ud569\ub2c8\ub2e4. \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \uccab \ub450 \ub370\uc774\ud130\uc14b\uc758 \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130\ub97c HSTU [72]\uc640 \ub3d9\uc77c\ud558\uac8c \uc720\uc9c0\ud588\uc9c0\ub9cc \ub808\uc774\uc5b4 \uc218\ub9cc \ub2e4\ub985\ub2c8\ub2e4. KuaiRand \ub370\uc774\ud130\uc14b\uc758 \uacbd\uc6b0, hidden dimension\uc744 50\uc73c\ub85c \uc124\uc815\ud558\uace0 negative sample \uc218\ub97c \uae30\ubcf8\uc801\uc73c\ub85c 128\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. Optimizer, learning rate \ubc0f weight decay\uc640 \uac19\uc740 \ub2e4\ub978 \ubaa8\ub4e0 \ud30c\ub77c\ubbf8\ud130\ub294 HSTU [72]\uc640 \uc77c\uce58\ud569\ub2c8\ub2e4. \uc138 \ub370\uc774\ud130\uc14b \ubaa8\ub450 embedding dimension \ubc0f self-attention hidden vector dimension\uc774 \ub3d9\uc77c\ud569\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc778 \ubaa8\ub378 \uc6a9\ub7c9 \ube44\uad50\ub97c \uc704\ud574 \ub808\uc774\uc5b4 \uc218\ub97c 2\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \ub610\ud55c, 4\ubc30 \ub808\uc774\uc5b4 \uc218(8 \ub808\uc774\uc5b4)\ub85c \ud655\uc7a5\ud558\uc5ec \"XX-Large\"\ub85c \uc9c0\uc815\ud558\uace0 scaling \ud6a8\uacfc\ub97c \ubd84\uc11d\ud588\uc2b5\ub2c8\ub2e4.\n\n6.2 \uc131\ub2a5 \ube44\uad50 (RQ1)\n\n6.2.1 \uacf5\uac1c \ub370\uc774\ud130\uc14b \uc131\ub2a5\n\uc81c\uc548\ub41c FuXi-\ud835\udefc \ubc0f \uae30\ubcf8 \ubaa8\ub378\uc758 \uc804\ubc18\uc801\uc778 \uc131\ub2a5 \ube44\uad50\ub294 \ud45c 2\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc5d0 \ub530\ub974\uba74 \ub2e4\uc74c \uad00\ucc30\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\u2022 \uccab\uc9f8, \uc0dd\uc131 \ubaa8\ub378(i.e., SASRec, LLaMa, HSTU, FuXi-\ud835\udefc)\ub294 BPRMF, GRU4Rec \ubc0f NARM\uacfc \uac19\uc740 \uc804\ud1b5\uc801\uc778 \ubaa8\ub378\ubcf4\ub2e4 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4.\n\u2022 \ub450 \ubc88\uc9f8\ub85c, FuXi-\ud835\udefc\ub294 2\uac1c\uc758 \ub808\uc774\uc5b4\ub9cc\uc73c\ub85c\ub3c4 \uae30\ubcf8 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.\n\n[\u2026 \uae38\uc774 \uc81c\ud55c\uc73c\ub85c \uc0dd\ub7b5 \u2026]"
        },
        {
          "name": "Conclusion",
          "original": "In our paper, we proposed a novel model called FuXi-\ud835\udefc, which\nleverages Adaptive Multi-channel Self-attention to enhance the\ninteractions with temporal and positional features, and Multi-stage\nFeed-Forward Networks (MFFNs) to facilitate implicit interactions.\nOur offline and online A/B experiments demonstrate that FuXi-\ud835\udefc\nconsistently outperforms prior models, and reveal the effectiveness\nof each component. Additionally, the performance continually im-\nproves while scaling up our model, highlighting its potential for\nlarge-scale recommendation systems. In future work, we plan to\nextend our model to tackle more complex recommendation prob-\nlems, such as multi-behavior and multi-modal recommendations,\nand to apply our model to scenarios involving long sequences.",
          "translated": "\uacb0\ub860\n\n\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud55c \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc778 FuXi-\ud835\udefc\ub294 \uc801\uc751\ud615 \ub2e4\ucc44\ub110 \uc790\uae30 \uc8fc\uc758(Adaptive Multi-channel Self-attention)\ub97c \ud65c\uc6a9\ud558\uc5ec \uc2dc\uac04\uc801 \ubc0f \uc704\uce58\uc801 \ud2b9\uc9d5\uacfc\uc758 \uc0c1\ud638\uc791\uc6a9\uc744 \uac15\ud654\ud558\uace0, \ub2e4\ub2e8\uacc4 \uc804\uc1a1-\uc9c4\ucd9c \ub124\ud2b8\uc6cc\ud06c(Multi-stage Feed-Forward Networks, MFFNs)\ub97c \ud1b5\ud574 \uc554\ubb35\uc801\uc778 \uc0c1\ud638\uc791\uc6a9\uc744 \ucd09\uc9c4\ud569\ub2c8\ub2e4. \uc624\ud504\ub77c\uc778 \ubc0f \uc628\ub77c\uc778 A/B \uc2e4\ud5d8\uc744 \ud1b5\ud574 FuXi-\ud835\udefc\uac00 \uae30\uc874 \ubaa8\ub378\ub4e4\uc744 \uc77c\uad00\ub418\uac8c \ub2a5\uac00\ud568\uc744 \ubcf4\uc5ec\uc8fc\uba70, \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \ud6a8\uacfc\ub97c \uc785\uc99d\ud569\ub2c8\ub2e4. \ub610\ud55c \ubaa8\ub378\uc744 \ud655\uc7a5\ud568\uc5d0 \ub530\ub77c \uc131\ub2a5\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c \ud5a5\uc0c1\ub428\uc73c\ub85c\uc368 \ub300\uaddc\ubaa8 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc5d0 \ub300\ud55c \uc7a0\uc7ac\ub825\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \ud5a5\ud6c4 \uc5f0\uad6c\uc5d0\uc11c\ub294 \ub2e4\uc911 \ud589\ub3d9(multi-behavior) \ubc0f \ub2e4\uc911 \ubaa8\ub2ec(multi-modal) \ucd94\ucc9c\uacfc \uac19\uc740 \ubcf4\ub2e4 \ubcf5\uc7a1\ud55c \ucd94\ucc9c \ubb38\uc81c\uc5d0 FuXi-\ud835\udefc\ub97c \uc801\uc6a9\ud558\uace0, \uae34 \uc2dc\ud000\uc2a4\ub97c \ud3ec\ud568\ud558\ub294 \uc2dc\ub098\ub9ac\uc624\uc5d0 \uc801\uc6a9\ud560 \uacc4\ud68d\uc785\ub2c8\ub2e4."
        },
        {
          "name": "References",
          "original": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and\nAdnan Aziz. 2022. Understanding scaling laws for recommendation models.\narXiv preprint arXiv:2208.08489 (2022).\n[3] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165 (2020).\n[4] Junyi Chen, Lu Chi, Bingyue Peng, and Zehuan Yuan. 2024. HLLM: Enhancing\nSequential Recommendations via Hierarchical Large Language Models for Item\nand User Modeling. arXiv preprint arXiv:2409.12740 (2024).\n[5] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and\nHongyuan Zha. 2018. Sequential recommendation with user memory networks.\nIn Proceedings of the eleventh ACM international conference on web search and\ndata mining. 108\u2013116.\n[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan\nAnil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.\n2016. Wide & Deep Learning for Recommender Systems. arXiv:1606.07792 [cs.LG]\nhttps://arxiv.org/abs/1606.07792\n[7] Sharad Chitlangia, Krishna Reddy Kesari, and Rajat Agarwal. 2023. Scaling\ngenerative pre-training for user ad activity sequences. (2023).\n[8] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks\nfor youtube recommendations. In Proceedings of the 10th ACM conference on\nrecommender systems. 191\u2013198.\n[9] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek,\nJustin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao\nWang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj\nKumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran,\nFisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier,\nAlexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas\nMensink, Alexander Kolesnikov, Filip Paveti\u0107, Dustin Tran, Thomas Kipf, Mario\nLu\u010di\u0107, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. 2023.\nScaling Vision Transformers to 22 Billion Parameters. arXiv:2302.05442 [cs.CV]\nhttps://arxiv.org/abs/2302.05442\n[10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,\net al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).\n[11] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units\nfor neural network function approximation in reinforcement learning. Neural\nnetworks 107 (2018), 3\u201311.\n[12] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin.\n2017. Convolutional sequence to sequence learning. In International conference\non machine learning. PMLR, 1243\u20131252.\n[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction.\narXiv:1703.04247 [cs.IR] https://arxiv.org/abs/1703.04247\n[14] Wei Guo, Hao Wang, Luankang Zhang, Jin Yao Chin, Zhongzhou Liu, Kai\nCheng, Qiushi Pan, Yi Quan Lee, Wanqi Xue, Tingjia Shen, et al . 2024. Scal-\ning New Frontiers: Insights into Large Recommendation Models. arXiv preprint\narXiv:2412.00714 (2024).\n[15] Xingzhuo Guo, Junwei Pan, Ximei Wang, Baixu Chen, Jie Jiang, and Mingsheng\nLong. 2023. On the Embedding Collapse when Scaling up Recommendation\nModels. arXiv preprint arXiv:2310.04400 (2023).\n[16] Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu,\nDefu Lian, and Enhong Chen. 2024. Efficient Noise-Decoupling for Multi-Behavior\nSequential Recommendation. In Proceedings of the ACM on Web Conference 2024 .\n3297\u20133306.\n[17] Yongqiang Han, Likang Wu, Hao Wang, Guifeng Wang, Mengdi Zhang, Zhi\nLi, Defu Lian, and Enhong Chen. 2023. Guesr: A global unsupervised data-\nenhancement with bucket-cluster sampling for sequential recommendation. In\nInternational Conference on Database Systems for Advanced Applications . Springer,\n286\u2013296.\n[18] Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse\nPredictive Analytics. arXiv:1708.05027 [cs.LG] https://arxiv.org/abs/1708.05027\n[19] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Ja-\ncob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al .\n2020. Scaling laws for autoregressive generative modeling. arXiv preprint\narXiv:2010.14701 (2020).\n[20] B Hidasi. 2015. Session-based Recommendations with Recurrent Neural Networks.\narXiv preprint arXiv:1511.06939 (2015).\n[21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al. 2022. Training compute-optimal large language models.\narXiv preprint arXiv:2203.15556 (2022).\n[22] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang.\n2018. Improving sequential recommendation with knowledge-enhanced mem-\nory networks. In The 41st international ACM SIGIR conference on research &\ndevelopment in information retrieval. 505\u2013514.\n[23] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE,\n197\u2013206.\n[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n[25] Anton Klenitskiy and Alexey Vasilev. 2023. Turning Dross Into Gold Loss: is\nBERT4Rec really better than SASRec?. In Proceedings of the 17th ACM Conference\non Recommender Systems (RecSys \u201923). ACM, 1120\u20131125. https://doi.org/10.1145/\n3604915.3610644\n[26] John P Kotter. 2012. Accelerate. Harvard business review 90, 11 (2012), 45\u201358.\n[27] Honghao Li, Yiwen Zhang, Yi Zhang, Hanwei Li, Lei Sang, and Jieming Zhu.\n2024. DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction.\narXiv:2407.13349 [cs.IR] https://arxiv.org/abs/2407.13349\n[28] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural attentive session-based recommendation. In Proceedings of the 2017 ACM\non Conference on Information and Knowledge Management . 1419\u20131428.\n[29] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware self-\nattention for sequential recommendation. In Proceedings of the 13th international\nconference on web search and data mining . 322\u2013330.\n[30] Li Li, Jiawei Peng, Huiyi Chen, Chongyang Gao, and Xu Yang. 2024. How to\nconfigure good in-context sequence for visual question answering. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 26710\u2013\n26720.\n[31] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable\nclick-through rate prediction through hierarchical attention. In Proceedings of\nthe 13th international conference on web search and data mining . 313\u2013321.\n[32] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn:\nModeling feature interactions via graph neural networks for ctr prediction. In\nProceedings of the 28th ACM international conference on information and knowledge\nmanagement. 539\u2013548.\n[33] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\nGuangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-\nteractions for recommender systems. In Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery & data mining . 1754\u20131763.\n[34] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang.\n2019. Feature generation by convolutional neural network for click-through rate\nprediction. In The World Wide Web Conference. 1119\u20131129.\n[35] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A convolutional click\nprediction model. In Proceedings of the 24th ACM international on conference on\ninformation and knowledge management . 1743\u20131746.\n[36] Weiwen Liu, Wei Guo, Yong Liu, Ruiming Tang, and Hao Wang. 2023. User\nBehavior Modeling with Deep Learning for Recommendation: Recent Advances.\nIn Proceedings of the 17th ACM Conference on Recommender Systems . 1286\u20131287.\n[37] Zihan Liu, Yupeng Hou, and Julian McAuley. 2024. Multi-Behavior Generative\nRecommendation. arXiv preprint arXiv:2405.16871 (2024).\n[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library. Advances\n9\n\nWWW \u201925, 28 April - 2 May, 2025, Sydney, Australia Ye and Guo, et al.\nin neural information processing systems 32 (2019).\n[39] Yingzhe Peng, Xinting Hu, Jiawei Peng, Xin Geng, Xu Yang, et al. [n. d.]. LIVE:\nLearnable In-Context Vector for Visual Question Answering. InThe Thirty-eighth\nAnnual Conference on Neural Information Processing Systems .\n[40] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun\nWang. 2016. Product-based Neural Networks for User Response Prediction.\narXiv:1611.00144 [cs.LG] https://arxiv.org/abs/1611.00144\n[41] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng\nGuo, Yong Yu, and Xiuqiang He. 2018. Product-based Neural Networks for User\nResponse Prediction over Multi-field Categorical Data. arXiv:1807.00311 [cs.IR]\nhttps://arxiv.org/abs/1807.00311\n[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. Journal of machine\nlearning research 21, 140 (2020), 1\u201367.\n[43] Steffen Rendle. 2010. Factorization machines. In2010 IEEE International conference\non data mining. IEEE, 995\u20131000.\n[44] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint\narXiv:1205.2618 (2012).\n[45] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-\nizing personalized markov chains for next-basket recommendation. InProceedings\nof the 19th international conference on World wide web . 811\u2013820.\n[46] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint\narXiv:2002.05202 (2020).\n[47] Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng\nGuo, Defu Lian, Ruiming Tang, and Enhong Chen. 2024. Predictive Models in\nSequential Recommendations: Bridging Performance Laws with Data Quality\nInsights. arXiv preprint arXiv:2412.00430 (2024).\n[48] Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen,\nDefu Lian, and Enhong Chen. 2024. Exploring User Retrieval Integration towards\nLarge Language Models for Cross-Domain Sequential Recommendation. arXiv\npreprint arXiv:2406.03085 (2024).\n[49] Kyuyong Shin, Hanock Kwak, Su Young Kim, Max Nihl\u00e9n Ramstr\u00f6m, Jisu Jeong,\nJung-Woo Ha, and Kyung-Min Kim. 2023. Scaling law for recommendation\nmodels: Towards general-purpose user representations. In Proceedings of the\nAAAI Conference on Artificial Intelligence, Vol. 37. 4596\u20134604.\n[50] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,\nand Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-\nattentive neural networks. InProceedings of the 28th ACM international conference\non information and knowledge management . 1161\u20131170.\n[51] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\nresentations from transformer. In Proceedings of the 28th ACM international\nconference on information and knowledge management . 1441\u20131450.\n[52] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-\ntion via convolutional sequence embedding. In Proceedings of the eleventh ACM\ninternational conference on web search and data mining . 565\u2013573.\n[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[54] A Vaswani. 2017. Attention is all you need. Advances in Neural Information\nProcessing Systems (2017).\n[55] Hao Wang, Defu Lian, Hanghang Tong, Qi Liu, Zhenya Huang, and Enhong\nChen. 2021. Hypersorec: Exploiting hyperbolic user and item representations\nwith multiple aspects for social-aware recommendation. ACM Transactions on\nInformation Systems (TOIS) 40, 2 (2021), 1\u201328.\n[56] Hao Wang, Tong Xu, Qi Liu, Defu Lian, Enhong Chen, Dongfang Du, Han Wu,\nand Wen Su. 2019. MCNE: An end-to-end framework for learning multiple\nconditional network representations of social network. In Proceedings of the 25th\nACM SIGKDD international conference on knowledge discovery & data mining .\n1064\u20131072.\n[57] Hao Wang, Mingjia Yin, Luankang Zhang, Sirui Zhao, and Enhong Chen. [n. d.].\nMF-GSLAE: A Multi-Factor User Representation Pre-training Framework for\nDual-Target Cross-Domain Recommendation. ACM Transactions on Information\nSystems ([n. d.]).\n[58] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network\nfor ad click predictions. In Proceedings of the ADKDD\u201917. 1\u20137.\n[59] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,\nand Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons\nfor web-scale learning to rank systems. In Proceedings of the web conference 2021 .\n1785\u20131797.\n[60] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al . 2024. A survey on large\nlanguage models for recommendation. World Wide Web 27, 5 (2024), 60.\n[61] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based recommendation with graph neural networks. In Proceedings of\nthe AAAI conference on artificial intelligence , Vol. 33. 346\u2013353.\n[62] Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, and Enhong Chen.\n2024. Breaking Determinism: Fuzzy Modeling of Sequential Recommendation\nUsing Discrete State Space Diffusion Model. arXiv preprint arXiv:2410.23994\n(2024).\n[63] Wenjia Xie, Rui Zhou, Hao Wang, Tingjia Shen, and Enhong Chen. 2024. Bridging\nUser Dynamics: Transforming Sequential Recommendations with Schr\u00f6dinger\nBridge and Diffusion Models. In Proceedings of the 33rd ACM International Con-\nference on Information and Knowledge Management . 2618\u20132628.\n[64] Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong\nYu, Yong Liu, Defu Lian, and Enhong Chen. 2024. Multi-granularity Interest\nRetrieval and Refinement Network for Long-Term User Behavior Modeling in\nCTR Prediction. arXiv preprint arXiv:2411.15005 (2024).\n[65] Xu Yang, Yingzhe Peng, Haoxuan Ma, Shuo Xu, Chi Zhang, Yucheng Han, and\nHanwang Zhang. 2023. Lever LM: configuring in-context sequence to lever\nlarge vision language models. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems.\n[66] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. 2024.\nExploring diverse in-context configurations for image captioning. Advances in\nNeural Information Processing Systems 36 (2024).\n[67] Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Zhi Li, Sirui Zhao, Zhen Wang, Defu\nLian, and Enhong Chen. 2024. Learning Partially Aligned Item Representation\nfor Cross-Domain Sequential Recommendation. arXiv preprint arXiv:2405.12473\n(2024).\n[68] Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian,\nand Enhong Chen. 2024. Dataset regeneration for sequential recommendation.\nIn Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining. 3954\u20133965.\n[69] Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong\nLiu, Ruiming Tang, Defu Lian, and Enhong Chen. 2023. APGL4SR: A Generic\nFramework with Adaptive and Personalized Global Collaborative Information\nin Sequential Recommendation. In Proceedings of the 32nd ACM International\nConference on Information and Knowledge Management . 3009\u20133019.\n[70] Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong\nLiu, Ruiming Tang, Defu Lian, and Enhong Chen. 2024. Entropy law: The story\nbehind data compression and llm performance. arXiv preprint arXiv:2407.06645\n(2024).\n[71] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhao-\njie Gong, Fangda Gu, Michael He, et al. 2024. Actions speak louder than words:\nTrillion-parameter sequential transducers for generative recommendations.arXiv\npreprint arXiv:2402.17152 (2024).\n[72] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhao-\njie Gong, Fangda Gu, Michael He, et al. 2024. Actions speak louder than words:\nTrillion-parameter sequential transducers for generative recommendations.arXiv\npreprint arXiv:2402.17152 (2024).\n[73] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022. Scaling\nVision Transformers. arXiv:2106.04560 [cs.CV] https://arxiv.org/abs/2106.04560\n[74] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022. Scaling\nvision transformers. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition. 12104\u201312113.\n[75] Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normaliza-\ntion. In Advances in Neural Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems 2019, NeurIPS 2019, Decem-\nber 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach, Hugo Larochelle,\nAlina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Gar-\nnett (Eds.). 12360\u201312371. https://proceedings.neurips.cc/paper/2019/hash/\n1e8a19426224ca89e83cef47f1e7f53b-Abstract.html\n[76] Gaowei Zhang, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and Ji-Rong\nWen. 2023. Scaling Law of Large Sequential Recommendation Models. arXiv\npreprint arXiv:2311.11351 (2023).\n[77] Luankang Zhang, Hao Wang, Suojuan Zhang, Mingjia Yin, Yongqiang Han,\nJiaqing Zhang, Defu Lian, and Enhong Chen. 2024. A Unified Framework for\nAdaptive Representation Enhancement and Inversed Learning in Cross-Domain\nRecommendation. arXiv preprint arXiv:2404.00268 (2024).\n[78] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep Learning over Multi-\nfield Categorical Data: \u2013A Case Study on User Response Prediction. In Advances\nin Information Retrieval: 38th European Conference on IR Research, ECIR 2016,\nPadua, Italy, March 20\u201323, 2016. Proceedings 38 . Springer, 45\u201357.\n[79] Xikun Zhang, Dongjin Song, Yushan Jiang, Yixin Chen, and Dacheng Tao. 2024.\nLearning System Dynamics without Forgetting. arXiv preprint arXiv:2407.00717\n(2024).\n[80] Xikun Zhang, Dongjin Song, and Dacheng Tao. 2022. Cglb: Benchmark tasks for\ncontinual graph learning. Advances in Neural Information Processing Systems 35\n(2022), 13006\u201313021.\n[81] Xikun Zhang, Dongjin Song, and Dacheng Tao. 2022. Hierarchical prototype\nnetworks for continual graph representation learning. IEEE Transactions on\nPattern Analysis and Machine Intelligence 45, 4 (2022), 4622\u20134636.\n10\n\nFuXi-\ud835\udefc: Scaling Recommendation Model with Feature Interaction Enhanced Transformer WWW \u201925, 28 April - 2 May, 2025, Sydney, Australia\n[82] Xikun Zhang, Chang Xu, and Dacheng Tao. 2020. Context aware graph con-\nvolution for skeleton-based action recognition. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition . 14333\u201314342.\n[83] Xikun Zhang, Chang Xu, Xinmei Tian, and Dacheng Tao. 2019. Graph edge convo-\nlutional neural networks for skeleton-based action recognition. IEEE transactions\non neural networks and learning systems 31, 8 (2019), 3047\u20133060.\n[84] Yuren Zhang, Enhong Chen, Binbin Jin, Hao Wang, Min Hou, Wei Huang, and\nRunlong Yu. 2022. Clustering based behavior sampling with long sequential data\nfor CTR prediction. In Proceedings of the 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval . 2195\u20132200.\n11",
          "translated": "[\ucc38\uace0\ubb38\ud5cc \uc0dd\ub7b5]"
        }
      ]
    },
    {
      "id": "e21a2e83-325b-4b34-bf15-d4acd105742b",
      "title": "Rethinking Overconfidence in VAEs: Can Label Smoothing Help?",
      "authors": [
        "Woo-Seong Yun",
        "Yeojun Choi",
        "Yoonsik Cho"
      ],
      "abstract": "By leveraging the expressive power of deep generative models, Variational Autoencoder (VAE)-based recommender models have demonstrated competitive performance. However, deep neural networks (DNNs) tend to exhibit overconfidence in their predictive distributions as training progresses. This issue is further exacerbated by two inherent characteristics of collaborative filtering (CF): (1) extreme data sparsity and (2) implicit feedback. Despite its importance, there has been a lack of systematic study into this problem. To fill the gap, this paper explores the above limitations with label smoothing (LS) from both theoretical and empirical aspects. Our extensive analysis demonstrates that overconfidence leads to embedding collapse, where latent representations collapse into a narrow subspace. Furthermore, we investigate the conditions under which LS helps recommendation, and observe that the optimal LS factor decreases proportionally with data sparsity. To the best of our knowledge, this is the first study in VAE-based CF that discovers the relationship between overconfidence and embedding collapse, and highlights the necessity of explicitly addressing them. Our code is available at https://github.com/yunwooseong/RethinkVAE.",
      "year": 2025,
      "arxiv_id": null,
      "arxiv_url": null,
      "doi": "10.1145/3705328.3748039",
      "paper_url": "https://dl.acm.org/doi/10.1145/3705328.3748039",
      "category": "recsys",
      "tags": [
        {
          "id": "3e36ba36-2a7d-4cb5-9b57-83f4e1cee79a",
          "name": "VAE"
        }
      ],
      "published_at": "2025-09-22",
      "created_at": "2026-01-29T18:17:24.423618",
      "updated_at": "2026-01-29T18:17:24.423618",
      "conference": "RecSys'25"
    },
    {
      "id": "3b59362b-ee9a-43db-8384-8ad5564f013a",
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": [
        "Shunyu Yao",
        "Dian Yu",
        "Jeffrey Zhao",
        "Izhak Shafran",
        "Thomas L. Griffiths",
        "Yuan Cao",
        "Karthik Narasimhan"
      ],
      "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
      "year": 2023,
      "arxiv_id": "2305.10601",
      "arxiv_url": "https://arxiv.org/abs/2305.10601",
      "category": "recsys",
      "tags": [
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2023-05-17",
      "created_at": "2026-01-29T18:17:44.088329",
      "updated_at": "2026-01-29T18:17:44.088329",
      "conference": "NeurIPS'23"
    },
    {
      "id": "1290e19a-7664-4a23-8abb-c9d7d0d26d2c",
      "title": "FuXi-$\u03b3$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism",
      "authors": [
        "Dezhi Yi",
        "Wei Guo",
        "Wenyang Cui",
        "Wenxuan He",
        "Huifeng Guo",
        "Yong Liu",
        "Zhenhua Dong",
        "Ye Lu"
      ],
      "abstract": "Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$\u03b3$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$\u03b3$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$\u03b3$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\\times$ and inference by up to 6.18$\\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.",
      "year": 2025,
      "arxiv_id": "2512.12740",
      "arxiv_url": "https://arxiv.org/abs/2512.12740",
      "category": "recsys",
      "tags": [
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-12-14",
      "created_at": "2026-01-29T18:20:45.141059",
      "updated_at": "2026-01-30T14:38:25.687459",
      "conference": null,
      "summary": {
        "one_line": "FuXi-$\u03b3$ presents a computationally efficient sequential recommendation framework leveraging an exponential-power temporal encoder and diagonal-sparse positional mechanism for improved performance and scalability.",
        "contribution": "This work introduces a novel architecture for sequential recommendation that addresses the computational bottlenecks of traditional Transformer models. Specifically, FuXi-$\u03b3$ employs an exponential-power temporal encoder to effectively model temporal dependencies and a diagonal-sparse positional mechanism to reduce computational complexity.",
        "methodology": "The framework utilizes a decoder-only Transformer structure with an exponential-power temporal encoder, which encodes relative temporal intervals using a tunable decay function. Furthermore, a diagonal-sparse positional mechanism prunes attention blocks via a diagonal-sliding strategy based on Toeplitz matrix persymmetry, optimizing memory access.",
        "results": "Experiments on multiple datasets demonstrate FuXi-$\u03b3$'s state-of-the-art recommendation quality, achieving significant speedups in training (up to 4.74x) and inference (up to 6.18x), highlighting its practical scalability."
      },
      "translation": {
        "title": "\uc81c\ubaa9: FuXi-$\u03b3$: \uc5d1\uc2a4\ud3ec\ub10c\ud2b8-\ud30c\uc6cc \uc2dc\uac04 \uc778\ucf54\ub354\uc640 \ub300\uac01-\ud76c\uc18c \uc704\uce58 \uba54\ucee4\ub2c8\uc998\uc744 \uc774\uc6a9\ud55c \ud6a8\uc728\uc801\uc778 \uc21c\ucc28 \ucd94\ucc9c",
        "abstract": "\ucd08\ub85d: \uc21c\ucc28 \ucd94\ucc9c\uc740 \uc0ac\uc6a9\uc790\uc758 \uc5ed\uc0ac\ub97c \uae30\ubc18\uc73c\ub85c \uc9c4\ud654\ud558\ub294 \uc120\ud638\ub3c4\ub97c \ubaa8\ub378\ub9c1\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \ucd5c\uadfc\uc758 \ubc1c\uc804\uc740 \uc804\uc5ed \uc758\uc874\uc131\uc744 \ud3ec\ucc29\ud558\uae30 \uc704\ud574 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \uc544\ud0a4\ud14d\ucc98\ub97c \ud65c\uc6a9\ud558\uc9c0\ub9cc, \uae30\uc874 \ubc29\ubc95\uc740 \uc2dc\uac04 \uc778\ucf54\ub529\uc758 \ubd88\uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \uc811\uadfc\uacfc \uae34 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud55c \ube7d\ube7d\ud55c \uc5b4\ud150\uc158\uc73c\ub85c \uc778\ud574 \ub192\uc740 \uacc4\uc0b0 \uc624\ubc84\ud5e4\ub4dc\ub97c \uacaa\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc81c\ud55c \uc0ac\ud56d\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \uc6d0\uce59\uc5d0 \uae30\ubc18\ud55c \uc544\ud0a4\ud14d\ucc98 \uc124\uacc4\ub97c \ud1b5\ud574 \ud6a8\uacfc\uc640 \ud6a8\uc728\uc131\uc744 \ubaa8\ub450 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc0c8\ub85c\uc6b4 \uc21c\ucc28 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\uc778 FuXi-$\u03b3$\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4. FuXi-$\u03b3$\ub294 \ub514\ucf54\ub354\ub9cc \uc0ac\uc6a9\ud558\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uad6c\uc870\ub97c \ucc44\ud0dd\ud558\uace0 \ub450 \uac00\uc9c0 \uc8fc\uc694 \ud601\uc2e0\uc744 \ub3c4\uc785\ud569\ub2c8\ub2e4. (1) Ebbinghaus \ub9dd\uac01 \uace1\uc120\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc740 \uc870\uc815 \uac00\ub2a5\ud55c \uc9c0\uc218 \uac10\uc1e0 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0c1\ub300\uc801\uc778 \uc2dc\uac04 \uac04\uaca9\uc744 \uc778\ucf54\ub529\ud558\ub294 \uc5d1\uc2a4\ud3ec\ub10c\ud2b8-\ud30c\uc6cc \uc2dc\uac04 \uc778\ucf54\ub354\uc785\ub2c8\ub2e4. \uc774 \uc778\ucf54\ub354\ub294 \ub192\uc740 \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ub2e8\uae30 \ubc0f \uc7a5\uae30 \uc120\ud638\ub3c4\ub97c \uc720\uc5f0\ud558\uac8c \ubaa8\ub378\ub9c1\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub294 \uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \uc811\uadfc\uacfc \uc21c\uc218\ud55c \ud589\ub82c \uc5f0\uc0b0\uc744 \ud1b5\ud574 \uac00\ub2a5\ud569\ub2c8\ub2e4. (2) Toeplitz \ud589\ub82c\uc758 \ub300\uce6d\uc131\uc744 \uae30\ubc18\uc73c\ub85c \ub0ae\uc740 \uae30\uc5ec\ub3c4\ub97c \uac00\uc9c4 \uc5b4\ud150\uc158 \ube14\ub85d\uc744 \uac00\uc9c0\uce58\uae30\ud558\ub294 \ub300\uac01-\ud76c\uc18c \uc704\uce58 \uba54\ucee4\ub2c8\uc998\uc785\ub2c8\ub2e4. \ub124 \uac1c\uc758 \uc2e4\uc81c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc, FuXi-$\u03b3$\ub294 \ucd94\ucc9c \ud488\uc9c8\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\ub294 \ub3d9\uc2dc\uc5d0 \ud559\uc2b5 \uc2dc\uac04\uc744 \ucd5c\ub300 4.74\ubc30 \ubc0f \ucd94\ub860 \uc2dc\uac04\uc744 \ucd5c\ub300 6.18\ubc30 \uac00\uc18d\ud654\ud558\uc5ec \uae34 \uc2dc\ud000\uc2a4 \ucd94\ucc9c\uc5d0 \ub300\ud55c \uc2e4\uc6a9\uc801\uc774\uace0 \ud655\uc7a5 \uac00\ub2a5\ud55c \uc194\ub8e8\uc158\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \ucf54\ub4dc\ub294 https://github.com/Yeedzhi/FuXi-gamma \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      },
      "full_summary": "## FuXi-\ud835\udefe: \ud6a8\uc728\uc801\uc778 \uc21c\ucc28 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c \uc694\uc57d (\ud55c\uad6d\uc5b4)\n\n**1. \uc5f0\uad6c \ubc30\uacbd \ubc0f \ub3d9\uae30 (Introduction & Motivation)**\n\nFuXi-\ud835\udefe\ub294 \uc0ac\uc6a9\uc790\ub4e4\uc758 \uc9c4\ud654\ud558\ub294 \uc120\ud638\ub3c4\ub97c \ubaa8\ub378\ub9c1\ud558\uc5ec \uc21c\ucc28 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \ud6a8\uc728\uc131\uacfc \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \uae30\uc874\uc758 \uc21c\ucc28 \ucd94\ucc9c \ubaa8\ub378\ub4e4\uc740 \ubcf5\uc7a1\ud55c \uad6c\uc870\uc640 \ub192\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc73c\ub85c \uc778\ud574 \ud655\uc7a5\uc131\uc774 \ub5a8\uc5b4\uc9c0\uace0, \ud2b9\ud788 \uae34 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130 \ucc98\ub9ac \uc2dc \ube44\ud6a8\uc728\uc801\uc778 \uacbd\uc6b0\uac00 \ub9ce\uc558\uc2b5\ub2c8\ub2e4. FuXi-\ud835\udefe\ub294 \uc774\ub7ec\ud55c \ubb38\uc81c\uc810\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574, \uc0ac\uc6a9\uc790 \ud589\ub3d9 \ud328\ud134\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\uace0, \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \ud2b9\ud788, \uae34 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uace0, \uc2e4\uc6a9\uc801\uc778 \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \ub192\uc774\ub294 \ub370 \uc911\uc810\uc744 \ub450\uace0 \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n**2. \ud575\uc2ec \uad6c\uc131 \uc694\uc18c**\n\nFuXi-\ud835\udefe\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.\n\n*   **\uc2dc\uac04 \uc778\ucf54\ub354 (Temporal Encoder):** Ebbinghaus \ub9dd\uac01 \uace1\uc120\uc744 \uae30\ubc18\uc73c\ub85c \uc124\uacc4\ub418\uc5b4, \uc0ac\uc6a9\uc790 \uc120\ud638\ub3c4 \ubcc0\ud654\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4.  \uc774\ub97c \ud1b5\ud574 \uc2dc\uac04 \uc815\ubcf4\uc5d0 \ub530\ub978 \uc0ac\uc6a9\uc790 \ud589\ub3d9 \ud328\ud134\uc744 \uc815\ud655\ud558\uac8c \ud30c\uc545\ud558\uace0,  \ub2e4\uc591\ud55c \uc0ac\uc6a9\uc790 \ud589\ub3d9 \ud328\ud134\uc5d0 \uc801\uc751\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n*   **\uc704\uce58 \uc778\ucf54\ub354 (Positional Encoder):**  \uc0c1\ub300\uc801 \uc704\uce58 \uc815\ubcf4\ub97c \ud65c\uc6a9\ud558\uc5ec,  \uac01 \ud56d\ubaa9 \uac04\uc758 \uad00\uacc4\ub97c \ud559\uc2b5\ud569\ub2c8\ub2e4.  \uc774\ub294 \ud2b9\ud788,  \uae34 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc5d0\uc11c \ud56d\ubaa9 \uac04\uc758 \uc21c\uc11c\uc801 \uad00\uacc4\ub97c \ud30c\uc545\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.\n*   **Transformer \ube14\ub85d:**  \uae30\uc874\uc758 Transformer \uad6c\uc870\ub97c \ud65c\uc6a9\ud558\uc5ec,  \ubaa8\ub378\uc758 \ud45c\ud604 \ub2a5\ub825\uc744 \uadf9\ub300\ud654\ud569\ub2c8\ub2e4.\n\n**3. \uc8fc\uc694 \uae30\uc5ec \ubc0f \uacb0\uacfc**\n\n*   **\uc0c8\ub85c\uc6b4 \ud504\ub808\uc784\uc6cc\ud06c \uc124\uacc4:** FuXi-\ud835\udefe\ub294 \uc0ac\uc6a9\uc790 \uc120\ud638\ub3c4 \ubaa8\ub378\ub9c1\uacfc \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \ub3d9\uc2dc\uc5d0 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc0c8\ub85c\uc6b4 \uc21c\ucc28 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4.\n*   **\uc2dc\uac04 \uc778\ucf54\ub354\uc758 \ud601\uc2e0\uc801\uc778 \uc124\uacc4:** Ebbinghaus \ub9dd\uac01 \uace1\uc120\uc744 \uae30\ubc18\uc73c\ub85c \ud55c \uc2dc\uac04 \uc778\ucf54\ub354\ub294 \uc0ac\uc6a9\uc790 \ud589\ub3d9 \ud328\ud134\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4.\n*   **\uc704\uce58 \uc778\ucf54\ub354\uc758 \ud6a8\uc728\uc801\uc778 \ud65c\uc6a9:**  \uc0c1\ub300\uc801 \uc704\uce58 \uc815\ubcf4\ub97c \ud65c\uc6a9\ud558\uc5ec,  \uae34 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130 \ucc98\ub9ac\uc758 \ud6a8\uc728\uc131\uc744 \ub192\uc785\ub2c8\ub2e4.\n*   **\uc2e4\ud5d8\uc801 \uac80\uc99d:**  \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc2e4\ud5d8\uc744 \ud1b5\ud574, FuXi-\ud835\udefe\uac00 \uae30\uc874 \ubaa8\ub378 \ub300\ube44 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788,  \ub300\uaddc\ubaa8 \uc74c\uc545 \ub370\uc774\ud130\uc14b\uc5d0\uc11c FuXi-\ud835\udefe\ub294 HR@10\uc5d0\uc11c 25.06%\uc758 \uac1c\uc120 \ud6a8\uacfc\uc640 NDCG@10\uc5d0\uc11c 42.86%\uc758 \uac1c\uc120 \ud6a8\uacfc\ub97c \ubcf4\uc600\uc2b5\ub2c8\ub2e4.\n\n**\uc694\uc57d:** FuXi-\ud835\udefe\ub294 \ud6a8\uc728\uc801\uc778 \uc21c\ucc28 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc744 \uc704\ud55c \ud601\uc2e0\uc801\uc778 \ud504\ub808\uc784\uc6cc\ud06c\ub85c\uc11c, \uc0ac\uc6a9\uc790 \uc120\ud638\ub3c4 \ubaa8\ub378\ub9c1\uc758 \uc815\ud655\uc131\uacfc \uc2e4\uc6a9\uc131\uc744 \ub3d9\uc2dc\uc5d0 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uae30\uc5ec\ud560 \uac83\uc73c\ub85c \uae30\ub300\ub429\ub2c8\ub2e4.",
      "full_translation": [
        {
          "name": "Abstract",
          "original": "FuXi-\ud835\udefe: Efficient Sequential Recommendation with\nExponential-Power Temporal Encoder and\nDiagonal-Sparse Positional Mechanism\nDezhi Yi\ndezhi.yi@mail.nankai.edu.cn\nCollege of Computer Science, DISSec\nNankai University\nTianjin, China\nWei Guo\nguowei67@huawei.com\nHuawei Technologies\nShanghai, China\nWenyang Cui\ncuiwenyang@mail.nankai.edu.cn\nCollege of Computer Science, DISSec\nNankai University\nTianjin, China\nWenxuan He\nwxhe@mail.nankai.edu.cn\nCollege of Computer Science, DISSec\nNankai University\nTianjin, China\nHuifeng Guo\nhuifeng.guo@huawei.com\nHuawei Technologies\nShenzhen, China\nYong Liu\nliu.yong6@huawei.com\nHuawei Technologies\nShanghai, China\nZhenhua Dong\ndongzhenhua@huawei.com\nHuawei Technologies\nShenzhen, China\nYe Lu\u2217\nluye@nankai.edu.cn\nCollege of Computer Science, DISSec\nNankai University\nTianjin, China",
          "translated": "**\ucd08\ub85d**\n\nFuXi-\ud835\udefe: \uc9c0\uc218-\uc804\ub825 \uc2dc\uacc4\uc5f4 \uc778\ucf54\ub354\uc640 \ub300\uac01-\ud76c\uc18c \uc704\uce58 \uba54\ucee4\ub2c8\uc998\uc744 \uc774\uc6a9\ud55c \ud6a8\uc728\uc801\uc778 \uc21c\ucc28 \ucd94\ucc9c\n\nDezhi Yi\ndezhi.yi@mail.nankai.edu.cn\nNankai \ub300\ud559\uad50 \ucef4\ud4e8\ud130\uacfc\ud559\ubd80, DISSec\n\nWei Guo\nguowei67@huawei.com\nHuawei Technologies\n\nWenyang Cui\ncuiwenyang@mail.nankai.edu.cn\nNankai \ub300\ud559\uad50 \ucef4\ud4e8\ud130\uacfc\ud559\ubd80, DISSec\n\nWenxuan He\nwxhe@mail.nankai.edu.cn\nNankai \ub300\ud559\uad50 \ucef4\ud4e8\ud130\uacfc\ud559\ubd80, DISSec\n\nHuifeng Guo\nhuifeng.guo@huawei.com\nHuawei Technologies\n\nYong Liu\nliu.yong6@huawei.com\nHuawei Technologies\n\nZhenhua Dong\ndongzhenhua@huawei.com\nHuawei Technologies\n\nYe Lu\u2217\nluye@nankai.edu.cn\nNankai \ub300\ud559\uad50 \ucef4\ud4e8\ud130\uacfc\ud559\ubd80, DISSec"
        },
        {
          "name": "Abstract",
          "original": "Sequential recommendation aims to model users\u2019 evolving pref-\nerences based on their historical interactions. Recent advances\nleverage Transformer-based architectures to capture global depen-\ndencies, but existing methods often suffer from high computational\noverhead, primarily due to discontinuous memory access in tempo-\nral encoding and dense attention over long sequences. To address\nthese limitations, we propose FuXi-\ud835\udefe, a novel sequential recommen-\ndation framework that improves both effectiveness and efficiency\nthrough principled architectural design. FuXi-\ud835\udefe adopts a decoder-\nonly Transformer structure and introduces two key innovations:\n(1) An exponential-power temporal encoder that encodes relative\ntemporal intervals using a tunable exponential decay function in-\nspired by the Ebbinghaus forgetting curve. This encoder enables\nflexible modeling of both short-term and long-term preferences\nwhile maintaining high efficiency through continuous memory ac-\ncess and pure matrix operations. (2) A diagonal-sparse positional\nmechanism that prunes low-contribution attention blocks using a\ndiagonal-sliding strategy guided by the persymmetry of Toeplitz\nmatrix. Extensive experiments on four real-world datasets demon-\nstrate that FuXi-\ud835\udefe achieves state-of-the-art performance in recom-\nmendation quality, while accelerating training by up to4.74 \u00d7 and\n\u2217Corresponding author.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nKDD \u201926, Jeju Island, Republic of Korea\n\u00a92026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2258-5/2026/08\nhttps://doi.org/10.1145/3770854.3780176\ninference by up to6.18 \u00d7, making it a practical and scalable solu-\ntion for long-sequence recommendation. Our code is available at\nhttps://github.com/Yeedzhi/FuXi-gamma.\nCCS Concepts\n\u2022Information systems\u2192Recommender systems.\nKeywords\nSequential Recommendation, Temporal Encoder, Sparse Positional\nMechanism\nACM Reference Format:\nDezhi Yi, Wei Guo, Wenyang Cui, Wenxuan He, Huifeng Guo, Yong Liu,\nZhenhua Dong, and Ye Lu. 2026. FuXi-\ud835\udefe: Efficient Sequential Recommen-\ndation with Exponential-Power Temporal Encoder and Diagonal-Sparse\nPositional Mechanism. InProceedings of the 32nd ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining V.1 (KDD \u201926), August 09\u201313,\n2026, Jeju Island, Republic of Korea.ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3770854.3780176",
          "translated": "\ucd08\ub85d\n\n\uc21c\ucc28 \ucd94\ucc9c\uc740 \uc0ac\uc6a9\uc790\uc758 \uc9c4\ud654\ud558\ub294 \uc120\ud638\ub3c4\ub97c \uadf8\ub4e4\uc758 \uacfc\uac70 \uc0c1\ud638 \uc791\uc6a9\uc5d0 \uae30\ubc18\ud558\uc5ec \ubaa8\ub378\ub9c1\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \ucd5c\uadfc\uc758 \ubc1c\uc804\uc740 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uae30\ubc18 \uc544\ud0a4\ud14d\ucc98\ub97c \ud65c\uc6a9\ud558\uc5ec \uc804\uc5ed \uc758\uc874\uc131\uc744 \ud3ec\ucc29\ud558\uc9c0\ub9cc, \uae30\uc874 \ubc29\ubc95\uc740 \uc2dc\uac04\uc801 \uc778\ucf54\ub529\uc758 \ubd88\uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \uc811\uadfc\uacfc \uae34 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud55c \ub374\uc2a4 \uc5b4\ud150\uc158\uc73c\ub85c \uc778\ud574 \ub192\uc740 \uacc4\uc0b0 \uc624\ubc84\ud5e4\ub4dc\ub97c \uacaa\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc81c\ud55c \uc0ac\ud56d\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \ud6a8\uacfc\uc131\uacfc \ud6a8\uc728\uc131\uc744 \ubaa8\ub450 \uac1c\uc120\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc21c\ucc28 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\uc778 FuXi-\ud835\udefe\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4. FuXi-\ud835\udefe\ub294 \ub514\ucf54\ub354\ub9cc \uc0ac\uc6a9\ud558\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uad6c\uc870\ub97c \ucc44\ud0dd\ud558\uace0 \ub450 \uac00\uc9c0 \uc8fc\uc694 \ud601\uc2e0\uc744 \ub3c4\uc785\ud569\ub2c8\ub2e4. (1) \uc5d0\ube59\ud558\uc6b0\uc2a4 \ub9dd\uac01 \uace1\uc5d0 \uc601\uac10\uc744 \ubc1b\uc740 \uc870\uc815 \uac00\ub2a5\ud55c \uc9c0\uc218 \uac10\uc1e0 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0c1\ub300\uc801\uc778 \uc2dc\uac04 \uac04\uaca9\uc744 \uc778\ucf54\ub529\ud558\ub294 \uc9c0\uc218 \uc804\ub825 \uc2dc\uac04\uc801 \uc778\ucf54\ub354\ub97c \ud1b5\ud574 \ub2e8\uae30 \ubc0f \uc7a5\uae30 \uc120\ud638\ub3c4\ub97c \uc720\uc5f0\ud558\uac8c \ubaa8\ub378\ub9c1\ud558\uba74\uc11c \ub192\uc740 \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc774 \uc778\ucf54\ub354\ub294 \uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \uc811\uadfc\uacfc \uc21c\uc218\ud55c \ud589\ub82c \uc5f0\uc0b0\uc744 \ud1b5\ud574 \ub192\uc740 \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. (2) \ud1a0\ud50c\ub9ac\uce20 \ud589\ub82c\uc758 \ub300\uce6d\uc131\uc744 \uc548\ub0b4\ud558\ub294 \ub300\uac01-\ud76c\uc18c \uc704\uce58 \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub0ae\uc740 \uae30\uc5ec\ub3c4\ub97c \uac00\uc9c4 \uc5b4\ud150\uc158 \ube14\ub85d\uc744 \uac00\uc9c0\uce58\uae30\ud558\ub294 \ub300\uac01-\uc2ac\ub77c\uc774\ub529 \uc804\ub7b5\uc744 \ud1b5\ud574 \uc7a5\uc1c4\uc801 \ucd94\ucc9c\uc5d0 \ub300\ud55c \uc2e4\uc6a9\uc801\uc774\uace0 \ud655\uc7a5 \uac00\ub2a5\ud55c \uc194\ub8e8\uc158\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub124 \uac1c\uc758 \uc2e4\uc81c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uc218\ud589\ud55c \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc, FuXi-\ud835\udefe\uac00 \ucd94\ucc9c \ud488\uc9c8\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\ub294 \ub3d9\uc2dc\uc5d0 \ud6c8\ub828 \uc18d\ub3c4\ub97c \ucd5c\ub300 4.74 \u00d7 \uac00\uc18d\ud654\ud558\uace0 \ucd94\ub860 \uc18d\ub3c4\ub97c \ucd5c\ub300 6.18 \u00d7 \uac00\uc18d\ud654\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucf54\ub4dc\ub294 \ub2e4\uc74c\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: https://github.com/Yeedzhi/FuXi-gamma.\n\n\uac1c\ub150:\n\u2022 \uc815\ubcf4 \uc2dc\uc2a4\ud15c \u2192 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\n\n\ud0a4\uc6cc\ub4dc:\n\uc21c\ucc28\uc801 \uc778\ucf54\ub354, \ud76c\uc18c \uc704\uce58 \uba54\ucee4\ub2c8\uc998, \ucd94\ucc9c \uc2dc\uc2a4\ud15c\n\nACM \ucc38\uc870 \ud615\uc2dd:\nDezhi Yi, Wei Guo, Wenyang Cui, Wenxuan He, Huifeng Guo, Yong Liu, Zhenhua Dong, and Ye Lu. 2026. FuXi-\ud835\udefe: Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism with Efficient Sequential Recommendation. InProceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD \u201926), August 09\u201313, 2026, Jeju Island, Republic of Korea.ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3770854.3780176"
        },
        {
          "name": "Introduction",
          "original": "Sequential recommendation aims to predict users\u2019 next interaction\nbased on their historical behavior sequences [22, 43, 52]. Accurate\npredictions improve user experience and drive substantial com-\nmercial value [21, 32, 47]. The training efficiency of recommenda-\ntion models affects their update frequency and service freshness,\nwhile inference efficiency impacts system latency and deployment\ncost [29, 30, 46]. Hence, designing recommendation frameworks\nthat are both effective and efficient is a core challenge.\nRecent advances in generative recommendation models [1, 52,\n55] leverage autoregressive architectures to achieve superior perfor-\nmance and scaling effects compared to traditional methods [16, 22,\narXiv:2512.12740v1  [cs.IR]  14 Dec 2025\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al.\n43]. However, these models often introduce efficiency bottlenecks\ndue to architectural complexity. For example, although HSTU [55]\nimproves HR@10 by 55.18% over SASRec [ 22], it suffers from a\n3.21\u00d7 slowdown in training efficiency. FuXi-\ud835\udefc [52] further improves\nHR@10 by 5.66% over HSTU but incurs an additional 9.63% effi-\nciency loss. Such inefficiency forces production systems to reduce\nmodel size to meet latency requirements, weakening the scaling\nbenefits of the autoregressive paradigm.\nA primary efficiency bottleneck in sequential recommendation\nlies in temporal encoding module. Temporal information is essential\nfor capturing the dynamics of user preferences over time, which is\nfundamental to modeling interest patterns [2, 28, 51]. However, tem-\nporal encoders in existing state-of-the-art frameworks often conflict\nwith parallel hardware due to irregular and fragmented memory\naccess, severely limiting computational efficiency. For example,\nboth HSTU [55] and FuXi-\ud835\udefc [52] employ a T5-style bucket-based\nencoding scheme [37], where relative temporal intervals are log-\ntransformed and discretized into distinct indices for bucket lookup.\nThis operation pattern results in \ud835\udc5b2 non-contiguous and unstruc-\ntured memory accesses for a sequence of length \ud835\udc5b, posing a major\nobstacle to parallel execution on modern hardware. Although FuXi-\n\ud835\udefd [53] introduces an inverse-proportion decay function to alleviate\nsome of these issues, both approaches lack grounding in cognitive\nprinciples, limiting their expressiveness and explainability.\nAnother overlooked inefficiency stems from positional encoding\nmodule. Modern frameworks introduce relative positional encod-\ning even when absolute positional embedding and temporal signal\nare already available. For example, FuXi- \ud835\udefc [52] and FuXi- \ud835\udefd [53]\nincorporate positional encoding as a separate attention channel\nwith \ud835\udc42(\ud835\udc5b 2) complexity, which becomes an efficiency bottleneck\nfor long sequences. Recent advances in block-sparse semantic at-\ntention [8, 20, 27, 50] offer promising solutions, but have yet to be\nexplored in the context of positional attention in recommendation.\nTo address dual challenges of efficiency and effectiveness in mod-\neling user interests, we propose a novel sequential recommenda-\ntion framework named FuXi-\ud835\udefe. Building upon FuXi-\ud835\udefd [53], FuXi-\ud835\udefe\nadopts an autoregressive architecture with dual attention channels\nfor temporal and positional modeling. We introduce two key innova-\ntions: (1) An exponential-power temporal encoder, inspired by the\nEbbinghaus forgetting curve [5], which models time-aware user in-\nterest decay through a continuous, fully matrix-based formulation.\nThis cognitively motivated design offers strong expressiveness and\nhardware-friendly execution, yielding up to11.00 \u00d7 speedup over\nthe bucket-based encoder and significant gains in recommendation\nperformance. (2) A diagonal-sparse positional mechanism, which\nidentifies and prunes low-contribution attention blocks based on\nimportance scoring on the leftmost column, reducing positional\nattention overhead by74.56%while preserving recommendation\nquality. Together with practical optimizations such as data type pre-\nconversion, FuXi-\ud835\udefe achieves up to4.74 \u00d7 and6.18 \u00d7 improvements\nin training and inference efficiency, respectively, while consistently\noutperforming state-of-the-art baselines across multiple datasets.\nOur main contributions are summarized as follows:\n\u2022 We propose a novel sequential recommendation framework,\nFuXi-\ud835\udefe, that enhances both effectiveness and efficiency, particu-\nlarly in scenarios involving long sequences.\n\u2022 We design an exponential-power temporal encoder that models\nuser interest decay through a tunable exponential function. It\noffers flexibility to adapt to diverse behavioral patterns and\nachieves high efficiency through regular memory access and\npure matrix operations.\n\u2022 We introduce a diagonal-sparse positional mechanism that prunes\nredundant attention blocks, substantially reducing computa-\ntional overhead while preserving recommendation accuracy.\n\u2022 We conduct extensive experiments on four real-world datasets,\ndemonstrating that FuXi-\ud835\udefe achieves state-of-the-art performance.\nOn a large-scale industrial music dataset, FuXi-\ud835\udefe achieves im-\nprovements of25.06%in HR@10 and42.86%in NDCG@10 over\nstrong autoregressive baselines.",
          "translated": "**\uc11c\ub860**\n\n\uc21c\ucc28 \ucd94\ucc9c\uc740 \uc0ac\uc6a9\uc790\uc758 \ub2e4\uc74c \uc0c1\ud638 \uc791\uc6a9\uc744 \uc608\uce21\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\uba70, \uc774\ub4e4\uc758 \uc5ed\uc0ac\uc801 \ud589\ub3d9 \uc2dc\ud000\uc2a4\uc5d0 \uae30\ubc18\ud569\ub2c8\ub2e4 [22, 43, 52]. \uc815\ud655\ud55c \uc608\uce21\uc740 \uc0ac\uc6a9\uc790 \uacbd\ud5d8\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uace0 \uc0c1\ub2f9\ud55c \uc0c1\uc5c5\uc801 \uac00\uce58\ub97c \ucc3d\ucd9c\ud569\ub2c8\ub2e4 [21, 32, 47]. \ucd94\ucc9c \ubaa8\ub378\uc758 \ud559\uc2b5 \ud6a8\uc728\uc131\uc740 \uc5c5\ub370\uc774\ud2b8 \ube48\ub3c4 \ubc0f \uc11c\ube44\uc2a4 \uc2e0\uc120\ub3c4\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, \ucd94\ub860 \ud6a8\uc728\uc131\uc740 \uc2dc\uc2a4\ud15c \uc9c0\uc5f0 \uc2dc\uac04 \ubc0f \ubc30\ud3ec \ube44\uc6a9\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4 [29, 30, 46]. \ub530\ub77c\uc11c \ud6a8\uacfc\uc801\uc774\uace0 \ud6a8\uc728\uc801\uc778 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc124\uacc4\ud558\ub294 \uac83\uc740 \ud575\uc2ec\uc801\uc778 \uacfc\uc81c\uc785\ub2c8\ub2e4.\n\n\ucd5c\uadfc \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378\uc758 \ubc1c\uc804 [1, 52, 55]\ub294 \uc804\ud1b5\uc801\uc778 \ubc29\ubc95\uacfc \ube44\uad50\ud558\uc5ec \uc6b0\uc218\ud55c \uc131\ub2a5\uacfc \ud655\uc7a5 \ud6a8\uacfc\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574 \uc790\ubc1c\uc801 \uc21c\ud658 \uc544\ud0a4\ud14d\ucc98\ub97c \ud65c\uc6a9\ud569\ub2c8\ub2e4 [16, 22, arXiv:2512.12740v1 [cs.IR] 14 Dec 2025]. \uadf8\ub7ec\ub098 \uc774\ub7ec\ud55c \ubaa8\ub378\uc740 \uc885\uc885 \uc544\ud0a4\ud14d\ucc98 \ubcf5\uc7a1\uc131\uc73c\ub85c \uc778\ud574 \ud6a8\uc728\uc131 \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc57c\uae30\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, HSTU [55]\uac00 SASRec [22]\ubcf4\ub2e4 HR@10\uc744 55.18% \uac1c\uc120\ud588\uc9c0\ub9cc, \ud559\uc2b5 \ud6a8\uc728\uc131\uc5d0 3.21\u00d7\uc758 \uc18d\ub3c4 \uc800\ud558\ub97c \uacaa\uc5c8\uc2b5\ub2c8\ub2e4. FuXi-\ud835\udefc [52]\ub294 HSTU\ubcf4\ub2e4 HR@10\uc744 5.66% \uac1c\uc120\ud588\uc9c0\ub9cc, 9.63%\uc758 \ucd94\uac00\uc801\uc778 \ud6a8\uc728\uc131 \uc190\uc2e4\uc744 \ucd08\ub798\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ube44\ud6a8\uc728\uc131\uc740 \uc9c0\uc5f0 \uc2dc\uac04 \uc694\uad6c \uc0ac\ud56d\uc744 \ucda9\uc871\ud558\uae30 \uc704\ud574 \ubaa8\ub378 \ud06c\uae30\ub97c \uc904\uc5ec\uc57c \ud558\ub294 \uc0dd\uc0b0 \uc2dc\uc2a4\ud15c\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uccd0 \uc790\ubc1c\uc801 \uc21c\ud658 \ud30c\ub77c\ubbf8\ud130\uc758 \ud655\uc7a5 \ud6a8\uacfc\ub97c \uc57d\ud654\uc2dc\ud0b5\ub2c8\ub2e4.\n\n\uc21c\ucc28 \ucd94\ucc9c\uc758 \uc8fc\uc694 \ud6a8\uc728\uc131 \ubcd1\ubaa9 \ud604\uc0c1\uc740 \uc2dc\uac04 \uc778\ucf54\ub529 \ubaa8\ub4c8\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \uc2dc\uac04 \uc815\ubcf4\ub294 \uc0ac\uc6a9\uc790\uc758 \uc2dc\uac04 \uacbd\uacfc\uc5d0 \ub530\ub978 \uc120\ud638\ub3c4 \ubcc0\ud654\ub97c \ud3ec\ucc29\ud558\ub294 \ub370 \ud544\uc218\uc801\uc774\uba70, \uc774\ub294 \uad00\uc2ec \ud328\ud134 \ubaa8\ub378\ub9c1\uc758 \uae30\ucd08\uc785\ub2c8\ub2e4 [2, 28, 51]. \uadf8\ub7ec\ub098 \ucd5c\ucca8\ub2e8 \ud504\ub808\uc784\uc6cc\ud06c\uc5d0\uc11c \uc2dc\uac04 \uc778\ucf54\ub354\ub294 \ubd88\uaddc\uce59\ud558\uace0 \ubd84\uc808\ub41c \uba54\ubaa8\ub9ac \uc561\uc138\uc2a4\ub85c \uc778\ud574 \ubcd1\ub82c \ud558\ub4dc\uc6e8\uc5b4\uc640 \ucda9\ub3cc\ud558\uba70, \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uc2ec\uac01\ud558\uac8c \uc81c\ud55c\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, HSTU [55]\uc640 FuXi-\ud835\udefc [52] \ubaa8\ub450 T5 \uc2a4\ud0c0\uc77c\uc758 \ubc14\ud0b7 \uae30\ubc18 \uc778\ucf54\ub529 \uc2a4\ud0b4\uc744 \uc0ac\uc6a9\ud558\ub294\ub370, \uc5ec\uae30\uc11c \uc0c1\ub300\uc801\uc778 \uc2dc\uac04 \uac04\uaca9\uc774 \ub85c\uadf8 \ubcc0\ud658\ub418\uace0 \uace0\uc720\ud55c \uc778\ub371\uc2a4\ub85c \ubd84\ud560\ub429\ub2c8\ub2e4. \uc774 \ud328\ud134\uc740 \ud835\udc5b\uac1c\uc758 \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0\uc11c \ud835\udc5b\u00b2\uac1c\uc758 \ube44\uc5f0\uc18d\uc801\uc774\uace0 \uad6c\uc870\ud654\ub418\uc9c0 \uc54a\uc740 \uba54\ubaa8\ub9ac \uc561\uc138\uc2a4\ub97c \ubc1c\uc0dd\uc2dc\ucf1c \ud604\ub300 \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \ubcd1\ub82c \uc2e4\ud589\uc744 \uc704\ud55c \uc8fc\uc694 \uc7a5\uc560\ubb3c\uc785\ub2c8\ub2e4. FuXi-\ud835\udefd [53]\uac00 \uc774\ub7ec\ud55c \ubb38\uc81c \uc644\ud654\uc5d0 \uc77c\ubd80 \uc601\ud5a5\uc744 \ubbf8\ucce4\uc9c0\ub9cc, \uc774 \uc811\uadfc \ubc29\uc2dd\uc740 \uc778\uc9c0 \uc6d0\ub9ac\uc5d0 \ub300\ud55c \uae30\ubc18\uc744 \uac16\ucd94\uc9c0 \ubabb\ud558\uc5ec \ud45c\ud604\ub825\uacfc \uc124\uba85 \uac00\ub2a5\uc131\uc744 \uc81c\ud55c\ud569\ub2c8\ub2e4.\n\n\ub610 \ub2e4\ub978 \uac04\uacfc\ub41c \ube44\ud6a8\uc728\uc131\uc740 \uc704\uce58 \uc778\ucf54\ub529 \ubaa8\ub4c8\uc5d0\uc11c \ube44\ub86f\ub429\ub2c8\ub2e4. \ud604\ub300 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc808\ub300\uc801\uc778 \uc704\uce58 \uc784\ubca0\ub529 \ubc0f \uc2dc\uac04 \uc2e0\ud638\uac00 \uc774\ubbf8 \uc874\uc7ac\ud558\ub294 \uacbd\uc6b0\uc5d0\ub3c4 \uc0c1\ub300\uc801\uc778 \uc704\uce58 \uc778\ucf54\ub529\uc744 \ub3c4\uc785\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, FuXi- \ud835\udefc [52]\uc640 FuXi- \ud835\udefd [53]\ub294 \ud835\udc42(\ud835\udc5b\u00b2) \ubcf5\uc7a1\ub3c4\ub97c \uac16\ub294 \uc8fc\uc758 \ucc44\ub110\ub85c \uc704\uce58 \uc778\ucf54\ub529\uc744 \ubcc4\ub3c4\uc758 \ucc44\ub110\ub85c \ud1b5\ud569\ud558\ub294\ub370, \uc774\ub294 \uae34 \uc2dc\ud000\uc2a4\uc5d0 \ub300\ud55c \ud6a8\uc728\uc131 \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uc57c\uae30\ud569\ub2c8\ub2e4. \ucd5c\uadfc \ube14\ub85d-\uc2a4\ud30c\uc2a4 \uc758\ubbf8 \uc8fc\uc758 [8, 20, 27, 50]\uc758 \ubc1c\uc804\uc740 \uc720\ub9dd\ud55c \uc194\ub8e8\uc158\uc744 \uc81c\uacf5\ud558\uc9c0\ub9cc, \uc704\uce58 \uc8fc\uc758\uc5d0 \uc801\uc6a9\ub418\uc9c0\ub294 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\n\n\uc0ac\uc6a9\uc790\uc758 \uad00\uc2ec\uc744 \ubaa8\ub378\ub9c1\ud558\ub294 \ud6a8\uc728\uc131\uacfc \ud6a8\uacfc\ub77c\ub294 \ub450 \uac00\uc9c0 \uacfc\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 FuXi-\ud835\udefe\ub77c\ub294 \uc0c8\ub85c\uc6b4 \uc21c\ucc28 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4. FuXi-\ud835\udefd [53]\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 FuXi-\ud835\udefe\ub294 \uc774\uc911 \uc8fc\uc758 \ucc44\ub110\uc744 \uac16\ub294 \uc790\ubc1c\uc801 \uc21c\ud658 \uc544\ud0a4\ud14d\ucc98\ub97c \ucc44\ud0dd\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \uc2dc\uac04 \ubc0f \uc704\uce58 \ubaa8\ub378\ub9c1\uc744 \uc704\ud55c \ub450 \uac00\uc9c0 \uc8fc\uc694 \ud601\uc2e0\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4: (1) Ebbinghaus \ub9dd\uac01 \uace1\uc120 [5]\uc5d0 \uc601\uac10\uc744 \ubc1b\uc740 \uc9c0\uc218 \uae30\ubc18 \uc2dc\uac04 \uc778\ucf54\ub354\ub85c, \uc2dc\uac04 \uc758\uc2dd\uc744 \uac16\ub294 \uc0ac\uc6a9\uc790 \uad00\uc2ec \uac10\uc18c\ub97c \uc5f0\uc18d\uc801\uc774\uace0 \uc644\uc804\ud788 \ud589\ub82c \uae30\ubc18\uc758 \ud615\ud0dc\ub85c \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4. \uc774 \uc778\uc9c0\uc801\uc73c\ub85c \ub3d9\uae30\ud654\ub41c \uc124\uacc4\ub294 \ud558\ub4dc\uc6e8\uc5b4 \uce5c\ud654\uc801\uc778 \uc2e4\ud589\uc744 \ud1b5\ud574 \ubc14\ud0b7 \uae30\ubc18 \uc778\ucf54\ub354\ubcf4\ub2e4 \ucd5c\ub300 11.00\u00d7\uc758 \uc18d\ub3c4 \ud5a5\uc0c1 \ubc0f \ucd94\ucc9c \uc131\ub2a5\uc5d0\uc11c \uc0c1\ub2f9\ud55c \uc774\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. (2) \ub0ae\uc740 \uae30\uc5ec\ub3c4\ub97c \uac16\ub294 \uc8fc\uc758 \ube14\ub85d\uc744 \uc911\uc694\ub3c4 \uc810\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac00\uc7a5\uc790\ub9ac \uc5f4\uc5d0\uc11c \uc2dd\ubcc4\ud558\uace0 \uc798\ub77c\ub0b4\ub294 \ub300\uac01 \uc2a4\ud30c\uc2a4 \uc704\uce58 \uba54\ucee4\ub2c8\uc998\uc73c\ub85c, \uc704\uce58 \uc8fc\uc758 \uc624\ubc84\ud5e4\ub4dc\ub97c 74.56% \uc904\uc774\uba74\uc11c \ucd94\ucc9c \ud488\uc9c8\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \ub370\uc774\ud130 \uc720\ud615 \uc0ac\uc804 \ubcc0\ud658\uacfc \uac19\uc740 \uc2e4\uc6a9\uc801\uc778 \ucd5c\uc801\ud654\uc640 \ud568\uaed8, FuXi-\ud835\udefe\ub294 \ud559\uc2b5 \ubc0f \ucd94\ub860 \ud6a8\uc728\uc131\uc744 \ucd5c\ub300 4.74\u00d7 \ubc0f 6.18\u00d7 \ud5a5\uc0c1\uc2dc\ud0a4\uba74\uc11c, \uc5ec\ub7ec \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \uae30\uc900\uc120\ubcf4\ub2e4 \uc9c0\uc18d\uc801\uc73c\ub85c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.\n\n\uc800\ud76c\uc758 \uc8fc\uc694 \uae30\uc5ec\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n\n*   \uae34 \uc2dc\ud000\uc2a4\uc5d0 \ud2b9\ud788 \ud6a8\uacfc\uc801\uc774\uace0 \ud6a8\uc728\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc0c8\ub85c\uc6b4 \uc21c\ucc28 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\uc778 FuXi-\ud835\udefe\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4.\n*   \uc0ac\uc6a9\uc790 \uad00\uc2ec \uac10\uc18c\ub97c \uc870\uc815 \uac00\ub2a5\ud55c \uc9c0\uc218 \ud568\uc218\ub97c \ud1b5\ud574 \ubaa8\ub378\ub9c1\ud558\ub294 \ub300\uac01 \uc2a4\ud30c\uc2a4 \uc2dc\uac04 \uc778\ucf54\ub354\ub97c \uc124\uacc4\ud569\ub2c8\ub2e4. \uc774\ub294 \ub2e4\uc591\ud55c \ud589\ub3d9 \ud328\ud134\uc5d0 \uc801\uc751\ud560 \uc218 \uc788\ub294 \uc720\uc5f0\uc131\uc744 \uc81c\uacf5\ud558\uba70, \uc815\uaddc \uba54\ubaa8\ub9ac \uc561\uc138\uc2a4\uc640 \uc21c\uc218 \ud589\ub82c \uc5f0\uc0b0\uc744 \ud1b5\ud574 \ub192\uc740 \ud6a8\uc728\uc131\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.\n*   \uc911\ubcf5\ub41c \uc8fc\uc758 \ube14\ub85d\uc744 \uc904\uc774\uba74\uc11c \ucd94\ucc9c \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\ub294 \ub300\uac01 \uc2a4\ud30c\uc2a4 \uc704\uce58 \uba54\ucee4\ub2c8\uc998\uc744 \ub3c4\uc785\ud569\ub2c8\ub2e4.\n*   FuXi-\ud835\udefe\uac00 \ub124 \uac1c\uc758 \uc2e4\uc81c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \ucd5c\ucca8\ub2e8 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc74c\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8\uc744 \uc218\ud589\ud569\ub2c8\ub2e4."
        },
        {
          "name": "Related Work",
          "original": "2.1 Sequential Recommendation\nSequential recommendation aims to predict users\u2019 future prefer-\nences based on their interaction sequences. Early methods employ\nprobabilistic models, such as Markov Chains [39] and session-based\nkNN [15, 17], to capture short-term transitions. Subsequent deep\nlearning approaches model higher-order dependencies through\nCNNs [44], RNNs [ 34], and GRU-based architecture [ 16]. With\nthe introduction of Transformer [45], global dependency modeling\nbecomes possible via self-attention. SASRec [22] uses multi-head\nself-attention to identify important items. BERT4Rec [43] leverages\nbidirectional encoding for improved prediction. Further extensions\nexplore more complex settings, including cross-domain [60], multi-\nmodal [57], and multi-behavior scenarios [42].\nTraditional frameworks typically follow a ranking-based for-\nmulation, which limits their scalability and adaptability. Genera-\ntive recommendation reframes the task as sequence generation,\nenabling unified, end-to-end modeling. TIGER [38] pioneers gener-\native recommendation for zero-shot scenarios. HLLM [1] tokenizes\nitem IDs for autoregressive modeling with large language mod-\nels. HSTU [55] scales recommendation using Transformer-based\ngeneration. FuXi-\ud835\udefc [52] introduces a three-channel design to sep-\narately model semantic, temporal, and positional features, while\nFuXi-\ud835\udefd[53] improves efficiency by removing semantic modeling.\nDespite recent progress, balancing effectiveness and efficiency\nremains a core challenge. To this end, we propose FuXi- \ud835\udefe that\nintegrates a cognitively inspired temporal encoder and a sparse\npositional mechanism to improve both performance and scalability.\n2.2 Temporal Encoder\nTemporal information plays a crucial role in capturing the evolving\nnature of user interests. Prior works explore various strategies, such\nas encoder-decoder timestamp modeling [59], joint learning of tem-\nporal patterns [51], and multi-granularity temporal embeddings [2].\nTiSASRec [28] explicitly incorporates both absolute positions and\nrelative temporal intervals. Time2Vec [23] represents temporal sig-\nnals through sinusoidal and linear activations. More recently, gen-\nerative recommendation models like HSTU [55] and FuXi-\ud835\udefc [52]\nadopt T5-style bucket encodings [ 37] to discretize relative tem-\nporal intervals. Despite effectiveness, the bucket-based method\nintroduces discontinuous memory access and poor hardware uti-\nlization, resulting in substantial efficiency bottlenecks, particularly\n\nFuXi-\ud835\udefe KDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea\nfor long sequences. FuXi-\ud835\udefd [53] replaces bucketing with an inverse-\nproportional decay function, offering improved efficiency. However,\nits fixed decay pattern lacks adaptability across different application\nscenarios, limiting modeling flexibility.\nTo overcome these limitations, we propose an exponential-power\ntemporal encoder inspired by the Ebbinghaus forgetting curve [5].\nThe design enables flexible modeling of temporal decay patterns and\nmaintains efficient, hardware-friendly continuous memory access.\n2.3 Positional Encoder\nPositional encoding provides contextual information for sequence\nmodeling tasks. Existing approaches can be broadly classified into\nabsolute and relative encoding methods. Absolute encodings di-\nrectly incorporate positional vectors into item representations, typ-\nically implemented using either fixed sinusoidal functions [45] or\nlearnable embeddings [9]. Relative encodings [3, 14, 19, 24, 37, 40]\nmodel pairwise item distances by learning weights associated with\nrelative positional offsets. Recent generative recommendation mod-\nels often combine both encoding types, integrating relative encod-\ning either as an additive bias in the semantic channel [55] or as a\nseparate attention matrix in an independent channel [52, 53].\nFollowing FuXi-\ud835\udefc [52] and FuXi-\ud835\udefd [53], we adopt relative posi-\ntional encoding as a separate attention matrix. However, we observe\nthat positional information is already partially captured by the em-\nbedding layer and the temporal encoder, introducing redundancy.\nTo mitigate this, we design a diagonal-sparse pruning method ap-\nplied after training. This method removes low-contribution blocks\nfrom the positional attention matrix, significantly reducing compu-\ntation overhead while preserving recommendation accuracy.",
          "translated": "**\uad00\ub828 \uc5f0\uad6c**\n\n2.1 \uc21c\ucc28\uc801 \ucd94\ucc9c\n\uc21c\ucc28\uc801 \ucd94\ucc9c\uc740 \uc0ac\uc6a9\uc790\uc758 \uc0c1\ud638 \uc791\uc6a9 \uc21c\uc11c\ub97c \uae30\ubc18\uc73c\ub85c \uc0ac\uc6a9\uc790\uc758 \ubbf8\ub798 \uc120\ud638\ub3c4\ub97c \uc608\uce21\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud55c\ub2e4. \ucd08\uae30 \ubc29\ubc95\uc73c\ub85c\ub294 \ud655\ub960 \ubaa8\ub378, \uc608\ub97c \ub4e4\uc5b4 \ub9c8\ub974\ucf54\ud504 \uccb4\uc778 [39] \ubc0f \uc138\uc158 \uae30\ubc18 kNN [15, 17]\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uae30\uc801\uc778 \uc804\ud658\uc744 \ud3ec\ucc29\ud588\ub2e4. \uc774\ud6c4 \uc2ec\uce35 \ud559\uc2b5 \uc811\uadfc \ubc29\uc2dd\uc740 CNN [44], RNN [34], GRU \uae30\ubc18 \uc544\ud0a4\ud14d\ucc98 [16]\ub97c \ud1b5\ud574 \uace0\ucc28\uc6d0 \uc758\uc874\uc131\uc744 \ubaa8\ub378\ub9c1\ud55c\ub2e4. Transformer [45]\uc758 \ub3c4\uc785\uc73c\ub85c \uc790\uae30 \uc8fc\uc758(self-attention)\ub97c \ud1b5\ud574 \uae00\ub85c\ubc8c \uc758\uc874\uc131 \ubaa8\ub378\ub9c1\uc774 \uac00\ub2a5\ud574\uc84c\ub2e4. SASRec [22]\ub294 \uc911\uc694\ud55c \ud56d\ubaa9\uc744 \uc2dd\ubcc4\ud558\uae30 \uc704\ud574 \ub2e4\uc911 \ud5e4\ub4dc \uc790\uae30 \uc8fc\uc758\ub97c \uc0ac\uc6a9\ud55c\ub2e4. BERT4Rec [43]\ub294 \ud5a5\uc0c1\ub41c \uc608\uce21\uc744 \uc704\ud574 \uc591\ubc29\ud5a5 \uc778\ucf54\ub529\uc744 \ud65c\uc6a9\ud55c\ub2e4. \ub354 \ub098\uc544\uac00 \ub354\uc6b1 \ubcf5\uc7a1\ud55c \uc124\uc815\ub4e4\uc744 \ud0d0\uad6c\ud558\uba70, \uad50\ucc28 \ub3c4\uba54\uc778 [60], \ub2e4\uc911 \ubaa8\ub2ec [57], \ub2e4\uc911 \ud589\ub3d9 \uc2dc\ub098\ub9ac\uc624 [42] \ub4f1\uc744 \ud3ec\ud568\ud55c\ub2e4.\n\uc804\ud1b5\uc801\uc778 \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ub7ad\ud0b9 \uae30\ubc18\uc758 \uad6c\uc131\uc744 \ub530\ub974\uba70, \uc774\ub294 \ud655\uc7a5\uc131\uacfc \uc801\uc751\uc131\uc5d0 \uc81c\ud55c\uc744 \ub454\ub2e4. \uc0dd\uc131\uc801 \ucd94\ucc9c\uc740 \uc2dc\ud000\uc2a4 \uc0dd\uc131\uc73c\ub85c \uc791\uc5c5\uc744 \uc7ac\uad6c\uc131\ud558\uc5ec, \ud1b5\ud569\uc801\uc774\uace0 \ub05d\uc5d0\uc11c \ub05d\uae4c\uc9c0 \ubaa8\ub378\ub9c1\uc744 \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4. TIGER [38]\ub294 \uc81c\ub85c\uc0f7 \uc2dc\ub098\ub9ac\uc624\ub97c \uc704\ud55c \uc0dd\uc131\uc801 \ucd94\ucc9c\uc758 \uac1c\ucc99\uc790\uc774\ub2e4. HLLM [1]\ub294 \ud56d\ubaa9 ID\ub97c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378\uacfc \ud568\uaed8 \uc790\uae30 \ud68c\uadc0 \ubaa8\ub378\ub9c1\uc744 \uc704\ud574 \ud1a0\ud070\ud654\ud55c\ub2e4. HSTU [55]\ub294 Transformer \uae30\ubc18 \uc0dd\uc131 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd94\ucc9c\uc744 \ud655\uc7a5\ud55c\ub2e4. FuXi-\ud835\udefc [52]\ub294 \uc758\ubbf8, \uc2dc\uac04 \ubc0f \uc704\uce58\uc801 \ud2b9\uc9d5\uc744 \uac01\uac01 \ubaa8\ub378\ub9c1\ud558\uae30 \uc704\ud55c \uc138 \uac00\uc9c0 \ucc44\ub110 \uc124\uacc4\ub97c \ub3c4\uc785\ud558\uace0, FuXi-\ud835\udefd[53]\ub294 \uc758\ubbf8 \ubaa8\ub378\ub9c1\uc744 \uc81c\uac70\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uac1c\uc120\ud55c\ub2e4.\n\ucd5c\uadfc\uc758 \uc9c4\uc804\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ud6a8\uacfc\uc640 \ud6a8\uc728\uc131 \uac04\uc758 \uade0\ud615\uc744 \ub9de\ucd94\ub294 \uac83\uc740 \uc5ec\uc804\ud788 \ud575\uc2ec \uacfc\uc81c\uc774\ub2e4. \uc774\ub7ec\ud55c \ub9e5\ub77d\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uc131\ub2a5\uacfc \ud655\uc7a5\uc131\uc744 \ubaa8\ub450 \uac1c\uc120\ud558\uae30 \uc704\ud574 \uc2dc\uac04\uc801 \uc778\ucf54\ub354\uc640 \ud76c\uc18c \uc704\uce58\uc801 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud569\ud558\ub294 FuXi- \ud835\udefe\ub97c \uc81c\uc548\ud55c\ub2e4.\n\n2.2 \uc2dc\uac04\uc801 \uc778\ucf54\ub354\n\uc2dc\uac04 \uc815\ubcf4\ub294 \uc0ac\uc6a9\uc790\uc758 \uad00\uc2ec\uc0ac\uc758 \uc9c4\ud654\ub97c \ud3ec\ucc29\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud55c\ub2e4. \uc774\uc804 \uc5f0\uad6c\uc5d0\uc11c\ub294 \uc2dc\uac04 \uc2a4\ud0ec\ud504 \ubaa8\ub378\ub9c1 [59], \uc2dc\uac04 \ud328\ud134\uc758 \uacf5\ub3d9 \ud559\uc2b5 [51], \ub2e4\uc911 \ud574\uc0c1\ub3c4 \uc2dc\uac04\uc801 \uc784\ubca0\ub529 [2] \ub4f1 \ub2e4\uc591\ud55c \uc804\ub7b5\uc744 \ud0d0\uad6c\ud588\ub2e4. TiSASRec [28]\ub294 \uc808\ub300\uc801\uc778 \uc704\uce58\uc640 \uc0c1\ub300\uc801\uc778 \uc2dc\uac04 \uac04\uaca9\uc744 \ubaa8\ub450 \uba85\uc2dc\uc801\uc73c\ub85c \ud1b5\ud569\ud55c\ub2e4. Time2Vec [23]\ub294 \uc2dc\uadf8\ub178\uc774\ub4dc \ubc0f \uc120\ud615 \ud65c\uc131\ud654\ub97c \ud1b5\ud574 \uc2dc\uac04 \uc2e0\ud638\ub97c \ud45c\ud604\ud55c\ub2e4. \ucd5c\uadfc\uc5d0\ub294 HSTU [55] \ubc0f FuXi-\ud835\udefc [52]\uc640 \uac19\uc740 \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378\uc774 T5 \uc2a4\ud0c0\uc77c\uc758 \ubc14\ud0b7 \uc778\ucf54\ub529 [37]\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c1\ub300\uc801\uc778 \uc2dc\uac04 \uac04\uaca9\uc744 \uc774\uc0b0\ud654\ud55c\ub2e4. \ubc14\ud0b7 \uae30\ubc18 \ubc29\ubc95\uc740 \ud6a8\uacfc\uc801\uc774\uc9c0\ub9cc, \ud2b9\ud788 \uae34 \uc2dc\ud000\uc2a4\uc5d0\uc11c \uc0c1\ub2f9\ud55c \ud6a8\uc728\uc131 \ubcd1\ubaa9 \ud604\uc0c1\uc744 \ucd08\ub798\ud558\ub294 \ube44\uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \uc811\uadfc \ubc0f \ub0ae\uc740 \ud558\ub4dc\uc6e8\uc5b4 \ud65c\uc6a9\uc744 \uc57c\uae30\ud55c\ub2e4.\nFuXi-\ud835\udefd [53]\ub294 \ubc14\ud0b7\uc744 \ubb36\ub294 \ub300\uc2e0 \uc5ed\ube44\ub840 \uac10\uc1e0 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uac1c\uc120\ud588\ub2e4. \ud558\uc9c0\ub9cc \uace0\uc815\ub41c \uac10\uc1e0 \ud328\ud134\uc740 \ub2e4\uc591\ud55c \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc2dc\ub098\ub9ac\uc624\uc5d0 \ub300\ud55c \uc801\uc751\uc131\uc744 \uacb0\uc5ec\ud558\uc5ec \ubaa8\ub378\ub9c1 \uc720\uc5f0\uc131\uc744 \uc81c\ud55c\ud55c\ub2e4.\n\uc774\ub7ec\ud55c \uc81c\ud55c \uc0ac\ud56d\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 Ebbinghaus \ub9dd\uac01 \uace1\uc120 [5]\uc5d0 \uc601\uac10\uc744 \ubc1b\uc740 \uc9c0\uc218 \uae30\ubc18 \uc2dc\uac04\uc801 \uc778\ucf54\ub354\ub97c \uc81c\uc548\ud55c\ub2e4. \uc774 \uc124\uacc4\ub294 \uc720\uc5f0\ud55c \uc2dc\uac04 \uac10\uc1e0 \ud328\ud134 \ubaa8\ub378\ub9c1\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uace0, \ud6a8\uc728\uc801\uc774\uba70 \ud558\ub4dc\uc6e8\uc5b4 \uce5c\ud654\uc801\uc778 \uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \uc811\uadfc\uc744 \uc720\uc9c0\ud55c\ub2e4.\n\n2.3 \uc704\uce58\uc801 \uc778\ucf54\ub354\n\uc704\uce58\uc801 \uc778\ucf54\ub529\uc740 \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1 \uc791\uc5c5\uc5d0 \ub300\ud55c \ubb38\ub9e5 \uc815\ubcf4\ub97c \uc81c\uacf5\ud55c\ub2e4. \uae30\uc874 \uc811\uadfc \ubc29\uc2dd\uc740 \uc808\ub300 \uc778\ucf54\ub529 \ubc29\ubc95\uacfc \uc0c1\ub300 \uc778\ucf54\ub529 \ubc29\ubc95\uc73c\ub85c \ud06c\uac8c \ubd84\ub958\ud560 \uc218 \uc788\ub2e4. \uc808\ub300 \uc778\ucf54\ub529\uc740 \ud56d\ubaa9 \ud45c\ud604\uc5d0 \uc704\uce58 \ubca1\ud130\ub97c \uc9c1\uc811 \ud1b5\ud569\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c, \uc77c\ubc18\uc801\uc73c\ub85c \uace0\uc815 \uc2dc\uadf8\ub178\uc774\ub4dc \ud568\uc218 [45] \ub610\ub294 \ud559\uc2b5 \uac00\ub2a5\ud55c \uc784\ubca0\ub529 [9]\ub97c \uc0ac\uc6a9\ud558\uc5ec \uad6c\ud604\ub41c\ub2e4. \uc0c1\ub300 \uc778\ucf54\ub529 [3, 14, 19, 24, 37, 40]\uc740 \uc0c1\ub300\uc801\uc778 \uc704\uce58 \uc624\ud504\uc14b\uc5d0 \ub300\ud55c \uac00\uc911\uce58\ub97c \ud559\uc2b5\ud558\uc5ec \ud56d\ubaa9 \uac04\uc758 \uc30d\ubcc4 \uac70\ub9ac\ub97c \ubaa8\ub378\ub9c1\ud55c\ub2e4. \ucd5c\uadfc \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378\uc740 \uc0c1\ub300 \uc778\ucf54\ub529\uc744 \ubd80\uac00\uc801 \ud3b8\ud5a5\uc73c\ub85c \uc0ac\uc6a9\ud558\uac70\ub098 \ub3c5\ub9bd\uc801\uc778 \ucc44\ub110\uc5d0\uc11c \ubcc4\ub3c4\uc758 \uc8fc\uc758 \ud589\ub82c\ub85c \uc0ac\uc6a9\ud558\uace0 \uc788\ub2e4 [52, 53].\nFuXi-\ud835\udefc [52] \ubc0f FuXi-\ud835\udefd [53]\ub97c \ub530\ub974\uba74, \uc6b0\ub9ac\ub294 \uc0c1\ub300\uc801\uc778 \uc704\uce58\uc801 \uc778\ucf54\ub529\uc744 \ubcc4\ub3c4\uc758 \uc8fc\uc758 \ud589\ub82c\ub85c \ucc44\ud0dd\ud55c\ub2e4. \uadf8\ub7ec\ub098 \uc704\uce58 \uc815\ubcf4\uac00 \uc774\ubbf8 \uc784\ubca0\ub529 \ub808\uc774\uc5b4\uc640 \uc2dc\uac04\uc801 \uc778\ucf54\ub354\uc5d0 \uc758\ud574 \ubd80\ubd84\uc801\uc73c\ub85c \ucea1\ucc98\ub418\uae30 \ub54c\ubb38\uc5d0 \uc911\ubcf5\uc774 \ubc1c\uc0dd\ud55c\ub2e4\ub294 \uac83\uc744 \uad00\ucc30\ud588\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \uc644\ud654\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \ud559\uc2b5 \ud6c4 \ub300\uac01\uc120-\ud76c\uc18c \uac00\uc9c0\uce58\uae30 \ubc29\ubc95\uc744 \uc124\uacc4\ud588\ub2e4. \uc774 \ubc29\ubc95\uc740 \uc8fc\uc758 \ud589\ub82c\uc5d0\uc11c \uae30\uc5ec\ub3c4\uac00 \ub0ae\uc740 \ube14\ub85d\uc744 \uc81c\uac70\ud558\uc5ec \uacc4\uc0b0 \uc624\ubc84\ud5e4\ub4dc\ub97c \ud06c\uac8c \uc904\uc774\uba74\uc11c \ucd94\ucc9c \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud55c\ub2e4."
        },
        {
          "name": "Methodology",
          "original": "In this section, we begin by formally defining the next-item sequen-\ntial recommendation task. We then present the overall architecture\nof FuXi-\ud835\udefe, followed by a detailed description of its core components.\n3.1 Problem Statement\nThe primary objective of sequential recommendation is to predict\nthe next item a user is likely to interact with, conditioned on their\nhistorical interaction sequence. Formally, letU=\n\b\n\ud835\udc621, \ud835\udc622, . . . , \ud835\udc62| U |\n\t\ndenote the set of users, and V=\n\b\n\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63| V |\n\t\nthe set of items.\nFor each user\ud835\udc62\u2208 U , the interaction history is represented as a time-\nordered sequence \ud835\udc46\ud835\udc62 =\nh\n(\ud835\udc63 (\ud835\udc62 )\n1 , \ud835\udc61 (\ud835\udc62 )\n1 ),(\ud835\udc63 (\ud835\udc62 )\n2 , \ud835\udc61 (\ud835\udc62 )\n2 ), . . . ,(\ud835\udc63 (\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 , \ud835\udc61 (\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 )\ni\n,\nwhere each tuple (\ud835\udc63 (\ud835\udc62 )\n\ud835\udc56 , \ud835\udc61 (\ud835\udc62 )\n\ud835\udc56 ) corresponds to an item interacted with\nat timestamp \ud835\udc61 (\ud835\udc62 )\n\ud835\udc56 . The goal is to predict the subsequent item \ud835\udc63 (\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 +1\nthat user \ud835\udc62 will interact with, given their historical sequence \ud835\udc46\ud835\udc62.\nThis task is typically formulated as estimating the conditional prob-\nability distribution over the item space: \ud835\udc43\n\u0010\n\ud835\udc63 (\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 +1 =\ud835\udc63|\ud835\udc46 \ud835\udc62\n\u0011\n,\u2200\ud835\udc63\u2208 V .\nDuring training, the model is optimized to predict the next item\ud835\udc63 (\ud835\udc62 )\n\ud835\udc56+1\nat each position \ud835\udc56 along the sequence \ud835\udc46\ud835\udc62. Thus the desired output\nsequence corresponds to\nh\n\ud835\udc63 (\ud835\udc62 )\n2 , \ud835\udc63 (\ud835\udc62 )\n3 , . . . , \ud835\udc63(\ud835\udc62 )\n\ud835\udc5b\ud835\udc62 +1\ni\n[22].\n3.2 Overall Architecture\nThe overall architecture of FuXi- \ud835\udefe is illustrated in Figure 1. Fol-\nlowing the design of FuXi- \ud835\udefd [53], FuXi-\ud835\udefe adopts a decoder-only\nSwiGLU FFN\nRMSNorm\nTemporal EncoderPositional Encoder\nRMSNorm\nEmbedding Layer\nV\nPrediction Layer\nU\nLinear\nFuXi-\u03b3 BlocksStack   L \u00d7\nFigure 1: Overall architecture of FuXi-\ud835\udefe.\nTransformer architecture without query-key attention, aiming to\nstreamline computation while preserving modeling capacity. Specif-\nically, FuXi-\ud835\udefe consists of three components: (1) Embedding Layer:\nEncodes input items with learned item embeddings and incorpo-\nrates absolute positional information. (2) FuXi-\ud835\udefe Block: Capture user\ninterest patterns via dual-channel self-attention layer and SwiGLU\nFFN. (3) Prediction Layer: Computes the probability distribution\nover the item vocabulary for the next-item prediction.\n3.2.1 Embedding Layer.To address the variability in user interac-\ntion sequence lengths, we normalize each user\u2019s interaction history\nto a fixed length \ud835\udc5b prior to the embedding layer. Sequences longer\nthan \ud835\udc5b are truncated, while shorter sequences are padded with a spe-\ncial \"padding item\". Each item \ud835\udc63\u2208 V is mapped to a \ud835\udc51-dimensional\nvector via a learnable embedding matrix \ud835\udc38\u2208R | V | \u00d7\ud835\udc51. In addition,\nwe incorporate absolute positional information by adding learnable\npositional embeddings. Let \ud835\udc52\ud835\udc56 and \ud835\udc5d\ud835\udc56 denote the item embedding\nand positional embedding of the \ud835\udc56-th position, respectively. For a\nuser sequence\ud835\udc46 \ud835\udc62, the output of the embedding layer is given by:\n\ud835\udc4b 0 =\nh\n\ud835\udc52 (\ud835\udc62)\n1 +\ud835\udc5d 1, . . . , \ud835\udc52(\ud835\udc62)\n\ud835\udc5b\ud835\udc62 +\ud835\udc5d \ud835\udc5b\ud835\udc62 ,0, . . . ,0\ni\n,(1)\nwhere vectors0correspond to padding positions from\ud835\udc5b \ud835\udc62 +1to\ud835\udc5b.\nThe resulting embedding \ud835\udc4b 0 is subsequently passed through a\nstack of FuXi-\ud835\udefe blocks to capture user interest patterns. The design\ndetails of FuXi-\ud835\udefeblock are described in Section 3.3.\n3.2.2 Prediction Layer & Optimization Objective.After passing\nthrough \ud835\udc3f stacked FuXi-\ud835\udefe blocks, each position in the sequence\nencodes sufficient contextual information from previously inter-\nacted items. To generate the prediction, we project the final hidden\nrepresentation onto the item space by computing dot product with\nthe transpose of input embedding matrix, followed by a softmax\nfunction to obtain a probability distribution over all candidate items:\n\ud835\udc43\n\u0010\n\ud835\udc63 (\ud835\udc62)\n\ud835\udc56 =\ud835\udc63|\nh\n(\ud835\udc63 (\ud835\udc62)\n1 , \ud835\udc61 (\ud835\udc62)\n1 ), . . . ,(\ud835\udc63 (\ud835\udc62)\n\ud835\udc56\u22121 , \ud835\udc61 (\ud835\udc62)\n\ud835\udc56\u22121 )\ni\u0011\n=Softmax\n\u0010\n\ud835\udc65 \ud835\udc3f\ud835\udc38T\n\u0011\n\ud835\udc63\n,\n(2)\nwhere \ud835\udc65 \ud835\udc3f denotes the final-layer representation at position \ud835\udc56\u2212 1,\nand \ud835\udc38 is the input embedding matrix. To improve training efficiency,\nwe adopt the sampled softmax loss, where the softmax is computed\nover the true item and\ud835\udc41randomly sampled negative items [25].\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al.\n3.3 FuXi-\ud835\udefeBlock\nFuXi-\ud835\udefe block comprises two components: a dual-channel self-attention\nlayer and a SwiGLU feed-forward network (FFN) [41]. To improve\ntraining stability, we adopt a pre-normalization [10] strategy, ap-\nplying layer normalization before each sub-layer computation.\n3.3.1 Dual-Channel Self-Attention Layer.This layer decouples rep-\nresentation learning into two distinct channels: a temporal channel\nand a positional channel. This design enables each channel\u2019s en-\ncoder to specialize in capturing different aspects of the sequence\ninformation. Then, we adopt a post-mixing strategy via point-wise\ninteraction and linear projection to aggregate signals.\nLet \ud835\udc4b\u2208R \ud835\udc5b\u00d7\ud835\udc51 denote the input embedding to one FuXi-\ud835\udefe block.\nWe first construct two projection representations of input sequence:\n\ud835\udc48 , \ud835\udc49=Split(\ud835\udf19(RMSNorm(\ud835\udc4b)\ud835\udc4a \ud835\udc62\ud835\udc63 )),(3)\nwhere RMSNorm denotes root mean square layer normalization [56],\n\ud835\udc4a\ud835\udc62\ud835\udc63 \u2208R \ud835\udc51\u00d73\ud835\udc51 is a learnable matrix, \ud835\udf19 denotes SiLU nonlinear trans-\nformation [6], \ud835\udc48\u2208R \ud835\udc5b\u00d72\ud835\udc51 and \ud835\udc49\u2208R \ud835\udc5b\u00d7\ud835\udc51 are two projected outputs.\nTo simplify computation and improve efficiency, we share \ud835\udc49\nbetween temporal and positional encoders to learn user interests.\nThen the outputs of two encoders are concatenated and mixed with\n\ud835\udc48via Hadamard product to enable explicit 2-order interactions:\n\ud835\udc3c=RMSNorm(Concat(\ud835\udc34 \ud835\udc61\ud835\udc60\ud835\udc49 ,\ud835\udc4a\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc49)) \u2299\ud835\udc48 ,(4)\nwhere \ud835\udc34\ud835\udc61\ud835\udc60 \u2208R \ud835\udc5b\u00d7\ud835\udc5b is a temporal attention matrix constructed based\non relative temporal intervals (see Section 3.4.1 for design details);\n\ud835\udc4a\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208R \ud835\udc5b\u00d7\ud835\udc5b is a learnable positional encoding matrix, where \ud835\udc4a \ud835\udc56,\ud835\udc57\n\ud835\udc5d\ud835\udc5c\ud835\udc60\ndenotes the attention weight for relative positional offset \ud835\udc56\u2212\ud835\udc57 and\nsatisfies\ud835\udc4a \ud835\udc56,\ud835\udc57\n\ud835\udc5d\ud835\udc5c\ud835\udc60 =\ud835\udc4a \ud835\udc56+\ud835\udc5a,\ud835\udc57+\ud835\udc5a\n\ud835\udc5d\ud835\udc5c\ud835\udc60 , i.e.,\ud835\udc4a \ud835\udc5d\ud835\udc5c\ud835\udc60 is a Toeplitz matrix.\nFinally, we pass the interaction result\ud835\udc3c\u2208R \ud835\udc5b\u00d72\ud835\udc51 through a linear\nlayer to aggregate temporal and positional signals, and introduce a\nresidual connection [13] to maintain original sequence information:\n\ud835\udc42=\ud835\udc3c\ud835\udc4a \ud835\udc5c +\ud835\udc4f+\ud835\udc4b ,(5)\nwhere\ud835\udc4a \ud835\udc5c \u2208R 2\ud835\udc51\u00d7\ud835\udc51 and\ud835\udc4f\u2208R \ud835\udc51 are learnable parameters.\n3.3.2 SwiGLU FFN.Each dual-channel self-attention layer is fol-\nlowed by a SwiGLU FFN, which further refines the representation\nthrough implicit interactions. To preserve the flow of gradients and\nenable deep modeling, we incorporate a residual connection around\nthe FFN. The computations are formally defined as:\n\ud835\udc42 \u2032 =RMSNorm(\ud835\udc42),\n\ud835\udc3b=(\ud835\udf19(\ud835\udc42 \u2032\ud835\udc4a1) \u2299 (\ud835\udc42 \u2032\ud835\udc4a2))\ud835\udc4a3 +\ud835\udc42, (6)\nwhere \ud835\udf19 is SiLU activation, \ud835\udc4a1,\ud835\udc4a 2 \u2208R \ud835\udc51\u00d7\ud835\udc51 FFN and \ud835\udc4a3 \u2208R \ud835\udc51FFN \u00d7\ud835\udc51 are\nlearnable weights,\ud835\udc3b\u2208R \ud835\udc5b\u00d7\ud835\udc51 is the final output of FuXi-\ud835\udefeblock.\n3.4 Exponential-Power Temporal Encoder\nEffectively and efficiently capturing the evolving nature of user\ninterests over time is a key challenge. To address this problem,\nwe propose an exponential-power temporal encoder that utilizes\na continuous and tunable decay mechanism, offering both strong\nmodeling capabilities and high computational efficiency.\n3.4.1 Algorithm Design.Motivated by the intuition that recent in-\nteractions better reflect users\u2019 current interests, we impose a mono-\ntonic decay pattern on temporal attention weights: smaller tempo-\nral intervals receive higher weights, while larger intervals receive\nlower weights. Inspired by the Ebbinghaus forgetting curve [ 5],\nwhich describes human memory decays exponentially over time,\nwe adopt a tunable exponential function to model the temporal\ndecay in user preferences. First, we represent temporal intervals\nwithin a user\u2019s interaction sequence as pairwise differences be-\ntween item timestamps. Specifically, we construct a relative tem-\nporal matrix \ud835\udc47\u2208N \ud835\udc5b\u00d7\ud835\udc5b for each sequence, where \ud835\udc47 \ud835\udc56,\ud835\udc57 denotes the\nabsolute difference between the timestamps of the \ud835\udc56-th and \ud835\udc57-th\nitems: \ud835\udc47 \ud835\udc56,\ud835\udc57 =|\ud835\udc61 \ud835\udc56 \u2212\ud835\udc61 \ud835\udc57 |. To reduce computation and memory cost,\nthis preprocessing is performed only once, and the resulting \ud835\udc47 is\nshared across all temporal encoders. Then, we apply a nonlinear\ntransformation to \ud835\udc47 using a power function. This design prevents\ninsufficient learning of long-term user preferences that can occur\nwhen excessively long temporal intervals produce very weak sig-\nnals. Finally, we adopt an exponential function to model the decay\nof user interests over time. Formally, the temporal attention matrix\n\ud835\udc34\ud835\udc61\ud835\udc60 \u2208R \ud835\udc5b\u00d7\ud835\udc5b used in Section 3.3.1 is defined as:\n\ud835\udc34\ud835\udc61\ud835\udc60 =\ud835\udefc\u00b7\ud835\udefe \ud835\udc47 \ud835\udefd\n,i.e.,\ud835\udc34 \ud835\udc56,\ud835\udc57\n\ud835\udc61\ud835\udc60 =\ud835\udefc\u00b7\ud835\udefe |\ud835\udc61\ud835\udc56 \u2212\ud835\udc61 \ud835\udc57 |\ud835\udefd\n, \ud835\udefe\u2208 (0,1),(7)\nwhere \ud835\udefc\u2208R is a learnable parameter representing the base interest\nintensity, \ud835\udefd\u2208R is a learnable parameter controlling the nonlinear-\nity of scaling transformation, and \ud835\udefe\u2208 ( 0, 1) is a decay parameter\nthat governs the rate of interest attenuation. A smaller \ud835\udefe induces\nfaster decay, emphasizing short-term patterns, whereas a larger\n\ud835\udefe preserves more long-term signals. This design offers flexibility\nacross diverse user behaviors and application scenarios, enabling\nthe model to capture both short-term and long-term preferences in\na simple yet expressive manner.\n3.4.2 Pre-Conversion of Data Type.In Equation 7, the parameters\n\ud835\udefc, \ud835\udefd, and \ud835\udefe are represented in float32, while the relative temporal\nmatrix \ud835\udc47 is originally in int64. Due to the inconsistency of data\ntypes, computing \ud835\udc47 \ud835\udefd implicitly introduces type casting at runtime,\nwhich incurs additional time overhead and becomes more costly\nwhen repeated across multiple layers. To mitigate this issue, we ex-\nplicitly cast \ud835\udc47 to float32 during preprocessing. This pre-conversion\nensures that all subsequent operations, such as power and exponen-\ntial calculations, are performed using the same data type, thereby\navoiding runtime casting and improving computational efficiency.\nThis simple optimization brings notable performance gains: (1) The\nexecution time of our exponential-power temporal encoder is re-\nduced by 64.82%. (2) Overall FuXi-\ud835\udefe achieves an additional 12.61%\nspeedup and 5.08% reduction in memory usage during training, and\na 15.53% speedup and 6.98% memory reduction during inference.\n3.4.3 Analysis.Our proposed exponential-power temporal encoder\nachieves strong performance in both architectural compatibility\nand computational efficiency (see Section 4.3 for details). Existing\nbucket-based method, as used in HSTU [55] and FuXi-\ud835\udefc [52], ap-\nplies logarithmic transformations to temporal intervals and then\nmaps them into discrete indices for bucket lookup. Although this\napproach offers some modeling flexibility, it suffers from several\nkey limitations: (1) The design does not reflect how user interests\nevolve over time. (2) Coarse-grained discretization leads to a loss of\n\nFuXi-\ud835\udefe KDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea\ndetail, as different temporal intervals may fall into the same bucket.\n(3) Most importantly, the irregular memory access during bucket\nlookup creates a critical efficiency bottleneck, especially when han-\ndling long sequences. FuXi-\ud835\udefd [53] proposes an inverse-proportion\nfunction to represent temporal decay, which helps reduce memory\naccess overhead and partly captures interest attenuation. However,\nit lacks adaptability due to the untunable decay pattern, which\noverly favors recent interactions and limits flexibility in modeling\nlonger-term user preferences. In contrast, our exponential-power\nencoder is inspired by cognitive memory theory and operates en-\ntirely through standard matrix computations. This design enables\nmore accurate modeling of adaptive-range interests and ensures\nsmooth and efficient execution on modern hardware.\n3.5 Diagonal-Sparse Positional Mechanism\nPositional information is explicitly encoded in the embedding layer\nand implicitly reflected through temporal signals. Therefore, we\nidentify and prune redundant weights in relative positional at-\ntention, further enhancing the inference performance of FuXi- \ud835\udefe.\nSpecifically, we design the diagonal-sparse positional mechanism,\nwhich consists of three steps: (1) Block-level division of the po-\nsitional attention map. (2) Importance scoring of each block. (3)\nSelection of important blocks for sparse attention computation.\nBlock Division Leftmost Scoring Diagonal-sliding Selection Block Sparse Attention\nFigure 2: Illustration of diagonal-sparse positional mecha-\nnism. In this example, sequence length \ud835\udc5b= 8, stride size \ud835\udc60= 2,\nand configured pruning ratio \ud835\udf0f= 50%. Red Blocks are pruned\ndue to lower importance scores. Only the remaining green\nblocks participate in positional attention computation.\n3.5.1 Block Division.Element-level unstructured pruning offers\naccuracy advantages but incurs considerable overhead in hardware\ndeployment. Row-level and column-level structured pruning are\nmore hardware-friendly but may overlook critical sparse patterns,\nleading to degraded accuracy [48]. To balance trade-off, we adopt\nblock-level semi-structured pruning strategy to retain accuracy\nand improve deployability. Specifically, we partition the positional\nattention map into several blocks of size\ud835\udc60\u00d7\ud835\udc60 , where\ud835\udc60 is configurable\nstride size. To accommodate sequences whose length \ud835\udc5b can not be\ndivisible by \ud835\udc60, we apply zero-padding to the top and right edges of\nattention map until the dimension becomes divisible. Since FuXi-\ud835\udefe\nadopts a decoder-only architecture, the upper-triangular region\nof the attention map is masked with 0. Therefore, our padding\noperation does not damage the original meaningful information.\n3.5.2 Leftmost Importance Scoring.An effective scoring method\nshould robustly identify the high-utility regions of attention map\nwhile maintaining computational efficiency. To this end, we pro-\npose the leftmost scoring method as shown in Figure 2. Given that\nattention map reflects contribution through weighted summation,\nwe use the sum of absolute values within each block as a proxy for\nimportance. To reduce computation, we utilize the persymmetry of\nToeplitz matrix: \ud835\udc4a \ud835\udc56,\ud835\udc57\n\ud835\udc5d\ud835\udc5c\ud835\udc60 =\ud835\udc4a \ud835\udc56+\ud835\udc5a,\ud835\udc57+\ud835\udc5a\n\ud835\udc5d\ud835\udc5c\ud835\udc60 , as established in Section 3.3.1.\nThis means all blocks lying on the same diagonal share identical\nvalues. Consequently, it suffices to compute importance scores only\nfor the blocks in the leftmost column, each of which uniquely rep-\nresents one diagonal slice of the entire map. Our scoring method\nis effective for two key reasons: (1) Coverage Guarantee: Since\neach positional weight is included in at least one leftmost block, the\nscoring process ensures full coverage of attention weights. (2) Struc-\ntural Awareness: The leftmost column intersects with all slash-like\npatterns (i.e., diagonals), enabling efficient detection of meaningful\nstructures and guiding sparse attention accordingly.\n3.5.3 Diagonal-Sliding Selection.Based on importance scores, we\npropose a diagonal-sliding block selection method to generate the\nfinal sparse attention mask. The formal algorithm is summarized\nin Algorithm 1. Let \ud835\udc5b denote the sequence length, \ud835\udc60 the configured\nstride size, and \ud835\udf0f\u2208 [ 0, 1] the configured pruning ratio. After esti-\nmating the importance scores of \ud835\udc5b\n\ud835\udc60 leftmost blocks, we identify the\ntop-\ud835\udc58 most unimportant blocks, where \ud835\udc58=\n\u0004 \ud835\udc5b\n\ud835\udc60 \u00b7\ud835\udf0f\n\u0005\n. Then, we slide\nthese identified blocks diagonally across the entire attention map to\ngenerate final sparse mask. This overall sparsity mechanism effec-\ntively and efficiently identifies and prunes redundant blocks within\nthe positional attention map in a diagonally structured manner.\nAlgorithm 1:Diagonal-Sliding Sparse Mask Generation\nInput: Positional attention map \ud835\udc4a\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208R \ud835\udc5b\u00d7\ud835\udc5b ; Stride size \ud835\udc60; Pruning ratio \ud835\udf0f\nOutput:Sparse mask\ud835\udc40\n1Block Division:\n2\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\u2190 \ud835\udc5b\n\ud835\udc60\n3\ud835\udc59\ud835\udc52 \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc5c\ud835\udc60\ud835\udc61_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\u2190\ud835\udc4a \ud835\udc5d\ud835\udc5c\ud835\udc60 [:,:\ud835\udc60].view(\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60, \ud835\udc60, \ud835\udc60)\n4Leftmost Importance Scoring:\n5\ud835\udc4e\ud835\udc4f\ud835\udc60_\ud835\udc60\ud835\udc62\ud835\udc5a\u2190sum( |\ud835\udc59\ud835\udc52 \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc5c\ud835\udc60\ud835\udc61_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60|,dim=(2,3) )\n6\ud835\udc5a\ud835\udc4e\ud835\udc65_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\u2190\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\u00d7\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\u22121\n7\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\u2190 \u230a\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\u00d7\ud835\udf0f\u230b\n8_, \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60_\ud835\udc61\ud835\udc5c_\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\u2190TopK(\ud835\udc4e\ud835\udc4f\ud835\udc60_\ud835\udc60\ud835\udc62\ud835\udc5a,k=\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58,largest=False)\n9\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc59_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60\u2190\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60_\ud835\udc61\ud835\udc5c_\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\u00d7\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60\n10Diagonal-Sliding Selection:\n11\ud835\udc40\u2190 [ ]\n12\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc52\ud835\udc5b\ud835\udc61\u2190\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc4f\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc58\ud835\udc60+1\n13foreach\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\u2208\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc59_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60do\n14while\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\u2264\ud835\udc5a\ud835\udc4e\ud835\udc65_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65do\n15\ud835\udc40 .append(\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65)\n16\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\u2190\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65+\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc52\ud835\udc5b\ud835\udc61\n17end\n18end\n19return\ud835\udc40",
          "translated": "**\ubc29\ubc95\ub860**\n\n\ubcf8 \uc139\uc158\uc5d0\uc11c\ub294 \ub2e4\uc74c \ud56d\ubaa9 \uc21c\ucc28 \ucd94\ucc9c \uc791\uc5c5\uc758 \uacf5\uc2dd\uc801 \uc815\uc758\ub85c \uc2dc\uc791\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c FuXi-\ud835\udefe\uc758 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98\ub97c \uc81c\uc2dc\ud558\uace0 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\ub97c \uc790\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n3.1 \ubb38\uc81c \uc815\uc758\n\uc21c\ucc28 \ucd94\ucc9c\uc758 \uc8fc\uc694 \ubaa9\ud45c\ub294 \uc0ac\uc6a9\uc790\uc758 \uc5ed\uc0ac\uc801 \uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4\uc5d0 \ub530\ub77c \ub2e4\uc74c \uc0c1\ud638 \uc791\uc6a9\ud560 \ud56d\ubaa9\uc744 \uc608\uce21\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ud615\uc2dd\uc801\uc73c\ub85c, U = {\ud835\udc621, \ud835\udc622, . . . , \ud835\udc62| U |}\ub97c \uc0ac\uc6a9\uc790\uc758 \uc9d1\ud569\uc73c\ub85c \uc815\uc758\ud558\uace0, V = {\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63| V |}\ub97c \ud56d\ubaa9\uc758 \uc9d1\ud569\uc73c\ub85c \uc815\uc758\ud569\ub2c8\ub2e4.\n\uac01 \uc0ac\uc6a9\uc790 \ud835\udc62\u2208 U\uc5d0 \ub300\ud574, \uc0c1\ud638 \uc791\uc6a9 \uae30\ub85d\uc740 \uc2dc\uac04 \uc21c\uc11c\ub300\ub85c \ub41c \uc2dc\ud000\uc2a4 \ud835\udc46\ud835\udc62 = {\u210e(\ud835\udc63(\ud835\udc62)1, \ud835\udc61(\ud835\udc62)1), (\ud835\udc63(\ud835\udc62)2, \ud835\udc61(\ud835\udc62)2), . . . , (\ud835\udc63(\ud835\udc62)\ud835\udc5b\ud835\udc62, \ud835\udc61(\ud835\udc62)\ud835\udc5b\ud835\udc62)\ud835\udc56}\ub85c \ud45c\ud604\ub418\uba70, \uc5ec\uae30\uc11c \uac01 \ud29c\ud50c (\ud835\udc63(\ud835\udc62)\ud835\udc56, \ud835\udc61(\ud835\udc62)\ud835\udc56)\ub294 \ud835\udc61(\ud835\udc62)\ud835\udc56 \uc2dc\uac04\uc5d0 \uc0c1\ud638 \uc791\uc6a9\ub41c \ud56d\ubaa9\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \ubaa9\ud45c\ub294 \ud835\udc46\ud835\udc62\uc758 \uc5ed\uc0ac\ub97c \ubc14\ud0d5\uc73c\ub85c \ub2e4\uc74c \uc0c1\ud638 \uc791\uc6a9\ud560 \ud56d\ubaa9 \ud835\udc63(\ud835\udc62)\ud835\udc5b\ud835\udc62+1\uc744 \uc608\uce21\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, \ud835\udc43\ud835\udc63(\ud835\udc62)\ud835\udc5b\ud835\udc62+1=\ud835\udc63|\ud835\udc46\ud835\udc62\u0011, \u2200\ud835\udc63\u2208 V\ub97c \ucd94\uc815\ud569\ub2c8\ub2e4.\n\ud6c8\ub828 \uc911\uc5d0 \ubaa8\ub378\uc740 \uc2dc\ud000\uc2a4 \ud835\udc46\ud835\udc62\uc758 \uac01 \uc704\uce58 \ud835\udc56\uc5d0 \ub530\ub77c \ub2e4\uc74c \ud56d\ubaa9 \ud835\udc63(\ud835\udc62)\ud835\udc56+1\uc744 \uc608\uce21\ud558\ub3c4\ub85d \ucd5c\uc801\ud654\ub429\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc6d0\ud558\ub294 \ucd9c\ub825 \uc2dc\ud000\uc2a4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: {\ud835\udc63(\ud835\udc62)2, \ud835\udc63(\ud835\udc62)3, . . . , \ud835\udc63(\ud835\udc62)\ud835\udc5b\ud835\udc62+1} [22].\n3.2 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98\nFuXi-\ud835\udefe\uc758 \uc804\uccb4 \uc544\ud0a4\ud14d\ucc98\ub294 \uadf8\ub9bc 1\uc5d0 \ub098\ud0c0\ub0a9\ub2c8\ub2e4. FuXi-\ud835\udefe[53]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba74\uc11c, FuXi-\ud835\udefe\ub294 \ucffc\ub9ac-\ud0a4 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998 \uc5c6\uc774 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\ub97c \ucc44\ud0dd\ud558\uc5ec \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \ub192\uc774\uba74\uc11c \ubaa8\ub378\ub9c1 \ub2a5\ub825\uc744 \ubcf4\uc874\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, FuXi-\ud835\udefe\ub294 (1) \uc784\ubca0\ub529 \ub808\uc774\uc5b4: \ud559\uc2b5 \uac00\ub2a5\ud55c \ud56d\ubaa9 \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc785\ub825 \ud56d\ubaa9\uc744 \uc778\ucf54\ub529\ud558\uace0 \uc808\ub300\uc801\uc778 \uc704\uce58 \uc815\ubcf4\ub97c \ud1b5\ud569\ud569\ub2c8\ub2e4. (2) FuXi-\ud835\udefe \ube14\ub85d: \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\ud2b8\ub9ad\uc2a4 \ud835\udc38\u2208R | V | \u00d7\ud835\udc51\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac01 \ud56d\ubaa9 \ud835\udc63\u2208 V\ub97c \ud835\udc51\ucc28\uc6d0 \ubca1\ud130\ub85c \ub9e4\ud551\ud569\ub2c8\ub2e4. \ub610\ud55c, \ud559\uc2b5 \uac00\ub2a5\ud55c \uc704\uce58 \uc784\ubca0\ub529\uc744 \ucd94\uac00\ud558\uc5ec \uc808\ub300\uc801\uc778 \uc704\uce58 \uc815\ubcf4\ub97c \ud1b5\ud569\ud569\ub2c8\ub2e4. \ud835\udc52\ud835\udc56\uc640 \ud835\udc5d\ud835\udc56\ub294 \ud835\udc56\ubc88\uc9f8 \uc704\uce58\uc758 \ud56d\ubaa9 \uc784\ubca0\ub529\uacfc \uc704\uce58 \uc784\ubca0\ub529\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc0ac\uc6a9\uc790 \uc2dc\ud000\uc2a4 \ud835\udc46 \ud835\udc62\uc5d0 \ub300\ud574, \uc784\ubca0\ub529 \ub808\uc774\uc5b4\uc758 \ucd9c\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: \ud835\udc4b0 = {\ud835\udc52(\ud835\udc62)1 +\ud835\udc5d 1, . . . , \ud835\udc52(\ud835\udc62)\ud835\udc5b\ud835\udc62 +\ud835\udc5d \ud835\udc5b\ud835\udc62 ,0, . . . ,0} (1), \uc5ec\uae30\uc11c \ubca1\ud130 0\uc740 \ud835\udc5b\ud835\udc62+1\ubd80\ud130 \ud835\udc5b\uae4c\uc9c0\uc758 \ud328\ub529 \uc704\uce58\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4.\n\uacb0\uacfc \uc784\ubca0\ub529 \ud835\udc4b0\ub294 \uc0ac\uc6a9\uc790 \uad00\uc2ec \ud328\ud134\uc744 \ucea1\ucc98\ud558\uae30 \uc704\ud574 FuXi-\ud835\udefe \ube14\ub85d\uc744 \ud1b5\ud574 \ud1b5\uacfc\ub429\ub2c8\ub2e4. FuXi-\ud835\udefe \ube14\ub85d\uc758 \uc124\uacc4 \uc138\ubd80 \uc815\ubcf4\ub294 \uc139\uc158 3.3\uc5d0\uc11c \uc124\uba85\ud569\ub2c8\ub2e4.\n3.2.2 \uc608\uce21 \ub808\uc774\uc5b4 & \ucd5c\uc801\ud654 \ubaa9\uc801\n3.2.1 \uc784\ubca0\ub529 \ub808\uc774\uc5b4. \uc0ac\uc6a9\uc790 \uc0c1\ud638 \uc791\uc6a9 \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub300\ud55c \ubcc0\ub3d9\uc131\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574, \uc784\ubca0\ub529 \ub808\uc774\uc5b4 \uc804\uc5d0 \uac01 \uc0ac\uc6a9\uc790\uc758 \uc0c1\ud638 \uc791\uc6a9 \uae30\ub85d\uc744 \uace0\uc815\ub41c \uae38\uc774 \ud835\udc5b\uc73c\ub85c \uc815\uaddc\ud654\ud569\ub2c8\ub2e4. \ud835\udc5b\ubcf4\ub2e4 \uae34 \uc2dc\ud000\uc2a4\ub294 \uc798\ub77c\ub0b4\uace0, \uc9e7\uc740 \uc2dc\ud000\uc2a4\ub294 \ud2b9\uc218 \"\ud328\ub529 \ud56d\ubaa9\"\uc73c\ub85c \ud328\ub529\ud569\ub2c8\ub2e4.\n3.2.2 \uc608\uce21 \ub808\uc774\uc5b4 & \ucd5c\uc801\ud654 \ubaa9\uc801.\n... (\uc774\ud6c4 \ub0b4\uc6a9 \uc0dd\ub7b5)"
        },
        {
          "name": "Experiment",
          "original": "4.1 Experimental Setup\nTable 1: Statistics of the datasets.\nDatasets #Users #Items #Actions Avg. Len.\nML-1M 6,040 3,706 1,000,209 165.60\nML-20M 138,493 26,744 20,000,263 144.41\nKuaiRand 25,634 7,550 1,432,897 55.90\nIndustrial 28,927,689 476,544 1,313,729,225 45.41\n4.1.1 Datasets.To evaluate the performance of our FuXi- \ud835\udefe, we\nconduct experiments on four real-world datasets, including three\npublic datasets and one large-scale industrial dataset. These datasets\ninvolve three different scenarios, including movie, video, and music.\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al.\nTable 2: Overall recommendation performance comparison on the public datasets. The best result is highlighted in bold, and\nthe second-best result is underlined. Superscript * indicates the statistically significant improvements (i.e., two-sided t-test\nwith \ud835\udc5d < 0.05). NG@10 and NG@50 stand for NDCG@10 and NDCG@50, respectively. Infer. denotes the inference latency: we\nnormalize our FuXi-\ud835\udefe\u2019s latency to 1.00, and report the latencies of all other models as multiples relative to FuXi-\ud835\udefe.\nModel ML-1M ML-20M KuaiRand\nHR@10 HR@50 NG@10 NG@50 MRRHR@10 HR@50 NG@10 NG@50 MRR Infer.HR@10 HR@50 NG@10 NG@50 MRR\nLinRec 0.2374 0.4923 0.1301 0.1860 0.1124 0.1629 0.3950 0.0845 0.1350 0.0748 0.98 0.0799 0.2454 0.0396 0.0750 0.0382\nFLASH 0.2731 0.5391 0.1506 0.2096 0.1288 0.2484 0.5028 0.1368 0.1928 0.1179 1.26 0.0977 0.2844 0.0495 0.0896 0.0468\nGRU4Rec 0.2840 0.5460 0.1551 0.2130 0.1311 0.2411 0.4987 0.1311 0.1877 0.1128 1.00 0.0954 0.2686 0.0482 0.0854 0.0452\nSASRec 0.2832 0.5478 0.1583 0.2170 0.1360 0.2645 0.5225 0.1465 0.2033 0.1257 1.05 0.1003 0.2878 0.0505 0.0909 0.0475\nLRURec 0.2910 0.5550 0.1641 0.2227 0.1409 0.2732 0.5326 0.1514 0.2086 0.1295 1.21 0.0825 0.2391 0.0415 0.0751 0.0394\nMamba4Rec 0.3140 0.5775 0.1766 0.2351 0.1500 0.2889 0.5491 0.1619 0.2194 0.1384 1.35 0.0903 0.2560 0.0455 0.0810 0.0427\nLLaMa 0.3075 0.5799 0.1704 0.2305 0.1441 0.3045 0.5685 0.1723 0.2306 0.1472 1.26 0.1071 0.3060 0.0542 0.0971 0.0508\nHSTU 0.2955 0.5727 0.1667 0.2279 0.1433 0.2929 0.5582 0.1653 0.2239 0.1418 1.27 0.1050 0.2911 0.0535 0.0936 0.0498\nFuXi-\ud835\udefc 0.3205 0.5892 0.1814 0.2409 0.1545 0.3359 0.5971 0.1956 0.2534 0.1677 1.34 0.1108 0.3081 0.0555 0.0981 0.0513\nFuXi-\ud835\udefd 0.3207 0.5811 0.1864 0.2443 0.1605 0.3324 0.5909 0.1945 0.2517 0.1673 1.03 0.1064 0.2979 0.0534 0.0946 0.0495\nFuXi-\ud835\udefe 0.3255 0.5928 0.1850 0.2443 0.1576 0.3346 0.5918 0.1960 0.2529 0.1684 1.00 0.1118 0.3082 0.0562 0.0984 0.0518\nLinRec-Large 0.0385 0.1301 0.0189 0.0381 0.0200 0.0483 0.1276 0.0235 0.0404 0.0221 0.94 0.0503 0.1153 0.0247 0.0386 0.0223\nFLASH-Large 0.2807 0.5464 0.1552 0.2140 0.1324 0.2908 0.5470 0.1639 0.2205 0.1402 1.78 0.0983 0.2852 0.0492 0.0894 0.0464\nGRU4Rec-Large0.2285 0.4857 0.1226 0.1790 0.1054 0.2001 0.4524 0.1048 0.1601 0.0910 1.07 0.0885 0.2463 0.0446 0.0786 0.0419\nSASRec-Large 0.0375 0.1303 0.0177 0.0371 0.0186 0.0414 0.1218 0.0207 0.0382 0.0208 1.12 0.0470 0.1054 0.0251 0.0376 0.0233\nLRURec-Large0.0392 0.1325 0.0188 0.0384 0.0195 0.0454 0.1288 0.0225 0.0404 0.0220 1.64 0.0525 0.1201 0.0269 0.0410 0.0242\nMamba4Rec-Large0.0505 0.1563 0.0253 0.0477 0.0254 0.0441 0.1367 0.0220 0.0418 0.0220 1.64 0.0513 0.1361 0.0268 0.0448 0.0254\nLLaMa-Large 0.3259 0.5920 0.1856 0.2450 0.1584 0.3459 0.6069 0.2020 0.2598 0.1730 1.78 0.1073 0.3042 0.0540 0.0964 0.0503\nHSTU-Large 0.3293 0.5930 0.1867 0.2454 0.1586 0.3405 0.6007 0.1992 0.2568 0.1710 1.79 0.1065 0.2944 0.0535 0.0938 0.0493\nFuXi-\ud835\udefc-Large 0.3321 0.5908 0.1919 0.2493 0.1641 0.3535 0.6086 0.2095 0.2660 0.1801 2.03 0.1107 0.3072 0.0552 0.0974 0.0508\nFuXi-\ud835\udefd-Large 0.3390 0.6020 0.1945 0.2529 0.1656 0.3551 0.6106 0.2109 0.2675 0.1814 1.15 0.1079 0.3046 0.0543 0.0966 0.0505\nFuXi-\ud835\udefe-Large0.3423* 0.6029* 0.1975* 0.2552* 0.1682*0.3588* 0.6137* 0.2135* 0.2700* 0.1836*1.000.1137* 0.3162* 0.0571* 0.1006* 0.0525*\nFor dataset preprocessing, we follow the common practice in [52,\n53, 55]. The statistics of the processed datasets are shown in Table 1.\n\u2022 MovieLens[ 12]: This is a widely used benchmark dataset for\nevaluating recommendation algorithms. The dataset includes\nusers\u2019 rating and tagging activities. In this paper, we adopt two\nwell-established versions for our experiments, i.e., MovieLens-\n1M (ML-1M1) and MovieLens-20M (ML-20M2).\n\u2022 KuaiRand[ 7]: The dataset3 is collected from sequential interac-\ntion logs of Kuaishou, a prominent short-video sharing mobile\napplication. This platform shows high engagement, averaging\n50+ interactions per active user.\n\u2022 Industrial: This dataset is constructed from the user records of\nan industrial mainstream music app, which has tens of millions\nof active users every month. This complex dataset can better\nevaluate the robustness and effectiveness of models.\n4.1.2 Baselines.For a competitive comparison, we evaluate FuXi- \ud835\udefe\nagainst a broad set of representative baselines spanning diverse ar-\nchitectures, including LinRec [33], FLASH [18], GRU4Rec [16], SAS-\nRec [22], LRURec [54], Mamba4Rec [31], LLaMa [10], HSTU [55],\nFuXi-\ud835\udefc [52], and FuXi- \ud835\udefd [53]. These baselines collectively cover\nthe key and efficient paradigms in modern sequential recommen-\ndation, such as RNN-based, Transformer-based, and Mamba-based\narchitectures, ensuring a fair and comprehensive evaluation.\n4.1.3 Evaluation Metrics.We employ three well-established eval-\nuation metrics to evaluate recommendation performance: Hit Ra-\ntio (HR), Normalized Discounted Cumulative Gain (NDCG), and\nMean Reciprocal Rank (MRR) [52, 55]. HR@K measures whether\nthe ground-truth item appears within the top-K positions of the\nrecommendation list. NDCG@K evaluates top-K recommendation\nquality by assigning higher scores to relevant items ranked closer\n1https://grouplens.org/datasets/movielens/1m/\n2https://grouplens.org/datasets/movielens/20m/\n3https://kuairand.com/\nto the top. MRR evaluates the ranking quality by computing the\nreciprocal rank of the first relevant item in the recommendation\nresults. For all these metrics, higher values indicate better recom-\nmendation performance. Following common practice [52, 53], we\nreport HR@K and NDCG@K with K = 10, 50 by default.\n4.1.4 Implementation Details.We implement FuXi- \ud835\udefe using Py-\nTorch4. For fair comparison, all models share the same hyper-\nparameter settings, primarily following FuXi-\ud835\udefc [52] and FuXi-\ud835\udefd [53].\nSpecifically, we use the AdamW optimizer [35] with a learning rate\nof 0.001 and a batch size of 128. The hidden dimensions are set to\n50, 64, 256, and 256 for ML-1M, KuaiRand, ML-20M, and Industrial\ndatasets, respectively. The dropout rate is set to 0.2, and the num-\nber of negative samples is set to 128. For our FuXi-\ud835\udefe, the temporal\nencoder\u2019s decay parameter\ud835\udefe is set to 0.8 for movie and video scenar-\nios, and 0.9 for music. This setting reflects the intuition that music\nconsumption typically exhibits stronger long-term user preferences,\nthus requiring a slower decay rate to preserve long-range interest\nsignals, whereas movie/video interactions are more sensitive to\nrecent behaviors. To evaluate base modeling capacity, we set the\nnumber of layers to 2. To analyze scaling effects, we extend the\nmodels to 8 layers (i.e., 4\u00d7 deeper) and denote the variants as \"XX-\nLarge\". For the Industrial dataset, we adopt 4 layers, consistent with\nthe deployed online model. To enable efficient large-scale training,\nwe utilize multi-NPU parallelism via the Accelerate library [26].\n4.2 Overall Performance\n4.2.1 Recommendation Performance on Public Datasets.As shown\nin Table 2, we summarize the key observations as follows:\n\u2022 Our proposed FuXi-\ud835\udefe consistently outperforms state-of-the-art\nbaselines across all datasets. Under the 8-layer configuration,\nFuXi-\ud835\udefe surpasses other autoregressive models by an average of\n4https://pytorch.org/\n\nFuXi-\ud835\udefe KDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea\n3.79% in HR@10, 2.37% in HR@50, 4.46% in NDCG@10, 3.49% in\nNDCG@50, and 4.24% in MRR. Even with shallow 2-layer con-\nfiguration, FuXi-\ud835\udefe still achieves the best or second-best perfor-\nmance, demonstrating its strong ability to capture user interest\npatterns. Notably, our proposed exponential-power temporal\nencoder contributes meaningfully to this improvement. An in-\ndepth study of this module is provided in Section 4.3.\n\u2022 Generative models based on autoregressive architectures overall\noutperform the traditional methods, which aligns with previous\nfindings [11, 49, 52, 53, 55]. For example, increasing the depth of\nGRU4Rec or SASRec from 2 to 8 layers results in performance\ndegradation, indicating that traditional architectures struggle\nto model complex item dependencies. In contrast, generative\nmodels exhibit strong scaling effects, highlighting their potential\nfor more expressive sequence recommendation modeling.\n\u2022 Our FuXi-\ud835\udefe achieves excellent inference efficiency, surpassed\nonly by LinRec on ML-20M. The efficiency gains primarily stem\nfrom three factors: (1) Nearly pure matrix-based architecture:\nEach FuXi-\ud835\udefe block consists almost entirely of hardware-friendly\nmatrix operations, enabling high hardware utilization. (2) Min-\nimalistic single-head attention: FuXi-\ud835\udefe employs no query-key\nattention and no multi-head expansion, avoiding the costly pro-\njections, scaling operations, activations, head-wise computa-\ntions, and tensor-splitting overheads typical of Transformers.\n(3) Efficient temporal encoder: The exponential-power temporal\nencoder relies on continuous matrix operations rather than dis-\ncrete bucket lookups, eliminating irregular memory access. Fur-\nther evaluation of efficiency against strong generative baselines\nacross varying sequence lengths is provided in Section 4.2.4.\nTable 3: Overall recommendation performance comparison\non the industrial dataset. The best result is highlighted in\nbold, and the second-best result is underlined. Superscript *\nindicates the statistically significant improvements (i.e., two-\nsided t-test with\ud835\udc5d< 0.05).\nModel Industrial\nHR@10 HR@50 NG@10 NG@50 MRR\nSASRec 0.2954 0.5742 0.1633 0.2256 0.1397\nLLaMa 0.3714 0.6231 0.2216 0.2779 0.1907\nHSTU 0.3831 0.6336 0.2294 0.2855 0.1972\nFuXi-\ud835\udefc 0.4060 0.6483 0.2479 0.3022 0.2137\nFuXi-\ud835\udefd 0.4734 0.6776 0.3174 0.3633 0.2811\nFuXi-\ud835\udefe 0.5064* 0.6894* 0.3559* 0.3969* 0.3196*\n4.2.2 Recommendation Performance on Industrial Dataset.As shown\nin Table 3, FuXi-\ud835\udefe shows more significant advantages on our large-\nscale industrial music dataset. Compared with other autoregressive\nbaselines, it achieves25.06%higher HR@10 and42.86%higher\nNDCG@10 on average. Against the strongest baseline FuXi-\ud835\udefd, our\nFuXi-\ud835\udefe achieves gains of 6.97% in HR@10, 1.74% in HR@50, 12.13%\nin NDCG@10, 9.25% in NDCG@50, and 13.70% in MRR. These re-\nsults further demonstrate that FuXi-\ud835\udefe generalizes well in various\napplication scenarios, benefiting from our temporal encoder that\nadaptively models both long-term and short-term user interests.\n4.2.3 Ablation Study.Table 4 reports the results of ablation studies\non the three public datasets. Among all components, our proposed\nexponential-power temporal encoder proves to be the most critical.\nIt allows the model to assign differentiated weights to items based\non time intervals, effectively capturing the temporal evolution of\nuser preferences. The positional encoder contributes moderately.\nSince item representations already incorporate absolute positional\nembeddings, and temporal information implicitly encodes sequen-\ntial order, the relative positional channel provides marginal yet\ncomplementary gains. The SwiGLU FFN\u2019s impact is relatively mi-\nnor, suggesting that FuXi-\ud835\udefe\u2019s dual-channel self-attention layer suf-\nficiently models sequence dependencies. Nevertheless, its implicit\ninteraction provides a minor boost to model expressiveness.\nTable 4: Ablation study based on FuXi-\ud835\udefe.\nSetting ML-1M ML-20M KuaiRand\nHR@10 NG@10HR@10 NG@10HR@10 NG@10\nFuXi-\ud835\udefe 0.3423 0.1975 0.3588 0.2135 0.1137 0.0571\nw/o SwiGLU FFN0.3361 0.1931 0.3496 0.2059 0.1117 0.0570\nw/o Positional Encoder0.3281 0.1865 0.3548 0.2096 0.1113 0.0559\nw/o Temporal Encoder0.3120 0.1767 0.3248 0.1882 0.1043 0.0521\n4.2.4 Efficiency Performance.Having established the effectiveness\nof FuXi-\ud835\udefe, we now evaluate its computational efficiency across\nvarying sequence lengths. Comparisons are made against other\nautoregressive generative models, which uniformly outperform\ntraditional baselines in both accuracy and scaling effects. We report\nresults on KuaiRand, with consistent trends observed across other\ndatasets. (1) Training Efficiency: As shown in Figure 3(a), FuXi-\ud835\udefe\nstably achieves the highest training efficiency across all sequence\nlengths, with its advantage growing more pronounced at longer\nsequences. At a length of 1000, it achieves speedups of4.74\u00d7, 4.48\u00d7,\nand 1.86\u00d7 over LLaMa, HSTU, and FuXi-\ud835\udefd, respectively. Notably,\nFuXi-\ud835\udefc encounters out-of-memory (OOM) issue at this scale, due to\nits computationally intensive three-channel architecture. (2) Infer-\nence Efficiency: Figure 3(b) further demonstrates FuXi-\ud835\udefe\u2019s leading\ninference efficiency, with the performance gap similarly widen-\ning at longer sequence lengths. At length 1000, FuXi- \ud835\udefe achieves\ninference speedups of6.18 \u00d7, 6.07\u00d7, and 2.24\u00d7 over LLaMa, HSTU,\nand FuXi-\ud835\udefd, respectively. These results underscore FuXi-\ud835\udefe\u2019s strong\nscalability and practical suitability for long-sequence recommen-\ndation, attributed to its streamlined dual-channel architecture and\nlightweight exponential-power temporal encoder.\n/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000013\n/uni00000014/uni0000001b\n/uni00000016/uni00000019\n/uni00000018/uni00000017\n/uni0000001a/uni00000015\n/uni0000001c/uni00000013/uni00000056/uni00000012/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000044\n/uni0000002b/uni00000036/uni00000037/uni00000038\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\nOOM\n(a) Training Efficiency\n/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000013\n/uni0000001a\n/uni00000014/uni00000017\n/uni00000015/uni00000014\n/uni00000015/uni0000001b/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni0000005b/uni00000014/uni00000013/uni00000003/uni00000056/uni0000000c\n/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000044\n/uni0000002b/uni00000036/uni00000037/uni00000038\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\nOOM (b) Inference Efficiency\nFigure 3: Overall efficiency performance comparison.\n4.3 Study of Temporal Encoder\n4.3.1 Compatibility.As shown in Table 5, we evaluate the com-\npatibility of different temporal encoders by integrating them into\nthree representative state-of-the-art architectures. Our exponential-\npower temporal encoder robustly yields the highest accuracy im-\nprovements across all architectures on all datasets. On average,\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al.\nTable 5: Compatibility comparison of temporal encoders.\nML-1M ML-20M KuaiRandArchitecture Temporal Encoder HR@10 NG@10 MRR HR@10 NG@10 MRR HR@10 NG@10 MRR\nw/o 0.3117 0.1719 0.1444 0.3151 0.1805 0.1544 0.1004 0.0502 0.0465\nUsed in HSTU, FuXi-\ud835\udefc 0.3293 0.1867 0.1586 0.3405 0.1992 0.1710 0.1065 0.0535 0.0493\nProposed in FuXi-\ud835\udefd 0.3224 0.1853 0.1589 0.3410 0.1995 0.1713 0.1048 0.0530 0.0492HSTU\nOurs 0.3312 0.1897 0.1618 0.3426 0.2012 0.1729 0.1084 0.0551 0.0513\nw/o 0.3104 0.1738 0.1476 0.3226 0.1870 0.1602 0.1020 0.0515 0.0483\nUsed in HSTU, FuXi-\ud835\udefc 0.3321 0.1919 0.1641 0.3535 0.2095 0.1801 0.1107 0.0552 0.0508\nProposed in FuXi-\ud835\udefd 0.3338 0.1891 0.1599 0.3538 0.2098 0.1803 0.1071 0.0539 0.0500FuXi-\ud835\udefc\nOurs 0.3361 0.1940 0.1657 0.3540 0.2104 0.1810 0.1114 0.0566 0.0520\nw/o 0.3120 0.1767 0.1508 0.3248 0.1882 0.1611 0.1043 0.0521 0.0488\nUsed in HSTU, FuXi-\ud835\udefc 0.3350 0.1931 0.1655 0.3550 0.2107 0.1811 0.1111 0.0557 0.0512\nProposed in FuXi-\ud835\udefd 0.3390 0.1945 0.1656 0.3551 0.2109 0.1814 0.1079 0.0543 0.0505FuXi-\ud835\udefe, FuXi-\ud835\udefd\nOurs 0.3423 0.1975 0.1682 0.3588 0.2135 0.1836 0.1137 0.0571 0.0525\n/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000013\n/uni00000015/uni00000019\n/uni00000018/uni00000015\n/uni0000001a/uni0000001b\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000016/uni00000013/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni0000004c/uni00000051/uni00000003/uni0000002b/uni00000036/uni00000037/uni00000038/uni0000000f/uni00000003/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000032/uni00000058/uni00000055/uni00000056\n(a) Under Different Max. Seq. Lengths\n/uni00000016/uni00000015/uni00000019/uni00000017/uni00000014/uni00000015/uni0000001b/uni00000015/uni00000018/uni00000019/uni00000018/uni00000014/uni00000015\n/uni00000025/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048\n/uni00000013\n/uni00000018\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c\n/uni00000038/uni00000056/uni00000048/uni00000047/uni00000003/uni0000004c/uni00000051/uni00000003/uni0000002b/uni00000036/uni00000037/uni00000038/uni0000000f/uni00000003/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000032/uni00000058/uni00000055/uni00000056 (b) Under Different Batch Sizes\nFigure 4: Efficiency comparison of temporal encoders.\nit improves HR@10, NDCG@10, and MRR by 8.82%,11.16%, and\n11.15%, respectively, compared to the non-temporal baseline. This\nperformance gain stems from its alignment with human memory\ndecay patterns via exponential modeling. In addition, the temporal\nencoders used in HSTU, FuXi-\ud835\udefc, and FuXi-\ud835\udefd also lead to varying\ndegrees of improvement, reaffirming the critical role of temporal\nmodeling in sequential recommendation task.\n4.3.2 Efficiency.Figure 4 presents a comparative analysis of the\ncomputational efficiency of our exponential-power temporal en-\ncoder against two strong baselines. Across all sequence lengths, our\nencoder achieves the lowest latency, owing to its continuous mem-\nory access pattern, lightweight computation, and pre-conversion of\ndata type. In contrast, bucket-based encoder (e.g., adopted by HSTU\nand FuXi-\ud835\udefc) exhibits severe degradation in efficiency as sequence\nlength increases. This is primarily due to their discontinuous mem-\nory addressing, which is fundamentally incompatible with modern\nparallel hardware architectures. Although FuXi-\ud835\udefd improves upon\nthis with an inverse-proportional decay function, it still falls short\nof our method. At a sequence length of 1000, our encoder achieves\n11.00\u00d7 and 2.52\u00d7 speedup over two baselines, respectively. Fur-\nthermore, while the latency of all encoders scales proportionally\nwith batch size, ours reliably maintains superior efficiency.\n4.3.3 Visualization.Figure 5 illustrates the encoding patterns of\ndifferent temporal encoders. The horizontal axis represents the rel-\native time interval between two items, and the vertical axis denotes\nthe corresponding attention weight assigned. We visualize the first-\nlayer temporal encoders of FuXi-\ud835\udefc, FuXi-\ud835\udefd, and FuXi-\ud835\udefe trained on\nML-1M and ML-20M. All encoders follow the intuitive pattern that\n/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013\n|ti tj|\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000037/uni00000048/uni00000050/uni00000053/uni00000052/uni00000055/uni00000044/uni0000004f/uni00000003/uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n(a) Layer-1 on ML-1M\n/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013\n|ti tj|\n/uni0000001b/uni00000011/uni00000013\n/uni00000017/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000019\n/uni00000014/uni00000011/uni00000019\n/uni00000017/uni00000011/uni0000001b\n/uni0000001b/uni00000011/uni00000013\n/uni00000037/uni00000048/uni00000050/uni00000053/uni00000052/uni00000055/uni00000044/uni0000004f/uni00000003/uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n (b) Layer-1 on ML-20M\nFigure 5: Visualization comparison of temporal encoders.\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000015/uni00000018\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000016/uni0000001b\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000018/uni00000014\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000019/uni00000017\n/uni00000013/uni00000011/uni00000016/uni00000018/uni0000001a/uni0000001a\n/uni00000013/uni00000011/uni00000016/uni00000018/uni0000001c/uni00000013/uni0000002b/uni00000035/uni00000023/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000013/uni0000001c/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000014/uni0000001a\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000015/uni0000001b\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000016/uni0000001c\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000018/uni00000013\n/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni0000002b/uni00000035/uni00000023/uni00000014/uni00000013\n/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n(a) On ML-20M\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c\n/uni00000013/uni00000011/uni00000014/uni00000013/uni0000001b/uni00000018\n/uni00000013/uni00000011/uni00000014/uni00000013/uni0000001c/uni00000019\n/uni00000013/uni00000011/uni00000014/uni00000014/uni00000013/uni0000001a\n/uni00000013/uni00000011/uni00000014/uni00000014/uni00000014/uni0000001b\n/uni00000013/uni00000011/uni00000014/uni00000014/uni00000015/uni0000001c\n/uni00000013/uni00000011/uni00000014/uni00000014/uni00000017/uni00000013/uni0000002b/uni00000035/uni00000023/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000018/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000018/uni0000001c\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000019/uni00000016\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000019/uni0000001a\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a/uni00000014\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a/uni00000018\n/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni0000002b/uni00000035/uni00000023/uni00000014/uni00000013\n/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013 (b) On KuaiRand\nFigure 6: Hyper-parameter study of\ud835\udefe.\nlonger intervals receive lower weights, emphasizing the importance\nof recent interactions. FuXi-\ud835\udefd exhibits a steep initial decline, allo-\ncating excessive weights to very short-term intervals and limiting\nlong-range modeling. FuXi-\ud835\udefc shows a sawtooth pattern, caused\nby its bucketization strategy, where multiple time intervals are\nmapped to the same bucket, and adjacent buckets are not smoothly\nconnected. In contrast, our FuXi-\ud835\udefe generates a smooth, continuous\ndecay curve, thanks to its exponential-power function design. This\nallows it to differentiate both short- and long-term intervals.\n4.3.4 Impact of Hyper-Parameter \ud835\udefe.Figure 6 presents the impact\nof the decay-rate hyper-parameter \ud835\udefe on model performance in two\ndomains: movie and video. As\ud835\udefe increases, temporal decay becomes\nslower. We observe that\ud835\udefe= 0.8achieves the best performance in\nboth domains, balancing the preservation of long-term preferences\nwith sensitivity to short-term variations. A too-fast decay (e.g.,\nsmaller \ud835\udefe) weakens long-range signals, while a too-slow decay\n\nFuXi-\ud835\udefe KDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea\n/uni0000005a/uni00000012/uni00000052/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000014\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000015\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000016\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000017/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni00000025/uni00000048/uni00000056/uni00000057/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000016/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000016/uni00000016\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000015/uni0000001c\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000014/uni00000015\n(a) NDCG@10 on ML-20M\n (b) FLOPs on ML-20M\nFigure 7: Impact of configured pruning ratio\ud835\udf0fon ML-20M.\n/uni0000005a/uni00000012/uni00000052/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000017\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000019\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001b/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni00000025/uni00000048/uni00000056/uni00000057/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a/uni00000014\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a/uni00000016\n/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a/uni00000017\n/uni00000013/uni00000011/uni00000013/uni00000018/uni00000019/uni0000001b\n(a) NDCG@10 on KuaiRand\n (b) FLOPs on KuaiRand\nFigure 8: Impact of configured pruning ratio \ud835\udf0f on KuaiRand.\n(e.g., larger \ud835\udefe) dilutes recent behavior effects. As mentioned in\nSection 4.1.4, we achieve the best results on our industrial music\ndataset with \ud835\udefe= 0.9, aligning with the domain characteristic that\nmusic preferences are more persistent and long-term oriented.\n4.4 Study of Sparse Positional Mechanism\nAlthough FuXi-\ud835\udefe already exhibits strong efficiency, we further in-\nvestigate deployment-oriented optimizations by introducing the\ndiagonal-sparse positional mechanism. We evaluate its effects on\nboth effectiveness and efficiency across three representative sce-\nnarios: ML-20M (movie), KuaiRand (video), and Industrial (music).\n4.4.1 Impact of Configured Pruning Ratio \ud835\udf0f.As shown in Figure 7,\non ML-20M, increasing the pruning ratio \ud835\udf0f leads to a mild and ac-\nceptable accuracy drop. Even at\ud835\udf0f= 60%, FuXi-\ud835\udefe retains 98.92% of its\noriginal accuracy, still outperforming the strongest competing base-\nline. Meanwhile, the FLOPs of positional attention are reduced by\n74.56%, highlighting substantial computational savings. As shown\nin Figure 8, on KuaiRand, we observe a slight accuracy improve-\nment at moderate sparsity levels, suggesting that pruning redun-\ndant attention blocks can even enhance generalization. At \ud835\udf0f= 60%,\nNDCG@10 decreases by only 0.53%, while FLOPs are reduced by\n70.68%. As shown in Figure 12 in Appendix C, on Industrial, the\nsparsity mechanism incurs almost no accuracy loss, substantially\nimproving its deployability. These results confirm that our semi-\nstructured pruning method effectively eliminates redundancy in\npositional attention while retaining critical information.\n4.4.2 Impact of Stride Size \ud835\udc60.Table 6 summarizes the effect of vary-\ning stride size \ud835\udc60 on accuracy and parameter density, using ML-20M\nas a representative dataset. The results show that \ud835\udc60= 8offers the\nbest trade-off, achieving strong accuracy with relatively low param-\neter density. Smaller\ud835\udc60 leads to finer-grained block division, enabling\nTable 6: Impact of stride size\ud835\udc60on ML-20M.\nStride\ud835\udc60=4\ud835\udc60=8\ud835\udc60=16\ud835\udc60=32\nHR@10 0.3574 0.3576 0.3556 0.3567\nNG@10 0.2126 0.2129 0.2115 0.2119\nDensity 33.14% 33.29% 33.47% 36.84%\n/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\nn\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000014/uni00000017\n/uni00000013/uni00000011/uni00000015/uni00000014/uni0000001b\n/uni00000013/uni00000011/uni00000015/uni00000015/uni00000015/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni0000005a/uni00000012/uni00000052/uni00000020/uni00000015/uni00000013/uni00000008\n/uni00000020/uni00000017/uni00000013/uni00000008\n/uni00000020/uni00000019/uni00000013/uni00000008\nFigure 9: Impact of sequence length\ud835\udc5bon ML-20M.\nmore precise sparsity, but it narrows the scoring range and limits\nparallelism due to fragmented block structures. In contrast, larger \ud835\udc60\nresults in coarse-grained blocks, which reduces the effectiveness of\nsparsity and may fail to capture fine-grained redundancy. Thus, the\nstride size should be adaptively selected based on the deployment\ncontext to balance accuracy, density, and efficiency.\n4.4.3 Impact of Sequence Length \ud835\udc5b.Figure 9 summarizes the robust-\nness of our sparsity mechanism across different sequence lengths,\nusing ML-20M as a representative dataset. The results show that\nthe method remains uniformly stable: even at \ud835\udf0f= 60%, FuXi-\ud835\udefe\npreserves over 98.65% of its original accuracy, and the performance\ndrop further diminishes as the sequences become longer. This trend\nsuggests that longer input sequences may contain higher redun-\ndancy in positional interactions, making them more amenable to\npruning. Consequently, our pruning method holds greater potential\nfor practical deployment in long-sequence recommender systems.",
          "translated": "**\uc2e4\ud5d8**\n\n4.1 \uc2e4\ud5d8 \uc124\uc815\n\ud45c 1: \ub370\uc774\ud130\uc14b \ud1b5\uacc4.\n\ub370\uc774\ud130\uc14b | \uc0ac\uc6a9\uc790 \uc218 | \uc544\uc774\ud15c \uc218 | \uc561\uc158 \uc218 | \ud3c9\uade0 \uae38\uc774\n------- | -------- | -------- | -------- | --------\nML-1M | 6,040 | 3,706 | 1,000,209 | 165.60\nML-20M | 138,493 | 26,744 | 20,000,263 | 144.41\nKuaiRand | 25,634 | 7,550 | 1,432,897 | 55.90\nIndustrial | 28,927,689 | 476,544 | 1,313,729,225 | 45.41\n\n4.1.1 \ub370\uc774\ud130\uc14b. FuXi- \ud835\udefe\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \uc138 \uac1c\uc758 \uacf5\uac1c \ub370\uc774\ud130\uc14b\uacfc \ud558\ub098\uc758 \ub300\uaddc\ubaa8 \uc0b0\uc5c5 \ub370\uc774\ud130\uc14b\uc744 \ud3ec\ud568\ud55c \ub124 \uac00\uc9c0 \uc2e4\uc81c \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc2e4\ud5d8\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ub370\uc774\ud130\uc14b\uc740 \uc601\ud654, \ube44\ub514\uc624 \ubc0f \uc74c\uc545\uc744 \ud3ec\ud568\ud55c \uc138 \uac00\uc9c0 \ub2e4\ub978 \uc2dc\ub098\ub9ac\uc624\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.\n\nKDD \u201926, 8\uc6d4 9\uc77c-13\uc77c, \uc81c\uc8fc\ub3c4, \ub300\ud55c\ubbfc\uad6d  \ub370\uc774\uc9c0 \uc774 \uc678 \uc5f0\uad6c\uc9c4\n\ud45c 2: \uacf5\uac1c \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc804\ubc18\uc801\uc778 \ucd94\ucc9c \uc131\ub2a5 \ube44\uad50. \ucd5c\uc801\uc758 \uacb0\uacfc\ub294 \uad75\uac8c \ud45c\uc2dc\ub418\uace0, \ub450 \ubc88\uc9f8 \ucd5c\uc801\uc758 \uacb0\uacfc\ub294 \ubc11\uc904\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc22b\uc790 *\ub294 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ubbf8\ud55c \uac1c\uc120 \uc0ac\ud56d\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4 (\uc989, p < 0.05\uc758 \uc774\uc6d0\uce21 \uac80\uc815). NG@10 \ubc0f NG@50\ub294 NDCG@10 \ubc0f NDCG@50\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. Infer.\ub294 \ucd94\ub860 \uc9c0\uc5f0 \uc2dc\uac04\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 FuXi-\ud835\udefe\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc744 1.00\uc73c\ub85c \uc815\uaddc\ud654\ud558\uace0 \ubaa8\ub4e0 \ub2e4\ub978 \ubaa8\ub378\uc758 \uc9c0\uc5f0 \uc2dc\uac04\uc744 FuXi-\ud835\udefe\uc5d0 \ub300\ud55c \ubc30\uc218\ub85c \ubcf4\uace0\ud569\ub2c8\ub2e4.\n\ubaa8\ub378 | ML-1M | ML-20M | KuaiRand\n------- | -------- | -------- | --------\nHR@10 |  |  |  |\nHR@50 |  |  |  |\nNG@10 |  |  |  |\nNG@50 |  |  |  |\nMRR |  |  |  |\nInfer. |  |  |  |\nHR@10 |  |  |  |\nHR@50 |  |  |  |\nNG@10 |  |  |  |\nNG@50 |  |  |  |\nMRR |  |  |  |\nLinRec | 0.2374 | 0.4923 | 0.1301 | 0.1860 | 0.1124 | 0.1629 | 0.3950 | 0.0885 | 0.2463 | 0.0446 | 0.0786 | 0.0419\nSASRec | 0.0375 | 0.1303 | 0.0177 | 0.0371 | 0.0186 | 0.0414 | 0.1218 | 0.0207 | 0.0382 | 0.0208 | 1.12 | 0.0470 | 0.1054 | 0.0251 | 0.0376 | 0.0233\nLRURec | 0.0392 | 0.1325 | 0.0188 | 0.0384 | 0.0195 | 0.0454 | 0.1288 | 0.0225 | 0.0404 | 0.0220 | 1.64 | 0.0525 | 0.1201 | 0.0269 | 0.0410 | 0.0242\nMamba4Rec | 0.0505 | 0.1563 | 0.0253 | 0.0477 | 0.0254 | 0.0441 | 0.1367 | 0.0220 | 0.0418 | 0.0220 | 1.64 | 0.0513 | 0.1361 | 0.0268 | 0.0448 | 0.0254\nLLaMa | 0.3259 | 0.5920 | 0.1856 | 0.2450 | 0.1584 | 0.3459 | 0.6069 | 0.2020 | 0.2598 | 0.1730 | 1.78 | 0.1073 | 0.3042 | 0.0540 | 0.0964 | 0.0503\nHSTU | 0.3293 | 0.5930 | 0.1867 | 0.2454 | 0.1586 | 0.3405 | 0.6007 | 0.1992 | 0.2568 | 0.1710 | 1.79 | 0.1065 | 0.2944 | 0.0535 | 0.0938 | 0.0493\nFuXi-\ud835\udefc | 0.3321* | 0.5908 | 0.1919 | 0.2493 | 0.1641 | 0.3535 | 0.6086 | 0.2095 | 0.2660 | 0.1801 | 2.03 | 0.1107 | 0.3072 | 0.0552 | 0.0974 | 0.0508\nFuXi-\ud835\udefd | 0.3390 | 0.6020 | 0.1945 | 0.2529 | 0.1656 | 0.3551 | 0.6106 | 0.2109 | 0.2675 | 0.1814 | 1.15 | 0.1079 | 0.3046 | 0.0543 | 0.0966 | 0.0505\nFuXi-\ud835\udefe | 0.3423* | 0.6029* | 0.1975* | 0.2552* | 0.1682* | 0.3588* | 0.6137* | 0.2135* | 0.2700* | 0.1836* | 1.000 | 0.1137* | 0.3162* | 0.0571* | 0.1006* | 0.0525*\n\n\ub370\uc774\ud130 \uc804\ucc98\ub9ac\uc5d0 \ub300\ud574, [52, 53, 55]\uc5d0\uc11c \ucc44\ud0dd\ud55c \uc77c\ubc18\uc801\uc778 \uad00\ud589\uc744 \ub530\ub790\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130\uc14b \ud1b5\uacc4\ub294 \ud45c 1\uc5d0 \ub098\uc640 \uc788\uc2b5\ub2c8\ub2e4.\n\u2022 MovieLens[ 12]: \uc774 \ub370\uc774\ud130\uc14b\uc740 \ucd94\ucc9c \uc54c\uace0\ub9ac\uc998\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\uc14b\uc5d0\ub294 \uc0ac\uc6a9\uc790 \ud3c9\uc810 \ubc0f \ud0dc\uae45 \ud65c\ub3d9\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 MovieLens-1M (ML-1M1) \ubc0f MovieLens-20M (ML-20M2)\ub77c\ub294 \ub450 \uac00\uc9c0 \ud655\ub9bd\ub41c \ubc84\uc804\uc744 \ucc44\ud0dd\ud588\uc2b5\ub2c8\ub2e4.\n\u2022 KuaiRand[ 7]: \ub370\uc774\ud130\uc14b3\uc740 Kuaishou\uc758 \uc21c\ucc28\uc801 \uc0c1\ud638 \uc791\uc6a9 \ub85c\uadf8\uc5d0\uc11c \uc218\uc9d1\ub41c \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. Kuaishou\ub294 \ud3c9\uade0 50+ \uc0c1\ud638 \uc791\uc6a9\uc744 \uac16\ub294 \ub192\uc740 \ucc38\uc5ec\uc728\uc744 \ubcf4\uc774\ub294 \ub300\ud45c\uc801\uc778 \ub2e8\ud3b8 \ube44\ub514\uc624 \uacf5\uc720 \ubaa8\ubc14\uc77c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc785\ub2c8\ub2e4.\n\u2022 Industrial: \uc774 \ub370\uc774\ud130\uc14b\uc740 \uc218\ubc31\ub9cc \uba85\uc758 \ud65c\uc131 \uc0ac\uc6a9\uc790\uac00 \ub9e4\ub2ec \ud65c\ub3d9\ud558\ub294 \uc0b0\uc5c5\uc6a9 \uc8fc\uc694 \uc74c\uc545 \uc571\uc758 \uc0ac\uc6a9\uc790 \uae30\ub85d\uc5d0\uc11c \uad6c\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ubcf5\uc7a1\ud55c \ub370\uc774\ud130\uc14b\uc740 \ubaa8\ub378\uc758 \uacac\uace0\uc131\uacfc \ud6a8\uacfc\ub97c \ub354 \uc798 \ud3c9\uac00\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.\n\n(\uc0dd\ub7b5)"
        },
        {
          "name": "Conclusion",
          "original": "In this paper, we propose a novel sequential recommendation frame-\nwork, named FuXi-\ud835\udefe, to address the dual challenges of efficiency\nand effectiveness in modeling user interests. Our FuXi-\ud835\udefe introduces\ntwo core innovations: Exponential-Power Temporal Encoder and\nDiagonal-Sparse Positional Mechanism. Exponential-power tempo-\nral encoder is inspired by cognitive memory theory and involves\nonly standard matrix operations, enabling it to flexibly capture both\nshort-term and long-term user interests while ensuring efficient exe-\ncution on modern hardware. Diagonal-sparse positional mechanism\nemploys a diagonally semi-structured manner to effectively identify\nand prune redundant attention blocks, thereby reducing compu-\ntational cost while preserving recommendation quality. Extensive\nexperiments on four real-world datasets demonstrate that our pro-\nposed FuXi-\ud835\udefe consistently outperforms state-of-the-art baselines in\nboth recommendation accuracy and computational efficiency. In\nfuture work, we plan to further improve the efficiency and extend\nthe framework to capture more complex user behavior patterns.",
          "translated": "\uacb0\ub860\n\n\ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131\uacfc \ud6a8\uacfc\uc801\uc778 \uc0ac\uc6a9\uc790 \uad00\uc2ec\uc0ac \ubaa8\ub378\ub9c1\uc774\ub77c\ub294 \ub450 \uac00\uc9c0 \uacfc\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \uc0c8\ub85c\uc6b4 \uc2dc\ud000\uc2a4 \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c, FuXi-\ud835\udefe\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4. FuXi-\ud835\udefe\ub294 \ud575\uc2ec\uc801\uc778 \ub450 \uac00\uc9c0 \ud601\uc2e0\uc744 \ub3c4\uc785\ud569\ub2c8\ub2e4. \uccab\uc9f8, \uc9c0\uac01\uc801 \uae30\uc5b5 \uc774\ub860\uc5d0\uc11c \uc601\uac10\uc744 \uc5bb\uc740 \uc9c0\uc218-\uac70\ub4ed\uc81c\uacf1 \uc2dc\uac04 \uc778\ucf54\ub354(Exponential-Power Temporal Encoder)\ub294 \ud45c\uc900 \ud589\ub82c \uc5f0\uc0b0\ub9cc\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uae30 \ubc0f \uc7a5\uae30 \uc0ac\uc6a9\uc790 \uad00\uc2ec\uc0ac\ub97c \uc720\uc5f0\ud558\uac8c \ud3ec\ucc29\ud558\uba74\uc11c \ud604\ub300 \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \ud6a8\uc728\uc801\uc778 \uc2e4\ud589\uc744 \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \ub458\uc9f8, \ub300\uac01-\ud76c\uc18c \uc704\uce58 \uba54\ucee4\ub2c8\uc998(Diagonal-Sparse Positional Mechanism)\uc740 \ub300\uac01 \ubc18\uc815\ud615 \ubc29\uc2dd\uc73c\ub85c \uc791\ub3d9\ud558\uc5ec \uc911\ubcf5\ub41c \uc5b4\ud150\uc158 \ube14\ub85d\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc2dd\ubcc4\ud558\uace0 \uc81c\uac70\ud568\uc73c\ub85c\uc368 \uacc4\uc0b0 \ube44\uc6a9\uc744 \uc904\uc774\uba74\uc11c \ucd94\ucc9c \ud488\uc9c8\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \uc2e4\uc81c \ub370\uc774\ud130 \uc138\ud2b8 4\uac1c\ub97c \ud65c\uc6a9\ud55c \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8 \uacb0\uacfc, \uc81c\uc548\ub41c FuXi-\ud835\udefe\uac00 \ucd94\ucc9c \uc815\ud655\ub3c4 \ubc0f \uacc4\uc0b0 \ud6a8\uc728\uc131 \uce21\uba74\uc5d0\uc11c \ucd5c\uc2e0 \uae30\uc220 \uc218\uc900\uc758 \uae30\ubc18 \ubaa8\ub378\ub4e4\uc744 \uc9c0\uc18d\uc801\uc73c\ub85c \ub2a5\uac00\ud568\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ud5a5\ud6c4 \uc5f0\uad6c\uc5d0\uc11c\ub294 \ud6a8\uc728\uc131\uc744 \ub354\uc6b1 \uac1c\uc120\ud558\uace0 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ud655\uc7a5\ud558\uc5ec \ub354\uc6b1 \ubcf5\uc7a1\ud55c \uc0ac\uc6a9\uc790 \ud589\ub3d9 \ud328\ud134\uc744 \ud3ec\ucc29\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4."
        },
        {
          "name": "Acknowledgments",
          "original": "This work is supported by the National Natural Science Foundation\nof China (62372253), the Natural Science Foundation of Tianjin Fund\n(23JCYBJC00010), and Nankai University School of Optometry and\nVision Science Open Fund Program (NKSGP202308). The work is\nalso sponsored by Huawei Innovation Research Program.\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al.",
          "translated": "This work is supported by the National Natural Science Foundation\nof China (62372253), the Natural Science Foundation of Tianjin Fund\n(23JCYBJC00010), and Nankai University School of Optometry and\nVision Science Open Fund Program (NKSGP202308). The work is\nalso sponsored by Huawei Innovation Research Program.\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al."
        },
        {
          "name": "References",
          "original": "[1] Junyi Chen, Lu Chi, Bingyue Peng, and Zehuan Yuan. 2024. Hllm: Enhancing\nsequential recommendations via hierarchical large language models for item and\nuser modeling.arXiv preprint arXiv:2409.12740(2024).\n[2] Sung Min Cho, Eunhyeok Park, and Sungjoo Yoo. 2020. MEANTIME: Mixture of\nattention mechanisms with multi-temporal embeddings for sequential recom-\nmendation. InProceedings of the 14th ACM Conference on recommender systems.\n515\u2013520.\n[3] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-\nlength context.arXiv preprint arXiv:1901.02860(2019).\n[4] Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models\nand efficient algorithms through structured state space duality.arXiv preprint\narXiv:2405.21060(2024).\n[5] Hermann Ebbinghaus. 1885. Memory: a contribution to experimental psychol-\nogy. teachers college, columbia university, new york.Trans. HA Ruger and CE\nBussenius. Original work published(1885).\n[6] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units\nfor neural network function approximation in reinforcement learning.Neural\nnetworks107 (2018), 3\u201311.\n[7] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,\nPeng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Rec-\nommendation Dataset with Randomly Exposed Videos. InProceedings of the\n31st ACM International Conference on Information and Knowledge Management\n(Atlanta, GA, USA)(CIKM \u201922). 3953\u20133957. doi:10.1145/3511808.3557624\n[8] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi,\nJunjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. 2024. Seerattention:\nLearning intrinsic sparse attention in your llms.arXiv preprint arXiv:2410.13276\n(2024).\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin.\n2017. Convolutional sequence to sequence learning. InInternational conference\non machine learning. PMLR, 1243\u20131252.\n[10] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek\nKadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex\nVaughan, et al. 2024. The llama 3 herd of models.arXiv preprint arXiv:2407.21783\n(2024).\n[11] Wei Guo, Hao Wang, Luankang Zhang, Jin Yao Chin, Zhongzhou Liu, Kai Cheng,\nQiushi Pan, Yi Quan Lee, Wanqi Xue, Tingjia Shen, et al. 2024. Scaling new fron-\ntiers: Insights into large recommendation models.arXiv preprint arXiv:2412.00714\n(2024).\n[12] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History\nand context.Acm transactions on interactive intelligent systems (tiis)5, 4 (2015),\n1\u201319.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. InProceedings of the IEEE conference on computer\nvision and pattern recognition. 770\u2013778.\n[14] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. De-\nberta: Decoding-enhanced bert with disentangled attention.arXiv preprint\narXiv:2006.03654(2020).\n[15] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov\nchains for sparse sequential recommendation. In2016 IEEE 16th international\nconference on data mining (ICDM). IEEE, 191\u2013200.\n[16] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks.arXiv\npreprint arXiv:1511.06939(2015).\n[17] Haoji Hu, Xiangnan He, Jinyang Gao, and Zhi-Li Zhang. 2020. Modeling personal-\nized item frequency information for next-basket recommendation. InProceedings\nof the 43rd international ACM SIGIR conference on research and development in\ninformation retrieval. 1071\u20131080.\n[18] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality\nin linear time. InInternational conference on machine learning. PMLR, 9099\u20139117.\n[19] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. 2020. Improve\ntransformer models with better relative position embeddings.arXiv preprint\narXiv:2009.13658(2020).\n[20] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo,\nSurin Ahn, Zhenhua Han, Amir H Abdi, Dongsheng Li, Chin-Yew Lin, et al .\n2024. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic\nsparse attention.Advances in Neural Information Processing Systems37 (2024),\n52481\u201352515.\n[21] Eija Kaasinen, Virpi Roto, Kristin Roloff, Kaisa V\u00e4\u00e4n\u00e4nen-Vainio-Mattila, Teija\nVainio, Wolfgang Maehr, Dhaval Joshi, and Sujan Shrestha. 2009. User experience\nof mobile internet: analysis and recommendations.International Journal of Mobile\nHuman Computer Interaction (IJMHCI)1, 4 (2009), 4\u201323.\n[22] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In2018 IEEE international conference on data mining (ICDM). IEEE,\n197\u2013206.\n[23] Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet\nSahota, Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus\nBrubaker. 2019. Time2vec: Learning a vector representation of time.arXiv\npreprint arXiv:1907.05321(2019).\n[24] Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking positional encoding in\nlanguage pre-training.arXiv preprint arXiv:2006.15595(2020).\n[25] Anton Klenitskiy and Alexey Vasilev. 2023. Turning dross into gold loss: is\nbert4rec really better than sasrec?. InProceedings of the 17th ACM Conference on\nRecommender Systems. 1120\u20131125.\n[26] John P Kotter. 2012. Accelerate.Harvard business review90, 11 (2012), 43\u201358.\n[27] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. 2025. Flexprefill: A\ncontext-aware sparse attention mechanism for efficient long-sequence inference.\narXiv preprint arXiv:2502.20766(2025).\n[28] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware self-\nattention for sequential recommendation. InProceedings of the 13th international\nconference on web search and data mining. 322\u2013330.\n[29] Jun Li and Ge Zhang. 2023. Fragment and Integrate Network (FIN): A Novel\nSpatial-Temporal Modeling Based on Long Sequential Behavior for Online Food\nOrdering Click-Through Rate Prediction. InProceedings of the 32nd ACM Interna-\ntional Conference on Information and Knowledge Management. 4688\u20134694.\n[30] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. Ctrl: Connect tabular\nand language model for ctr prediction.arXiv preprint arXiv:2306.02841(2023).\n[31] Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, and James Caverlee.\n2024. Mamba4rec: Towards efficient sequential recommendation with selective\nstate space models.arXiv preprint arXiv:2403.03900(2024).\n[32] Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang,\nWenjie Niu, Xiaokun Zhu, Yongjun Bao, et al. 2020. Category-Specific CNN for\nVisual-aware CTR Prediction at JD. com. InProceedings of the 26th ACM SIGKDD\ninternational conference on knowledge discovery & data mining. 2686\u20132696.\n[33] Langming Liu, Liu Cai, Chi Zhang, Xiangyu Zhao, Jingtong Gao, Wanyu Wang,\nYifu Lv, Wenqi Fan, Yiqi Wang, Ming He, et al. 2023. Linrec: Linear attention\nmechanism for long-term sequential recommender systems. InProceedings of\nthe 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval. 289\u2013299.\n[34] Qiang Liu, Shu Wu, Diyi Wang, Zhaokang Li, and Liang Wang. 2016. Context-\naware sequential recommendation. In2016 IEEE 16th International Conference on\nData Mining (ICDM). IEEE, 1053\u20131058.\n[35] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\narXiv preprint arXiv:1711.05101(2017).\n[36] Saptarshi Mitra, Rachid Karami, Haocheng Xu, Sitao Huang, and Hyoukjun\nKwon. 2025. Characterizing state space model (ssm) and ssm-transformer hy-\nbrid language model performance with long context length.arXiv preprint\narXiv:2507.12442(2025).\n[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer.Journal of machine\nlearning research21, 140 (2020), 1\u201367.\n[38] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan,\nTrung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al.\n2023. Recommender systems with generative retrieval.Advances in Neural\nInformation Processing Systems36 (2023), 10299\u201310315.\n[39] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-\nizing personalized markov chains for next-basket recommendation. InProceedings\nof the 19th international conference on World wide web. 811\u2013820.\n[40] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with\nrelative position representations.arXiv preprint arXiv:1803.02155(2018).\n[41] Noam Shazeer. 2020. Glu variants improve transformer.arXiv preprint\narXiv:2002.05202(2020).\n[42] Jiajie Su, Chaochao Chen, Zibin Lin, Xi Li, Weiming Liu, and Xiaolin Zheng.\n2023. Personalized behavior-aware transformer for multi-behavior sequential\nrecommendation. InProceedings of the 31st ACM international conference on\nmultimedia. 6321\u20136331.\n[43] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\nresentations from transformer. InProceedings of the 28th ACM international\nconference on information and knowledge management. 1441\u20131450.\n[44] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-\ntion via convolutional sequence embedding. InProceedings of the eleventh ACM\ninternational conference on web search and data mining. 565\u2013573.\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.Advances in neural information processing systems30 (2017).\n[46] Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang. 2023.\nBERT4CTR: An Efficient Framework to Combine Pre-trained Language Model\nwith Non-textual Features for CTR Prediction. InProceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining. 5039\u20135050.\n[47] Xinfei Wang. 2020. A survey of online advertising click-through rate prediction\nmodels. In2020 IEEE International Conference on Information Technology, Big Data\n\nFuXi-\ud835\udefe KDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea\nand Artificial Intelligence (ICIBA), Vol. 1. IEEE, 516\u2013521.\n[48] Kunpeng Xie, Ye Lu, Xinyu He, Dezhi Yi, Huijuan Dong, and Yao Chen. 2024.\nWinols: A large-tiling sparse winograd CNN accelerator on FPGAs.ACM Trans-\nactions on Architecture and Code Optimization21, 2 (2024), 1\u201324.\n[49] Wenjia Xie, Hao Wang, Minghao Fang, Ruize Yu, Wei Guo, Yong Liu, Defu Lian,\nand Enhong Chen. 2025. Breaking the Bottleneck: User-Specific Optimization and\nReal-Time Inference Integration for Sequential Recommendation. InProceedings\nof the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.\n2. 3333\u20133343.\n[50] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. 2025.\nXattention: Block sparse attention with antidiagonal scoring.arXiv preprint\narXiv:2503.16428(2025).\n[51] Wenwen Ye, Shuaiqiang Wang, Xu Chen, Xuepeng Wang, Zheng Qin, and Dawei\nYin. 2020. Time matters: Sequential recommendation with complex temporal\ninformation. InProceedings of the 43rd international ACM SIGIR conference on\nresearch and development in information retrieval. 1459\u20131468.\n[52] Yufei Ye, Wei Guo, Jin Yao Chin, Hao Wang, Hong Zhu, Xi Lin, Yuyang Ye, Yong\nLiu, Ruiming Tang, Defu Lian, et al . 2025. FuXi- \ud835\udefc: Scaling Recommendation\nModel with Feature Interaction Enhanced Transformer. InCompanion Proceedings\nof the ACM on Web Conference 2025. 557\u2013566.\n[53] Yufei Ye, Wei Guo, Hao Wang, Hong Zhu, Yuyang Ye, Yong Liu, Huifeng Guo,\nRuiming Tang, Defu Lian, and Enhong Chen. 2025. Fuxi-\\beta: Towards a light-\nweight and fast large-scale generative recommendation model.arXiv preprint\narXiv:2508.10615(2025).\n[54] Zhenrui Yue, Yueqi Wang, Zhankui He, Huimin Zeng, Julian McAuley, and Dong\nWang. 2024. Linear recurrent units for sequential recommendation. InProceedings\nof the 17th ACM international conference on web search and data mining. 930\u2013938.\n[55] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhao-\njie Gong, Fangda Gu, Michael He, et al. 2024. Actions speak louder than words:\nTrillion-parameter sequential transducers for generative recommendations.arXiv\npreprint arXiv:2402.17152(2024).\n[56] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization.\nAdvances in neural information processing systems32 (2019).\n[57] Lingzi Zhang, Xin Zhou, Zhiwei Zeng, and Zhiqi Shen. 2024. Multimodal pre-\ntraining for sequential recommendation via contrastive learning.ACM Transac-\ntions on Recommender Systems3, 1 (2024), 1\u201323.\n[58] Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong,\nand Ed H Chi. 2021. A model of two tales: Dual transfer learning framework for\nimproved long-tail item recommendation. InProceedings of the web conference\n2021. 2220\u20132231.\n[59] Yihu Zhang, Bo Yang, Haodong Liu, and Dongsheng Li. 2023. A time-aware self-\nattention based neural network model for sequential recommendation.Applied\nSoft Computing133 (2023), 109894.\n[60] Chuang Zhao, Xinyu Li, Ming He, Hongke Zhao, and Jianping Fan. 2023. Sequen-\ntial recommendation via an adaptive cross-domain knowledge decomposition. In\nProceedings of the 32nd ACM international conference on information and knowl-\nedge management. 3453\u20133463.\nA Theoretical Discussion of Temporal Encoder\nThe exponential-power temporal encoder, formulated as\n\ud835\udc53(\ud835\udc65)=\ud835\udefc\u00b7\ud835\udefe \ud835\udc65 \ud835\udefd\n,(8)\nextends the classical Ebbinghaus forgetting curve into a generalized\nexponential kernel. Its derivative is\n\ud835\udc53 \u2032 (\ud835\udc65)=\ud835\udefc \ud835\udefd(ln\ud835\udefe)\ud835\udc65 \ud835\udefd\u22121 \ud835\udefe\ud835\udc65 \ud835\udefd\n.(9)\nWhen0 <\ud835\udefd< 1, the term\ud835\udc65 \ud835\udefd\u22121 =\ud835\udc65 \u2212 (1\u2212\ud835\udefd) diverges as \ud835\udc65\u2192 0, violat-\ning Lipschitz continuity and potentially introducing optimization\ninstability. To maintain Lipschitz continuity and improve training\nrobustness in this regime, a simple yet effective solution is to shift\nthe input by a small positive constant, replacing\ud835\udc65 with \ud835\udc65+\ud835\udf16(\ud835\udf16> 0).\nB Analysis of Cold-Start & Sparse-Case\nCold-start users and long-tail items are fundamental sparsity chal-\nlenges in real-world recommendation systems. To further assess\nthe robustness of FuXi-\ud835\udefe, we evaluate model performance under\nthree challenging conditions: (1) cold-start users with very limited\ninteraction histories, (2) fresh items launched within short time\nwindows, and (3) long-tail items with low exposure frequency.\nB.1 Cold-Start Users\nWe partition the industrial dataset into five groups according to\nusers\u2019 historical interaction lengths. As shown in Figure 10, our\nFuXi-\ud835\udefe robustly outperforms all baselines across all user groups,\nincluding cold-start users with extremely short sequences. The\nperformance gap becomes even larger for long-sequence users,\nindicating that the tunable exponential-power temporal encoder ef-\nfectively captures both short-term and long-term interest patterns.\n/uni00000014/uni00000061/uni00000018/uni00000019/uni00000061/uni00000015/uni00000013/uni00000015/uni00000014/uni00000061/uni00000018/uni00000013/uni00000018/uni00000014/uni00000061/uni00000014/uni00000013/uni00000013/uni00000021/uni00000014/uni00000013/uni00000013\n/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000038/uni00000056/uni00000048/uni00000055\n/uni00000013/uni00000011/uni00000016/uni0000001c\n/uni00000013/uni00000011/uni00000017/uni00000016\n/uni00000013/uni00000011/uni00000017/uni0000001a\n/uni00000013/uni00000011/uni00000018/uni00000014\n/uni00000013/uni00000011/uni00000018/uni00000018\n/uni00000013/uni00000011/uni00000018/uni0000001c/uni0000002b/uni00000035/uni00000023/uni00000014/uni00000013\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n(a) HR@10 on Industrial\n/uni00000014/uni00000061/uni00000018/uni00000019/uni00000061/uni00000015/uni00000013/uni00000015/uni00000014/uni00000061/uni00000018/uni00000013/uni00000018/uni00000014/uni00000061/uni00000014/uni00000013/uni00000013/uni00000021/uni00000014/uni00000013/uni00000013\n/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000052/uni00000049/uni00000003/uni00000038/uni00000056/uni00000048/uni00000055\n/uni00000013/uni00000011/uni00000015/uni00000015\n/uni00000013/uni00000011/uni00000015/uni00000019\n/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000013/uni00000011/uni00000016/uni00000017\n/uni00000013/uni00000011/uni00000016/uni0000001b\n/uni00000013/uni00000011/uni00000017/uni00000015/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n (b) NDCG@10 on Industrial\nFigure 10: Analysis of cold-start users on Industrial.\nB.2 Fresh Items\nWe examine the model\u2019s ability to recommend fresh items, which\nlack sufficient historical feedback and are often difficult for models\nto rank accurately. Following the item-launching-time criterion,\nwe evaluate items introduced within \u2264 1 day and \u2264 1 week. As\nshown in Figure 11, FuXi-\ud835\udefe achieves the best performance across\nthe two freshness windows. These results highlight FuXi-\ud835\udefe\u2019s strong\ngeneralization ability to newly introduced content.\n/uni00000014/uni00000003/uni00000047/uni00000044/uni0000005c\n/uni00000014/uni00000003/uni0000005a/uni00000048/uni00000048/uni0000004e\n/uni0000002f/uni00000044/uni00000058/uni00000051/uni00000046/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000050\n/uni00000013/uni00000011/uni00000016/uni0000001b\n/uni00000013/uni00000011/uni00000017/uni00000015\n/uni00000013/uni00000011/uni00000017/uni00000019\n/uni00000013/uni00000011/uni00000018/uni00000013\n/uni00000013/uni00000011/uni00000018/uni00000017/uni0000002b/uni00000035/uni00000023/uni00000014/uni00000013\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n(a) HR@10 on Industrial\n/uni00000014/uni00000003/uni00000047/uni00000044/uni0000005c\n/uni00000014/uni00000003/uni0000005a/uni00000048/uni00000048/uni0000004e\n/uni0000002f/uni00000044/uni00000058/uni00000051/uni00000046/uni0000004b/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000050\n/uni00000013/uni00000011/uni00000015/uni00000015\n/uni00000013/uni00000011/uni00000015/uni00000019\n/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000013/uni00000011/uni00000016/uni00000017\n/uni00000013/uni00000011/uni00000016/uni0000001b/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n/uni00000029/uni00000058/uni0000003b/uni0000004c/uni00000010\n (b) NDCG@10 on Industrial\nFigure 11: Analysis of fresh items on Industrial.\nB.3 Long-Tail Items\nFollowing [49, 58], we categorize the top 20% most interacted items\nas head and the remainder as long-tail. As shown in Table 7, FuXi-\ud835\udefe\nstably achieves performance gains over FuXi-\ud835\udefc and FuXi-\ud835\udefd on both\nsubsets. We attribute this advantage to FuXi-\ud835\udefe\u2019s fine-grained tem-\nporal encoding, which produces richer sequential representations\nand alleviates embedding sparsity for long-tail items.\nTable 7: Analysis of long-tail items on ML-20M.\nModel Head Tail\nHR@10 NG@10 HR@10 NG@10\nFuXi-\ud835\udefc 0.3710 0.2205 0.0747 0.0395\nFuXi-\ud835\udefd 0.3705 0.2209 0.0756 0.0409\nFuXi-\ud835\udefe 0.3749 0.2234 0.0789 0.0413\n\nKDD \u201926, August 09\u201313, 2026, Jeju Island, Republic of Korea Dezhi Yi et al.\nC Supplementary Figure\nAs described in Section 4.4.1, Figure 12 demonstrates the impact of\nconfigured pruning ratio\ud835\udf0fon Industrial dataset.\n/uni0000005a/uni00000012/uni00000052/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008\n/uni00000013/uni00000011/uni00000016/uni00000013/uni00000019\n/uni00000013/uni00000011/uni00000016/uni00000015/uni00000013\n/uni00000013/uni00000011/uni00000016/uni00000016/uni00000017\n/uni00000013/uni00000011/uni00000016/uni00000017/uni0000001b\n/uni00000013/uni00000011/uni00000016/uni00000019/uni00000015/uni00000031/uni0000002a/uni00000023/uni00000014/uni00000013\n/uni00000025/uni00000048/uni00000056/uni00000057/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000018/uni0000001c\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000018/uni0000001c\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000019/uni00000013\n/uni00000013/uni00000011/uni00000016/uni00000018/uni00000018/uni0000001c\n(a) NDCG@10 on Industrial\n (b) FLOPs on Industrial\nFigure 12: Impact of configured pruning ratio \ud835\udf0f on Industrial.\nD Efficiency Discussion about Mamba4Rec\nAs shown in Table 2, on ML-20M, Mamba4Rec is not universally\nfaster, as linear computational complexity does not guarantee supe-\nrior runtime in all settings. Mamba\u2019s advantage typically emerges\nfor very long sequences (e.g., >2k), while for shorter sequences its\nefficiency is often lower than that of Transformers [4, 36]. The rea-\nson is that achieving linear scaling relies on recurrent state updates,\nwhich introduce non-negligible constant computational overhead.\nIn particular, each state update involves complex operations such\nas matrix multiplications, gating mechanisms, and discretization,\nrequiring additional kernel launches and memory synchronization.\nThese operations exhibit limited parallelism, preventing full uti-\nlization of AI accelerators. Consequently, although Mamba avoids\nthe \ud835\udc42(\ud835\udc5b 2) attention cost, its low hardware parallelism and large\nconstant factors reduce efficiency for typical sequence lengths.",
          "translated": "[\ucc38\uace0\ubb38\ud5cc \uc0dd\ub7b5]"
        }
      ]
    },
    {
      "id": "7cd0cabf-95fb-449b-a16a-416cedab060d",
      "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
      "authors": [
        "Jiaqi Zhai",
        "Lucy Liao",
        "Xing Liu",
        "Yueming Wang",
        "Rui Li",
        "Xuan Cao",
        "Leon Gao",
        "Zhaojie Gong",
        "Fangda Gu",
        "Michael He",
        "Yinghai Lu",
        "Yu Shi"
      ],
      "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (\"Generative Recommenders\"), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.",
      "year": 2024,
      "arxiv_id": "2402.17152",
      "arxiv_url": "https://arxiv.org/abs/2402.17152",
      "conference": "ICML'24",
      "category": "recsys",
      "tags": [
        {
          "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
          "name": "Industrial"
        },
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        }
      ],
      "published_at": "2024-02-27",
      "created_at": "2026-01-29T18:36:45.020247",
      "updated_at": "2026-01-30T18:21:07.012366",
      "summary": {
        "one_line": "This paper introduces HSTU, a trillion-parameter generative recommender model leveraging sequential transduction to achieve state-of-the-art performance and scalability in recommendation systems.",
        "contribution": "The research reformulates recommendation as a sequential transduction task, enabling the development of Generative Recommenders. HSTU, a novel architecture with 1.5 trillion parameters, demonstrates improved recommendation performance and scalability compared to existing methods.",
        "methodology": "HSTU utilizes a Transformer-based architecture optimized for high-cardinality, streaming data, employing a sequential transduction approach. The model's performance scales with training compute, mirroring the scaling trends observed in large language models like GPT-3/LLaMa-2.",
        "results": "HSTU achieves up to 65.8% improvements in NDCG across synthetic and public datasets, while also demonstrating a 5.3x to 15.2x speedup over FlashAttention2-based Transformers.  Furthermore, the model's performance improved by 12.4% in online A/B tests, indicating practical applicability."
      },
      "full_translation": [
        {
          "name": "Abstract",
          "original": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers\nfor Generative Recommendations\nJiaqi Zhai 1 Lucy Liao 1 Xing Liu 1 Yueming Wang1 Rui Li 1\nXuan Cao 1 Leon Gao 1 Zhaojie Gong 1 Fangda Gu 1 Michael He 1 Yinghai Lu 1 Yu Shi1",
          "translated": "**\uc694\uc57d**\n\n\ub9d0\uc774 \uc544\ub2cc \ud589\ub3d9\uc774 \uc911\uc694: \uc0dd\uc131 \ucd94\ucc9c\uc744 \uc704\ud55c \ud2b8\ub9b4\ub9ac\uc5b8 \ud30c\ub77c\ubbf8\ud130 \uc2dc\ud000\uc15c \ud2b8\ub79c\uc2a4\ub4c0\uc11c\n\nJiaqi Zhai 1, Lucy Liao 1, Xing Liu 1, Yueming Wang 1, Rui Li 1, Xuan Cao 1, Leon Gao 1, Zhaojie Gong 1, Fangda Gu 1, Michael He 1, Yinghai Lu 1, Yu Shi 1\n\n(\ud2b8\ub9b4\ub9ac\uc5b8 \ud30c\ub77c\ubbf8\ud130(trillion-parameter) \uc2dc\ud000\uc15c \ud2b8\ub79c\uc2a4\ub4c0\uc11c(sequential transducer)\ub294 \uc2dc\ud000\uc15c \ub370\uc774\ud130(sequential data)\ub97c \ucc98\ub9ac\ud558\uace0, \uc0dd\uc131\uc801\uc778 \ucd94\ucc9c(generative recommendations)\uc744 \uc0dd\uc131\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ubaa8\ub378\uc785\ub2c8\ub2e4.)"
        },
        {
          "name": "Abstract",
          "original": "Large-scale recommendation systems are charac-\nterized by their reliance on high cardinality, het-\nerogeneous features and the need to handle tens\nof billions of user actions on a daily basis. De-\nspite being trained on huge volume of data with\nthousands of features, most Deep Learning Rec-\nommendation Models (DLRMs) in industry fail to\nscale with compute. Inspired by success achieved\nby Transformers in language and vision domains,\nwe revisit fundamental design choices in recom-\nmendation systems. We reformulate recommen-\ndation problems as sequential transduction tasks\nwithin a generative modeling framework (\u201cGen-\nerative Recommenders\u201d), and propose a new ar-\nchitecture, HSTU, designed for high cardinality,\nnon-stationary streaming recommendation data.\nHSTU outperforms baselines over synthetic and\npublic datasets by up to 65.8% in NDCG, and is\n5.3x to 15.2x faster than FlashAttention2-based\nTransformers on 8192 length sequences. HSTU-\nbased Generative Recommenders, with 1.5 trillion\nparameters, improve metrics in online A/B tests\nby 12.4% and have been deployed on multiple\nsurfaces of a large internet platform with billions\nof users. More importantly, the model quality of\nGenerative Recommenders empirically scales as a\npower-law of training compute across three orders\nof magnitude, up to GPT-3/LLaMa-2 scale, which\nreduces carbon footprint needed for future model\ndevelopments, and further paves the way for the\nfirst foundation models in recommendations.",
          "translated": "\ub300\uaddc\ubaa8 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc740 \uace0\uce74\ub514\ub110\ub9ac\ud2f0(high cardinality), \uc774\uc9c8\uc801\uc778 \ud2b9\uc9d5, \uadf8\ub9ac\uace0 \ub9e4\uc77c \uc218\uc2ed\uc5b5 \uac74\uc758 \uc0ac\uc6a9\uc790 \ud589\ub3d9\uc744 \ucc98\ub9ac\ud574\uc57c \ud55c\ub2e4\ub294 \ud2b9\uc9d5\uc73c\ub85c \uc815\uc758\ub41c\ub2e4. \uc218\ucc9c \uac1c\uc758 \ud2b9\uc9d5\uc73c\ub85c \ud6c8\ub828\ub418\uc5c8\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ub300\ubd80\ubd84\uc758 \ub525\ub7ec\ub2dd \ucd94\ucc9c \ubaa8\ub378(Deep Learning Recommendation Models, DLRMs)\ub294 \ucef4\ud4e8\ud305 \uc790\uc6d0 \ud655\uc7a5\uc5d0 \uc2e4\ud328\ud55c\ub2e4. \uc5b8\uc5b4 \ubc0f \ube44\uc804 \ubd84\uc57c\uc5d0\uc11c \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformer)\uac00 \uc131\uacf5\uc744 \uac70\ub450\uc5c8\uc73c\ubbc0\ub85c, \uc6b0\ub9ac\ub294 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \uae30\ubcf8 \uc124\uacc4 \uc120\ud0dd \uc0ac\ud56d\uc744 \uc7ac\uace0\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \uc0dd\uc131 \ubaa8\ub378\ub9c1 \ud504\ub808\uc784\uc6cc\ud06c(Generative Recommenders) \ub0b4\uc758 \uc21c\ucc28\uc801 \uc804\uc774(sequential transduction) \uc791\uc5c5\uc73c\ub85c \ucd94\ucc9c \ubb38\uc81c\ub97c \uc7ac\uad6c\uc131\ud558\uace0, \uace0\uce74\ub514\ub110\ub9ac\ud2f0, \ube44\uc815\uc0c1\uc801\uc778 \uc2a4\ud2b8\ub9ac\ubc0d \ucd94\ucc9c \ub370\uc774\ud130\uc5d0 \uc801\ud569\ud558\ub3c4\ub85d \uc124\uacc4\ub41c HSTU \uc544\ud0a4\ud14d\ucc98\ub97c \uc81c\uc548\ud55c\ub2e4. HSTU\ub294 \ud569\uc131 \ub370\uc774\ud130\uc640 \uacf5\uac1c \ub370\uc774\ud130\uc14b\uc5d0\uc11c NDCG(Normalized Discounted Cumulative Gain) \uae30\uc900\uc73c\ub85c \ucd5c\ub300 65.8%\uae4c\uc9c0 \uae30\ubc18 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, 8192 \uae38\uc774 \uc2dc\ud000\uc2a4\uc5d0\uc11c FlashAttention2 \uae30\ubc18 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ubcf4\ub2e4 5.3\ubc30\uc5d0\uc11c 15.2\ubc30 \ub354 \ube60\ub978 \uc18d\ub3c4\ub97c \ubcf4\uc778\ub2e4. 1.5\uc870 \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 Generative Recommenders \uae30\ubc18\uc758 HSTU\ub294 \uc628\ub77c\uc778 A/B \ud14c\uc2a4\ud2b8\uc5d0\uc11c \uc9c0\ud45c\ub97c 12.4% \uac1c\uc120\ud588\uc73c\uba70, \uc218\uc2ed\uc5b5 \uba85\uc758 \uc0ac\uc6a9\uc790\ub97c \uac00\uc9c4 \ub300\uaddc\ubaa8 \uc778\ud130\ub137 \ud50c\ub7ab\ud3fc\uc758 \uc5ec\ub7ec \uc601\uc5ed\uc5d0 \ubc30\ud3ec\ub418\uc5c8\ub2e4. \ub354\uc6b1 \uc911\uc694\ud55c \uc810\uc740 Generative Recommenders\uc758 \ubaa8\ub378 \ud488\uc9c8\uc774 \ud6c8\ub828 \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \ud30c\uc6cc \ub7ad\ud06c(power-law) \ud568\uc218\ub85c \ub098\ud0c0\ub0b4\uba70, GPT-3/LLaMa-2 \uaddc\ubaa8\uae4c\uc9c0 \ud655\uc7a5\ub41c\ub2e4\ub294 \uacbd\ud5d8\uc801 \uc99d\uac70\uc774\ub2e4. \uc774\ub294 \ubbf8\ub798 \ubaa8\ub378 \uac1c\ubc1c\uc5d0 \ud544\uc694\ud55c \ud0c4\uc18c \ubc1c\uc790\uad6d(carbon footprint)\uc744 \uc904\uc774\uace0, \ucd94\ucc9c \ubd84\uc57c\uc758 \uccab \ubc88\uc9f8 \uae30\ubc18 \ubaa8\ub378(foundation models)\uc758 \uae38\uc744 \uc5f4\uc5b4\uc900\ub2e4."
        },
        {
          "name": "Introduction",
          "original": "Recommendation systems, quintessential in the realm of on-\nline content platforms and e-commerce, play a pivotal role\n1MRS, Meta AI. Correspondence to: <{jiaqiz, lucyyl,\nxingl, yuemingw, ruili }@meta.com>. Code available at\nhttps://github.com/facebookresearch/generative-recommenders.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nYear\nTraining PetaFLOP/s-days\n0.01\n1\n100\n10000\n2012 2014 2016 2018 2020 2022 2024\nFigure 1. Total compute used to train deep learning models over\nthe years. DLRM results are from (Mudigere et al., 2022); GRs are\ndeployed models from this work. DLRMs/GRs are continuously\ntrained in a streaming setting; we report compute used per year.\nin personalizing billions of user experiences on a daily basis.\nState-of-the-art approaches in recommendations have been\nbased on Deep Learning Recommendation Models (DL-\nRMs) (Mudigere et al., 2022) for about a decade (Covington\net al., 2016; Cheng et al., 2016; Zhou et al., 2018; Tang et al.,\n2020; Wang et al., 2021; Xia et al., 2023). DLRMs are char-\nacterized by their usage of heterogeneous features, such as\nnumerical features \u2013 counters and ratios, embeddings, and\ncategorical features such as creator ids, user ids, etc. Due\nto new content and products being added every minute, the\nfeature space is of extreme high cardinality, often in the\nrange of billions (Eksombatchai et al., 2018). To leverage\ntens of thousands of such features, DLRMs employ various\nneural networks to combine features, transform intermediate\nrepresentations, and compose the final outputs.\nDespite utilizing extensive human-engineered feature sets\nand training on vast amounts of data, most DLRMs in in-\ndustry scale poorly with compute (Zhao et al., 2023). This\nlimitation is noteworthy and remains unanswered.\nInspired by the success achieved by Transformers in lan-\nguage and vision, we revisit fundamental design choices in\nmodern recommendation systems. We observe that alter-\nnative formulations at billion-user scale need to overcome\nthree challenges. First, features in recommendation systems\nlack explicit structures. While sequential formulations have\nbeen explored in small-scale settings (detailed discussions\n1\narXiv:2402.17152v3  [cs.LG]  6 May 2024\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nin Appendix B), heterogeneous features, including high\ncardinality ids, cross features, counters, ratios, etc. play\ncritical roles in industry-scale DLRMs (Mudigere et al.,\n2022). Second, recommendation systems use billion-scale\nvocabularies that change continuously. A billion-scale dy-\nnamic vocabulary, in contrast to 100K-scale static ones in\nlanguage (Brown et al., 2020), creates training challenges\nand necessitates high inference cost given the need to con-\nsider tens of thousands of candidates in a target-aware fash-\nion (Zhou et al., 2018; Wang et al., 2020). Finally, com-\nputational cost represents the main bottleneck in enabling\nlarge-scale sequential models. GPT-3 was trained on a total\nof 300B tokens over a period of 1-2 months with thousands\nof GPUs (Brown et al., 2020). This scale appears daunting,\nuntil we contrast it with the scale of user actions. The largest\ninternet platforms serve billions of daily active users, who\nengage with billions of posts, images, and videos per day.\nUser sequences could be of length up to 105 (Chang et al.,\n2023). Consequentially, recommendation systems need to\nhandle a few orders of magnitude more tokens per day than\nwhat language models process over 1-2 months.\nIn this work, we treat user actions as a new modality in gen-\nerative modeling. Our key insights are, a) core ranking and\nretrieval tasks in industrial-scale recommenders can be cast\nas generative modeling problems given an appropriate new\nfeature space; b) this paradigm enables us to systematically\nleverage redundancies in features, training, and inference\nto improve efficiency. Due to our new formulation, we\ndeployed models that are three orders of magnitude more\ncomputationally complex than prior state-of-the-art, while\nimproving topline metrics by 12.4%, as shown in Figure 1.\nOur contributions are as follows. We first propose Gener-\native Recommenders (GRs) in Section 2, a new paradigm\nreplacing DLRMs. We sequentialize and unify the hetero-\ngeneous feature space in DLRMs, with the new approach\napproximating the full DLRM feature space as sequence\nlength tends to infinity. This enables us to reformulate the\nmain recommendation problems, ranking and retrieval, as\npure sequential transduction tasks in GRs. Importantly, this\nfurther enables model training to be done in a sequential,\ngenerative fashion, which permits us to train on orders of\nmagnitude more data with the same amount of compute.\nWe next address computational cost challenges through-\nout training and inference. We propose a new sequential\ntransduction architecture, Hierarchical Sequential Trans-\nduction Units (HSTU). HSTU modifies attention mecha-\nnism for large, non-stationary vocabulary, and exploits char-\nacteristics of recommendation datasets to achieve 5.3x to\n15.2x speedup vs FlashAttention2-based Transformers on\n8192 length sequences. Further, through a new algorithm,\nM-FALCON, that fully amortizes computational costs via\nmicro-batching (Section 3.4), we can serve 285x more com-\nplex GR models while achieving 1.50x-2.99x speedups, all\nwith the same inference budget used by traditional DLRMs.\nWe finally validate the proposed techniques over synthetic\ndatasets, public datasets, and deployments on multiple sur-\nfaces of a large internet platform with billions of daily ac-\ntive users in Section 4. To the best of our knowledge, our\nwork represents the first result that shows pure sequential\ntransduction-based architectures, like HSTU, in generative\nsettings (GRs) to significantly outperform DLRMs in large-\nscale industrial settings. Remarkably, not only did we over-\ncome known scaling bottlenecks in traditional DLRMs, we\nfurther succeeded in showing that scaling law (Kaplan et al.,\n2020) applies to recommendations, representing the poten-\ntial ChatGPT moment for recommendation systems.\n2. Recommendation as Sequential\nTransduction Tasks: From DLRMs to GRs\n2.1. Unifying heterogeneous feature spaces in DLRMs\nModern DLRM models are usually trained with a vast num-\nber of categorical (\u2018sparse\u2019) and numerical (\u2018dense\u2019) fea-\ntures. In GRs, we consolidate and encode these features into\na single unified time series, as depicted in Figure 2.\nCategorical (\u2018sparse\u2019) features. Examples of such fea-\ntures include items that user liked, creators in a category\n(e.g., Outdoors) that user is following, user languages, com-\nmunities that user joined, cities from which requests were\ninitiated, etc. We sequentialize these features as follows. We\nfirst select the longest time series, typically by merging the\nfeatures that represent items user engaged with, as the main\ntime series. The remaining features are generally time series\nthat slowly change over time, such as demographics or fol-\nlowed creators. We compress these time series by keeping\nthe earliest entry per consecutive segment and then merge\nthe results into the main time series. Given these time series\nchange very slowly, this approach does not significantly\nincrease the overall sequence length.\nNumerical (\u2018dense\u2019) features. Examples of such features\ninclude weighted and decayed counters, ratios, etc. For in-\nstance, one feature could represent user\u2019s past click through\nrate (CTR) on items matching a given topic. Compared\nto categorical features, these features change much more\nfrequently, potentially with every single (user, item) inter-\naction. It is therefore infeasible to fully sequentialize such\nfeatures from computational and storage perspectives. How-\never, an important observation is that the categorical features\n(e.g., item topics, locations) over which we perform these\naggregations are already sequentialized and encoded in GRs.\nHence, we can remove numerical features in GRs given a\nsufficiently expressive sequential transduction architecture\ncoupled with a target-aware formulation (Zhou et al., 2018)\ncan meaningfully capture numerical features as we increase\n2\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nSequentialized+model-based unified features Sequentialized+model-based unified features Sequentialized+model-based unified features\n   Numerical features   Numerical featuresCategorical features    Numerical featuresCategorical features\nGenerative Recommenders (GRs)\nmain time series\nDeep Learning Recommendation Models (DLRMs)\nG0\nt0\n\u03a60\nt1,a0\nF0,F1\nE0,E1,...\nG0\nmerge & \nsequentialize\nG0\nt1\nG0\nt0 \n\u03a61\nt2, a1\nH0\nt7\n... ...\n\u03a60\nt1,a0\n\u03a61\nt2, a1\n\u03a6n-1\nt9, an-1\nauxiliary time series 1\nG0\nt2\n\u03a6n-1\nt9, an-1\nCTR0\n(at t1)\nH0\nt7\nH0\nt8auxiliary time series 2\n...\n...\n...\n...\n...\nCausal-masked learned features \nvia (target-aware) cross attention\nsequentialize\nsequentialize\nG1\nt8\nG1\nt8\nH0\nt9\nRatio2\n(at t1)\nreplace w/ \nmodels\n\u03a8\u20190 (t2)\u03a8\u20190 (t2) \u03a8\u20191 (t9)\u03a8\u20191 (t9)...\n...\nP(X=x)CTR0\n(at t2)\nCTR0\n(at t9)\n... ...\nRatio2\n(at t2)\nRatio2\n(at t9)\nE1,E2,... E7,E8,...\nG0 G1\nH0\n...\n...\n...\n...\nF0,F3 F7\nemit example\nemit example\nemit example\n\u03a81(t1)\u03a81(t1) \u03a82(t2)\u03a82(t2) \u03a89(t9)\u03a89(t9)\u03a81(t1) \u03a82(t2) \u03a89(t9)\nG1\nt9\nTrainingTrainingFeaturesFeatures\nFigure 2. Comparison of features and training procedures: DLRMs vs GRs. E, F, G, H denote categorical features. \u03a6i represents the i-th\nitem in the merged main time series. \u03a8k(tj) denotes training example k emitted at time tj. Full notations can be found in Appendix A.\nthe overall sequence length and compute in GRs.\n2.2. Reformulating ranking and retrieval as sequential\ntransduction tasks\nGiven a list of n tokens x0, x1, . . . , xn\u22121 (xi \u2208 X) ordered\nchronologically, the time when those tokens are observed\nt0, t1, . . . , tn\u22121, a sequential transduction task maps this\ninput sequence to the output tokens y0, y1, . . . , yn\u22121 (yi \u2208\nX \u222a {\u2205}), where yi = \u2205 indicates that yi is undefined.\nWe use \u03a6i \u2208 Xc (Xc \u2286 X) to denote a content (e.g., images\nor videos) that the system provides to the user. Given new\ncontent are constantly created, Xc and X are non-stationary.\nThe user can respond to \u03a6i with some action ai (e.g., like,\nskip, video completion+share) ai \u2208 X. We denote the total\nnumber of contents that a user has interacted with by nc.\nThe standard ranking and retrieval tasks, in causal autore-\ngressive settings, can then be defined as sequential transduc-\ntion tasks (Table 1). We make the following observations:\nTask Specification (Inputs / Outputs)\nRanking xis \u03a60, a0, \u03a61, a1, . . . ,\u03a6nc\u22121, anc\u22121\nyis a0, \u2205, a1, \u2205, . . . , anc\u22121, \u2205\nRetrieval\nxis (\u03a60, a0), (\u03a61, a1), . . . ,(\u03a6nc\u22121, anc\u22121)\nyis \u03a6\u2032\n1, \u03a6\u2032\n2, . . . ,\u03a6\u2032\nnc\u22121, \u2205\n(\u03a6\u2032\ni = \u03a6i if ai is positive, otherwise \u2205)\nTable 1. Ranking and retrieval as sequential transduction tasks.\nOther categorical features are omitted for simplicity. We compare\nGRs with traditional sequential recommenders in Appendix B.2.\nRetrieval. In recommendation system\u2019s retrieval stage, we\nlearn a distribution p(\u03a6i+1|ui) over \u03a6i+1 \u2208 Xc, where ui\nis the user\u2019s representation at tokeni. A typical objective is\nto select arg max\u03a6\u2208Xc p(\u03a6|ui) to maximize some reward.\nThis differs from a standard autoregressive setup in two\nways. First, the supervision for xi, yi, is not necessarily\n\u03a6i+1, as users could respond negatively to \u03a6i+1. Second,\nyi is undefined when xi+1 represents a non-engagement\nrelated categorical feature, such as demographics.\nRanking. Ranking tasks in GRs pose unique challenges as\nindustrial recommendation systems often require a \u201ctarget-\naware\u201d formulation. In such settings, \u201cinteraction\u201d of target,\n\u03a6i+1, and historical features needs to occur as early as\npossible, which is infeasible with a standard autoregressive\nsetup where \u201cinteraction\u201d happens late (e.g., via softmax\nafter encoder output). We address this by interleaving items\nand actions in Table 1, which enables the ranking task to\nbe formulated as p(ai+1|\u03a60, a0, \u03a61, a1, . . . ,\u03a6i+1) (before\ncategorical features). We apply a small neural network to\ntransform outputs at \u03a6i+1 into multi-task predictions in\npractice. Importantly, this enables us to apply target-aware\ncross-attention to all nc engagements in one pass.\n2.3. Generative training\nIndustrial recommenders are commonly trained in a stream-\ning setup, where each example is processed sequentially as\nthey become available. In this setup, the total computational\nrequirement for self-attention based sequential transduction\narchitectures, such as Transformers (Vaswani et al., 2017),\nscales as P\ni ni(n2\ni d + nidf fd), where ni is the number of\ntokens of user i, and d is the embedding dimension. The\nfirst part in the parentheses comes from self-attention, with\nassumed O(n2) scaling factor due to most subquadratic al-\ngorithms involving quality tradeoffs and underperforming\nquadratic algorithms in wall-clock time (Dao et al., 2022).\nThe second part comes from pointwise MLP layers, with hid-\nden layers of size O(df f) = O(d). Taking N = maxi ni,\nthe overall time complexity reduces to O(N3d + N2d2),\nwhich is cost prohibitive for recommendation settings.\nTo tackle the challenge of training sequential transduc-\n3\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\ntion models over long sequences in a scalable manner, we\nmove from traditional impression-level training to genera-\ntive training, reducing the computational complexity by an\nO(N) factor, as shown at the top of Figure 2. By doing so,\nencoder costs are amortized across multiple targets. More\nspecifically, when we sample thei-th user at rate su(ni), the\ntotal training cost now scales as P\ni su(ni)ni(n2\ni d + nid2),\nwhich is reduced to O(N2d + N d2) by setting su(ni) to\n1/ni. One way to implement this sampling in industrial-\nscale systems is to emit training examples at the end of a\nuser\u2019s request or session, resulting in \u02c6su(ni) \u221d 1/ni.\n3. A High Performance Self-Attention Encoder\nfor Generative Recommendations\nTo scale up GRs for industrial-scale recommendation sys-\ntems with large, non-stationary vocabularies, we next intro-\nduce a new encoder design, Hierarchical Sequential Trans-\nduction Unit (HSTU). HSTU consists of a stack of identical\nlayers connected by residual connections (He et al., 2015).\nEach layer contains three sub-layers: Pointwise Projection\n(Equation 1), Spatial Aggregation (Equation 2), and Point-\nwise Transformation (Equation 3):\nU(X), V (X), Q(X), K(X) = Split(\u03d51(f1(X))) (1)\nA(X)V (X) = \u03d52\n\u0010\nQ(X)K(X)T + rabp,t\n\u0011\nV (X) (2)\nY (X) = f2 (Norm (A(X)V (X)) \u2299 U(X)) (3)\nwhere fi(X) denotes an MLP; we use one linear layer,\nfi(X) = Wi(X) + bi for f1 and f2 to reduce compute\ncomplexity and further batches computations for queries\nQ(X), keys K(X), values V (X), and gating weightsU(X)\nwith a fused kernel; \u03d51 and \u03d52 denote nonlinearity, for\nboth of which we use SiLU (Elfwing et al., 2017); Norm is\nlayer norm; and rabp,t denotes relative attention bias (Raffel\net al., 2020) that incorporates positional ( p) and temporal\n(t) information. Full notations used can be found in Table 9.\nHSTU encoder design allows for the replacement of hetero-\ngeneous modules in DLRMs with a single modular block.\nWe observe that there are, effectively, three main stages\nin DLRMs: Feature Extraction, Feature Interactions, and\nTransformations of Representations. Feature Extractions re-\ntrieves the pooled embedding representations of categorical\nfeatures. Their most advanced versions can be generalized\nas pairwise attention and target-aware pooling (Zhou et al.,\n2018), which is captured with HSTU layers.\nFeature Interaction is the most critical part of DLRMs.\nCommon approaches used include factorization machines\nand their neural network variants (Rendle, 2010; Guo\net al., 2017; Xiao et al., 2017), higher order feature interac-\ntions (Wang et al., 2021), etc. HSTU replaces feature interac-\ntions by enabling attention pooled features to directly \u201cinter-\nact\u201d with other features via Norm (A(X)V (X)) \u2299 U(X).\nFeature Interactions \nNeural Networks\n(FMs, DCN, \nTransformers, DHEN, \n\u2026)\nEmbedding \nOperators / \nEmbeddings\nCategorical \nFeatures in \nDLRMs\nNumerical \nFeatures in \nDLRMs\nBottom \nNeural \nNetworks\nTop Neural \nNetworks \n(MMoE, PLE, \u2026)\nSequentialized Unified Features\nRepresentation transformationsFeature extractions Feature interactions\nRaw Features\nAdd&Norm\nAdd&Norm\nPreprocessing\nA(X)=\u03d52(QKT+rabP,T)\nU, Q, K, V = \u03d51(f1(X))\nNorm(A(X)V(X))\u2299U(X)\nU(X)\nY(X) = f2(...)\nA(X)=\u03d52(QKT+rabP,T)\nU, Q, K, V = \u03d51(f1(X))\nNorm(A(X)V(X))\u2299U(X)\nU(X)\nY(X) = f2(...)\nA(X)=\u03d52(QKT+rabP,T)\nU, Q, K, V = \u03d51(f1(X))\nNorm(A(X)V(X))\u2299U(X)\nU(X)\nY(X) = f2(...)\nA(X)=\u03d52(QKT+rabP,T)\nU, Q, K, V = \u03d51(f1(X))\nNorm(A(X)V(X))\u2299U(X)\nU(X)\nY(X) = f2(...)\nA(X)=\u03d52(QKT+rabP,T)\nU, Q, K, V = \u03d51(f1(X))\nNorm(A(X)V(X))\u2299U(X)\nU(X)\nY(X) = f2(...)\nA(X)=\u03d52(QKT+rabP,T)\nU, Q, K, V = \u03d51(f1(X))\nNorm(A(X)V(X))\u2299U(X)\nU(X)\nY(X) = f2(...)\nFigure 3. Comparison of key model components: DLRMs vs GRs.\nThe complete DLRM setup (Mudigere et al., 2022) is shown on\nthe left side and a simplified HSTU is shown on the right.\nThis design is motivated by the difficulty of approximat-\ning dot products with learned MLPs (Rendle et al., 2020;\nZhai et al., 2023a). Given SiLU is applied to U(X),\nNorm (A(X)V (X)) \u2299 U(X) can also be interpreted as a\nvariant of SwiGLU (Shazeer, 2020).\nTransformations of Representations is commonly done with\nMixture of Experts (MoEs) and routing to handle diverse,\nheterogeneous populations. A key idea is to perform condi-\ntional computations by specializing sub-networks for differ-\nent users (Ma et al., 2018; Tang et al., 2020). Element-wise\ndot products in HSTU can virtually perform gating opera-\ntions used in MoEs up to a normalization factor.\n3.1. Pointwise aggregated attention\nHSTU adopts a new pointwise aggregated (normalized) at-\ntention mechanism (in contrast, softmax attention computes\nnormalization factor over the entire sequence). This is moti-\nvated by two factors. First, the number of prior data points\nrelated to target serves as a strong feature indicating the\nintensity of user preferences, which is hard to capture after\nsoftmax normalization. This is critical as we need to predict\nboth the intensity of engagements, e.g., time spent on a given\nitem, and the relative ordering of the items, e.g., predicting\nan ordering to maximize AUC. Second, while softmax acti-\nvation is robust to noise by construction, it is less suited for\nnon-stationary vocabularies in streaming settings.\nThe proposed pointwise aggregated attention mechanism is\ndepicted in Equation (2). Importantly, layer norm is needed\nafter pointwise pooling to stabilize training. One way to\n4\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nArchitecture HR@10 HR@50\nTransformers .0442 .2025\nHSTU (-rabp,t, Softmax) .0617 .2496\nHSTU (-rabp,t) .0893 .3170\nTable 2. Synthetic data in one-pass streaming settings.\nunderstand this design is through synthetic data following\na Dirichlet Process that generates streaming data over a\nnonstationary vocabulary (details in Appendix C). In this\nsetting, we can observe gaps as large as 44.7% between\nsoftmax and pointwise attention setups as shown in Table 2.\n3.2. Leveraging and algorithmically increasing sparsity\nIn recommendation systems, the length of user history se-\nquences often follows a skewed distribution, leading to\nsparse input sequences, particularly in the settings with\nvery long sequences. This sparsity can be leveraged to sig-\nnificantly improve the efficiency of the encoder. To achieve\nthis, we have developed an efficient attention kernel for\nGPUs that fuses back-to-back GEMMs in a manner sim-\nilar to (Rabe & Staats, 2021; Dao et al., 2022) but per-\nforms fully raggified attention computations. This essen-\ntially transforms the attention computation into grouped\nGEMMs of various sizes (Appendix G). As a result, self-\nattention in HSTU becomes memory-bound and scales as\n\u0398(P\ni n2\ni d2\nqkR\u22121) in terms of memory accesses, where ni\nis the sequence length for sample i, dqk is attention dimen-\nsion, and R is the register size. This approach by itself leads\nto 2-5x throughput gains as discussed in Section 4.2.\nWe further algorithmically increase the sparsity of user his-\ntory sequences via Stochastic Length (SL). One key char-\nacteristic of user history sequences in recommendations is\nthat user behaviors are temporally repetitive, as user behav-\niors manifest at multiple scales throughout their interaction\nhistory. This represents an opportunity to increase sparsity\nartificially without compromising model quality, thereby\nsignificantly reducing encoder cost that scales as \u0398(P\ni n2\ni ).\nWe can represent user j\u2019s history as a sequence (xi)nc,j\ni=0 ,\nwhere nc,j is the number of contents user interacted with.\nLet Nc = max j nc,j. Let (xik)L\nk=0 be a subsequence of\nlength L constructed from the original sequence (xi)nc,j\ni=0 .\nSL selects input sequences as follows:\n(xi)\nnc,j\ni=0 if nc,j \u2264 N \u03b1/2\nc\n(xik)N \u03b1/2\nc\nk=0 if nc,j > N \u03b1/2\nc , w/ probability 1 \u2212 N \u03b1\nc /n2\nc,j\n(xi)\nnc,j\ni=0 if nc,j > N \u03b1/2\nc , w/ probability N \u03b1\nc /n2\nc,j\n(4)\nwhich reduces attention-related complexity to O(N \u03b1\nc d) =\nO(N \u03b1d) for \u03b1 \u2208 (1, 2]. A more thorough discussion of\nsubsequence selection can be found in Appendix F.1. We\nremark that applying SL to training leads to a cost-effective\nsystem design, as training generally involves a significantly\nhigher computational cost compared to inference.\nAlpha (\u03b1) Max Sequence Lengths\n1,024 2,048 4,096 8,192\n1.6 71.5% 76.1% 80.5% 84.4%\n1.7 56.1% 63.6% 69.8% 75.6%\n1.8 40.2% 45.3% 54.1% 66.4%\n1.9 17.2% 21.0% 36.3% 64.1%\n2.0 3.1% 6.6% 29.1% 64.1%\nTable 3. Impact of Stochastic Length (SL) on sequence sparsity.\nTable 3 presents the sparsity (see Appendix F) for different\nsequence lengths and \u03b1 values, for a representative industry-\nscale configuration with 30-day user history. The settings\nthat result in negligible regression in model quality are un-\nderlined and highlighted in blue. The rows labeled \u201c\u03b1 = 2.0\u201d\nrepresents the base sparsity case where SL is not applied.\nLower \u03b1\u2019s are applicable to longer sequences up to the\nlongest sequence length we tested, 8,192.\n3.3. Minimizing activation memory usage\nIn recommendation systems, the use of large batch sizes is\ncrucial for both training throughput (Mudigere et al., 2022)\nand model quality (Yang et al., 2020; Chen et al., 2020;\nZhai et al., 2023a). Consequently, activation memory usage\nbecomes a major scaling bottleneck, in contrast to large\nlanguage models that are commonly trained with small batch\nsizes and dominated by parameter memory usage.\nCompared to Transformers, HSTU employs a simplified\nand fully fused design that significantly reduces activation\nmemory usage. Firstly, HSTU reduces the number of linear\nlayers outside of attention from six to two, aligning with\nrecent work that uses elementwise gating to reduce MLP\ncomputations (Hua et al., 2022; Gu et al., 2022). Secondly,\nHSTU aggressively fuses computations into single opera-\ntors, including \u03d51(f1(\u00b7)) in Equation (1), and layer norm,\noptional dropout, and output MLP in Equation (3). This\nsimplified design reduces the activation memory usage to\n2d+2d+4hdqk +4hdv +2hdv = 14d per layer in bfloat16.\nFor comparison, Transformers use a feedforward layer and\ndropout after attention (intermediate state of 3hdv), fol-\nlowed by a pointwise feedforward block consisting of layer\nnorm, linear, activation, linear, and dropout, with intermedi-\nate states of 2d + 4df f + 2d + 1d = 4 d + 4df f. Here,\nwe make standard assumptions that hdv \u2265 d and that\ndf f = 4d (Vaswani et al., 2017; Brown et al., 2020). Thus,\nafter accounting for input and input layer norm ( 4d) and\nqkv projections, the total activation states is 33d. HSTU\u2019s\ndesign hence enables scaling to > 2x deeper layers.\nAdditionally, large scale atomic ids used to represent vocab-\nularies also require significant memory usage. With a 10b\nvocabulary, 512d embeddings, and Adam optimizer, storing\nembeddings and optimizer states in fp32 already requires\n60TB memory. To alleviate memory pressure, we employ\nrowwise AdamW optimizers (Gupta et al., 2014; Khudia\n5\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\net al., 2021) and place optimizer states on DRAM, which\nreduces HBM usage per float from 12 bytes to 2 bytes.\n3.4. Scaling up inference via cost-amortization\nThe last challenge we address is the large number of candi-\ndates recommendation systems need to process at serving\ntime. We focus on ranking as for retrieval, encoder cost\nis fully amortizable, and efficient algorithms exist for both\nMIPS leveraging quantization, hashing, or partitioning (Je-\ngou et al., 2011; Shrivastava & Li, 2014; Li et al., 2002;\nZhai et al., 2011) and non-MIPS cases via beam search or\nhierarchical retrieval (Zhuo et al., 2020; Zhai et al., 2023a).\nFor ranking, we have up to tens of thousands of candi-\ndates (Covington et al., 2016; Wang et al., 2020). We pro-\npose an algorithm M-FALCON (Microbatched-Fast Atten-\ntion Leveraging Cacheable OperatioNs) to perform infer-\nence for m candidates with an input sequence size of n.\nWithin a forward pass, M-FALCON handles bm candidates\nin parallel by modifying attention masks and rabp,t biases\nsuch that the attention operations performed for bm candi-\ndates are exactly the same. This reduces the cost of apply-\ning cross-attention from O(bmn2d) to O((n + bm)2d) =\nO(n2d) when bm can be considered a small constant rela-\ntive to n. We optionally divide the overallm candidates into\n\u2308m/bm\u2309 microbatches of size bm to leverage encoder-level\nKV caching (Pope et al., 2022) either across forward passes\nto reduce cost, or across requests to minimize tail latency\n(More detailed discussions in Appendix H).\nOverall, M-FALCON enables model complexity to linearly\nscale up with the number of candidates in traditional DL-\nRMs\u2019s ranking stages; we succeeded in applying a 285x\nmore complex target-aware cross attention model at 1.5x-3x\nthroughput with a constant inference budget for a typical\nranking configuration discussed in Section 4.3.",
          "translated": "\uc11c\ub860\n\n\uc628\ub77c\uc778 \ucf58\ud150\uce20 \ud50c\ub7ab\ud3fc \ubc0f \uc804\uc790\uc0c1\uac70\ub798\uc5d0\uc11c \ud575\uc2ec\uc801\uc778 \uc5ed\ud560\uc744 \uc218\ud589\ud558\ub294 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc740 \ub9e4\uc77c \uc218\uc2ed\uc5b5 \uac74\uc758 \uc0ac\uc6a9\uc790 \uacbd\ud5d8\uc744 \uac1c\uc778\ud654\ud558\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \ucd5c\ucca8\ub2e8 \ucd94\ucc9c \uc811\uadfc \ubc29\uc2dd\uc740 \ub525 \ub7ec\ub2dd \ucd94\ucc9c \ubaa8\ub378(DLRM) (Mudigere et al., 2022)\uc744 \uc57d 10\ub144 \ub3d9\uc548 \uae30\ubc18\uc73c\ub85c \ud574 \uc654\uc2b5\ub2c8\ub2e4 (Covington et al., 2016; Cheng et al., 2016; Zhou et al., 2018; Tang et al., 2020; Wang et al., 2021; Xia et al., 2023). DLRM\uc740 \uc218\uce58\ud615 \ud2b9\uc9d5(\uce74\uc6b4\ud130 \ubc0f \ube44\uc728, \uc784\ubca0\ub529, \uc0dd\uc131\uc790 ID, \uc0ac\uc6a9\uc790 ID \ub4f1)\uacfc \uac19\uc740 \ub2e4\uc591\ud55c \ud2b9\uc9d5\uc744 \ud65c\uc6a9\ud558\ub294 \uac83\uc73c\ub85c \ud2b9\uc9d5\uc9c0\uc5b4\uc9d1\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 \ucf58\ud150\uce20\uc640 \uc81c\ud488\uc774 \ub9e4 \ubd84\ub9c8\ub2e4 \ucd94\uac00\ub428\uc5d0 \ub530\ub77c \ud2b9\uc9d5 \uacf5\uac04\uc740 \uc885\uc885 \uc218\uc2ed\uc5b5 \uac1c\uc5d0 \ub2ec\ud558\uba70, \uc774\ub294 Eksombatchai et al. (2018)\uc5d0\uc11c \uad00\ucc30\ud55c \ubc14\uc640 \uac19\uc2b5\ub2c8\ub2e4. 1\ub9cc \uac1c \uc774\uc0c1\uc758 \uc774\ub7ec\ud55c \ud2b9\uc9d5\uc744 \ud65c\uc6a9\ud558\uae30 \uc704\ud574 DLRM\uc740 \uc911\uac04 \ud45c\ud604\uc744 \uacb0\ud569\ud558\uace0 \ucd5c\uc885 \ucd9c\ub825\uc744 \uad6c\uc131\ud558\uae30 \uc704\ud574 \ub2e4\uc591\ud55c \uc2e0\uacbd\ub9dd\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\uc778\uac04\uc774 \uc124\uacc4\ud55c \ubc29\ub300\ud55c \ud2b9\uc9d5 \uc9d1\ud569\uacfc \ub9c9\ub300\ud55c \uc591\uc758 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ub300\ubd80\ubd84\uc758 \uc0b0\uc5c5 \uaddc\ubaa8 DLRM\uc740 \ucef4\ud4e8\ud305 \uc790\uc6d0\uc73c\ub85c \uc778\ud574 \uc81c\ub300\ub85c \ud655\uc7a5\ub418\uc9c0 \ubabb\ud569\ub2c8\ub2e4 (Zhao et al., 2023). \uc774\ub7ec\ud55c \uc81c\ud55c \uc0ac\ud56d\uc740 \uc8fc\ubaa9\ud560 \ub9cc\ud558\uba70 \ud574\uacb0\ub418\uc9c0 \uc54a\uc740 \ubb38\uc81c\uc785\ub2c8\ub2e4.\n\n\ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \uc5b8\uc5b4 \ubc0f \ube44\uc804 \ubd84\uc57c\uc5d0\uc11c\uc758 \uc131\uacf5\uc5d0 \uc601\uac10\uc744 \ubc1b\uc544, \uc6b0\ub9ac\ub294 \ud604\ub300 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \uae30\ubcf8 \uc124\uacc4 \uc120\ud0dd\uc744 \uc7ac\uace0\ud569\ub2c8\ub2e4. \ub300\uccb4 \ud615\uc2dd\uc744 \uc5b5\ub9cc \uba85 \uaddc\ubaa8\uc5d0\uc11c \uadf9\ubcf5\ud558\uae30 \uc704\ud574 \uc138 \uac00\uc9c0 \uacfc\uc81c\ub97c \ud574\uacb0\ud574\uc57c \ud569\ub2c8\ub2e4. \uccab\uc9f8, \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \ud2b9\uc9d5\uc740 \uba85\uc2dc\uc801\uc778 \uad6c\uc870\uac00 \ubd80\uc871\ud569\ub2c8\ub2e4. \uc21c\ucc28\uc801 \ud615\uc2dd\uc774 \uc18c\uaddc\ubaa8 \uc124\uc815\uc5d0\uc11c \ud0d0\uad6c\ub418\uc5c8\uc9c0\ub9cc (\ubd80\ub85d B\uc5d0\uc11c \uc790\uc138\ud55c \ub17c\uc758), \uace0\ucc28\uc6d0 \ud2b9\uc9d5, \uad50\ucc28 \ud2b9\uc9d5, \uce74\uc6b4\ud130, \ube44\uc728 \ub4f1\uc740 \uc0b0\uc5c5 \uaddc\ubaa8 DLRM\uc5d0\uc11c \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4 (Mudigere et al., 2022). \ub458\uc9f8, \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc740 100K \uaddc\ubaa8\uc758 \uc815\uc801 \uc5b4\ud718\uc640 \ub2ec\ub9ac \uc9c0\uc18d\uc801\uc73c\ub85c \ubcc0\ud654\ud558\ub294 \uc5b5\ub9cc \uaddc\ubaa8 \uc5b4\ud718\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4 (Brown et al., 2020). \uc774\ub7ec\ud55c \uc5b5\ub9cc \uaddc\ubaa8 \ub3d9\uc801 \uc5b4\ud718\ub294 1~2\uac1c\uc6d4 \ub3d9\uc548 \uc5b8\uc5b4\uc5d0\uc11c \ucc98\ub9ac\ud558\ub294 \uac83\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ub9ce\uc740 \ud1a0\ud070\uc744 \ucc98\ub9ac\ud574\uc57c \ud558\ubbc0\ub85c \ud6c8\ub828 \uacfc\uc81c\ub97c \uc57c\uae30\ud558\uace0 \ub300\uc0c1\uc5d0 \ub9de\ub294 \ubc29\uc2dd\uc73c\ub85c 1\ub9cc \uac1c \uc774\uc0c1\uc758 \ud6c4\ubcf4\ub97c \uace0\ub824\ud574\uc57c \ud558\ubbc0\ub85c \ucd94\ub860 \ube44\uc6a9\uc774 \ub192\uc2b5\ub2c8\ub2e4 (Zhou et al., 2018; Wang et al., 2020). \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \ub300\uaddc\ubaa8 \uc21c\ucc28\uc801 \ubaa8\ub378\uc744 \uac00\ub2a5\ud558\uac8c \ud558\ub294 \ucef4\ud4e8\ud305 \ube44\uc6a9\uc774 \uc8fc\uc694 \ubcd1\ubaa9 \ud604\uc0c1\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. GPT-3\ub294 \uc218\ucc9c \uac1c\uc758 GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd1d 300B \ud1a0\ud070\uc744 1~2\uac1c\uc6d4 \ub3d9\uc548 \ud6c8\ub828\ud588\uc2b5\ub2c8\ub2e4 (Brown et al., 2020). \uc774\ub7ec\ud55c \uaddc\ubaa8\ub294 \uc555\ub3c4\uc801\uc73c\ub85c \ubcf4\uc774\uc9c0\ub9cc, \uc778\ud130\ub137 \ud50c\ub7ab\ud3fc\uc774 \ub9e4\uc77c \uc218\uc2ed\uc5b5 \uba85\uc758 \ud65c\uc131 \uc0ac\uc6a9\uc790\uc5d0\uac8c \ucf58\ud150\uce20, \uc774\ubbf8\uc9c0 \ubc0f \ube44\ub514\uc624\ub97c \uc81c\uacf5\ud55c\ub2e4\ub294 \uc810\uc744 \uace0\ub824\ud574 \ubcfc \ub54c, \uc0ac\uc6a9\uc790 \ud589\ub3d9\uc758 \uaddc\ubaa8\ub97c \ube44\uad50\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9\uc790 \uc2dc\ud000\uc2a4\ub294 \ucd5c\ub300 10^5\uae4c\uc9c0 \uc774\uc5b4\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4 (Chang et al., 2023). \uacb0\uacfc\uc801\uc73c\ub85c \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc740 \uc5b8\uc5b4\uac00 1~2\uac1c\uc6d4 \ub3d9\uc548 \ucc98\ub9ac\ud558\ub294 \uac83\ubcf4\ub2e4 \ud558\ub8e8\uc5d0 \uba87 \ubc30 \ub354 \ub9ce\uc740 \ud1a0\ud070\uc744 \ucc98\ub9ac\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\uc774 \uc5f0\uad6c\uc5d0\uc11c \uc6b0\ub9ac\ub294 \uc0dd\uc131 \ubaa8\ub378\ub9c1\uc758 \uc0c8\ub85c\uc6b4 \ubaa8\ub2e4\ub9ac\uc778 \uc0ac\uc6a9\uc790 \ud589\ub3d9\uc744 \ub2e4\ub8f9\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \uc8fc\uc694 \ud1b5\ucc30\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. a) \uc801\uc808\ud55c \uc0c8\ub85c\uc6b4 \ud2b9\uc9d5 \uacf5\uac04\uc744 \ud1b5\ud574 \uc0b0\uc5c5 \uaddc\ubaa8 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \ud575\uc2ec \ub7ad\ud0b9 \ubc0f \uac80\uc0c9 \uc791\uc5c5\uc740 \uc0dd\uc131 \ubaa8\ub378\ub9c1 \ubb38\uc81c\ub85c \uad6c\uc131\ub420 \uc218 \uc788\uc73c\uba70, b) \uc774\ub7ec\ud55c \ud328\ub7ec\ub2e4\uc784\uc740 \ud2b9\uc9d5, \ud6c8\ub828 \ubc0f \ucd94\ub860\uc758 \uc911\ubcf5\uc744 \uccb4\uacc4\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uac1c\uc120\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 \uc6b0\ub9ac\uc758 \ud615\uc2dd\uc744 \ud1b5\ud574 \ubaa8\ub378\uc774 \uc774\uc804 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\ubcf4\ub2e4 3\ubc30 \ub354 \ubcf5\uc7a1\ud55c \ucef4\ud4e8\ud305 \ubcf5\uc7a1\uc131\uc744 \uac00\uc9c0\uba74\uc11c\ub3c4 12.4%\uc758 \ud0d1\ub77c\uc778 \uc9c0\ud45c \uac1c\uc120\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4 (Figure 1 \ucc38\uc870). \uc6b0\ub9ac\uc758 \uae30\uc5ec\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \uccab\uc9f8, \uc6b0\ub9ac\ub294 Section 2\uc5d0\uc11c Generative Recommenders (GRs)\ub77c\ub294 \uc0c8\ub85c\uc6b4 \ud328\ub7ec\ub2e4\uc784\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. GRs\ub294 DLRM\uc744 \ub300\uccb4\ud558\ub294 \uc21c\ucc28\ud654 \ubc0f \ub2e4\uc591\ud55c \ud2b9\uc9d5 \uacf5\uac04\uc744 \ud1b5\ud569\ud569\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 \uc811\uadfc \ubc29\uc2dd\uc740 \ubb34\ud55c\ub300\ub85c \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud558\ub294 DLRM \ud2b9\uc9d5 \uacf5\uac04\uc744 \uc2dc\ud000\uc2a4 \uae38\uc774\ub85c \uadfc\uc0ac\ud558\uc5ec \uc8fc\uc694 \ucd94\ucc9c \ubb38\uc81c\ub97c \uc21c\ucc28\uc801 \uc0dd\uc131 \ud2b8\ub79c\uc2a4\ub450\uc11c \uc791\uc5c5\uc73c\ub85c \uc7ac\uad6c\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 GRs\uc5d0\uc11c \uc21c\ucc28\uc801 \uc0dd\uc131 \ubc29\uc2dd\uc73c\ub85c \ud6c8\ub828\uc744 \uc218\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\uc5ec \ub3d9\uc77c\ud55c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc73c\ub85c \ud6e8\uc52c \ub354 \ub9ce\uc740 \ub370\uc774\ud130\ub97c \ud6c8\ub828\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n\n\ub458\uc9f8, \uc6b0\ub9ac\ub294 \ud6c8\ub828 \ubc0f \ucd94\ub860 \uc804\ubc18\uc5d0 \uac78\uccd0 \ucef4\ud4e8\ud305 \ube44\uc6a9 \ubb38\uc81c\ub97c \ud574\uacb0\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uacc4\uce35\uc801\n\n[... \uae38\uc774 \uc81c\ud55c\uc73c\ub85c \uc0dd\ub7b5 ...]"
        },
        {
          "name": "Experiments",
          "original": "4.1. Validating Inductive Hypotheses of HSTU Encoder\n4.1.1. T RADITIONAL SEQUENTIAL SETTINGS\nWe first evaluate the performance of HSTU on two popular\nrecommender datasets, MovieLens and Amazon Reviews.\nWe follow sequential recommendation settings in literature,\nincluding full shuffle and multi-epoch training. For baseline,\nwe use SASRec, a state-of-the-art Transformer implementa-\ntion (Kang & McAuley, 2018)1. We report Hit Rate@K and\nNDCG@K over the entire corpus, consistent with recent\nwork (Dallmann et al., 2021; Zhai et al., 2023a).\nResults are presented in Table 4. \u201cSASRec (2023)\u201d denotes\nthe best SASRec recipe reported in (Zhai et al., 2023a). The\n1Results for other baselines are reported in Appendix D.\nrows labeled \u201cHSTU\u201d use identical configurations as SAS-\nRec (same number of layers, heads, etc.). \u201cHSTU-large\u201d\nrepresents larger HSTU encoders (4x number of layers and\n2x number of heads). Results show that a) HSTU, with its\ndesign optimized for recommendations, significantly outper-\nforms the baseline when using the same configuration, and\nb) HSTU further improves performance when scaled up.\nIt is important to note that the evaluation methodology used\nhere differs significantly from industrial-scale settings, as\nfull-shuffle and multi-epoch training are generally not practi-\ncal in streaming settings used in industry (Liu et al., 2022).\n4.1.2. I NDUSTRIAL -SCALE STREAMING SETTINGS\nWe next compare the performance of HSTU, ablated HS-\nTUs, and transformers using industrial-scale datasets in a\nstreaming setting. Throughout the rest of this section, we re-\nport Normalized Entropy (NE) (He et al., 2014) for ranking.\nWe train the models over 100B examples (DLRM equiva-\nlent), with 64-256 H100s used per job. Given ranking is\ndone in a multi-task setting, we report the main engage-\nment event (\u201cE-Task\u201d) and the main consumption event\n(\u201cC-Task\u201d). In our context, we consider a 0.001 reduction\nin NE significant as it generally leads to .5% topline met-\nric improvements for billions of users. For retrieval, given\nthe setup is similar to language modeling, we report log\nperplexity. We fix encoder parameters in a smaller-scale\nsetting (l = 3, n = 2048, d = 512 for ranking and l = 6,\nn = 512 , d = 256 for retrieval), and grid-search other\nhyperparameters due to resource limits.\nWe show results in Table 5. First, HSTU significantly out-\nperforms Transformers, especially in ranking, likely due\nto pointwise attention and improved relative attention bi-\nases. Second, the gaps between the ablated HSTUs and\nHSTU confirm the effectiveness of our designs. Optimal\nlearning rates are about 10x lower for Softmax-based HSTU\nand Transformer vs the rest due to training stability. Even\nwith lower learning rates and pre-norm residual connec-\ntions (Xiong et al., 2020), we encountered frequent loss\nexplosions with standard Transformers in ranking. Finally,\nHSTU outperforms a popular Transformer variant used in\nLLMs, Transformer++ (Touvron et al., 2023a), which uses\nRoPE (Su et al., 2023), SwiGLU, etc. Overall, in this small-\nscale setting, HSTU shows better quality at 1.5x-2x faster\nwall-clock time and 50% less HBM usage.\n4.2. Encoder Efficiency\nStochastic Length. Figure 4 and Figure 5 (a) show the im-\npact of stochastic length (SL) on model metrics. At \u03b1 = 1.6,\na sequence of length4096 is turned into a sequence of length\n776 the majority of the time, or removing more than 80%\nof the tokens. Even after sparsity ratio increases to 64%-\n84%, the NEs we obtained for main tasks did not degrade by\n6\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nTable 4. Evaluations of methods on public datasets in multi-pass, full-shuffle settings.\nMethod HR@10 HR@50 HR@200 NDCG@10 NDCG@200\nML-1M\nSASRec (2023) .2853 .5474 .7528 .1603 .2498\nHSTU .3097 (+8.6%) .5754 (+5.1%) .7716 (+2.5%) .1720 (+7.3%) .2606 (+4.3%)\nHSTU-large .3294 (+15.5%) .5935 (+8.4%) .7839 (+4.1%) .1893 (+18.1%) .2771 (+10.9%)\nML-20M\nSASRec (2023) .2906 .5499 .7655 .1621 .2521\nHSTU .3252 (+11.9%) .5885 (+7.0%) .7943 (+3.8%) .1878 (+15.9%) .2774 (+10.0%)\nHSTU-large .3567 (+22.8%) .6149 (+11.8%) .8076 (+5.5%) .2106 (+30.0%) .2971 (+17.9%)\nBooks\nSASRec (2023) .0292 .0729 .1400 .0156 .0350\nHSTU .0404 (+38.4%) .0943 (+29.5%) .1710 (+22.1%) .0219 (+40.6%) .0450 (+28.6%)\nHSTU-large .0469 (+60.6%) .1066 (+46.2%) .1876 (+33.9%) .0257 (+65.8%) .0508 (+45.1%)\nTable 5. Evaluation of HSTU, ablated HSTU, and Transformers\non industrial-scale datasets in one-pass streaming settings.\nArchitecture Retrieval Ranking (NE)\nlog pplx. E-Task C-Task\nTransformers 4.069 NaN NaN\nHSTU (-rabp,t, Softmax) 4.024 .5067 .7931\nHSTU (-rabp,t) 4.021 .4980 .7860\nTransformer++ 4.015 .4945 .7822\nHSTU (original rab) 4.029 .4941 .7817\nHSTU 3.978 .4937 .7805\nAlpha\nNE Difference vs Baseline\n-0.0010\n0.0000\n0.0010\n0.0020\n0.0030\n-25.0%\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\n1.6 1.7 1.8 1.9 2\nAverage NE Across Tasks Percent Sparsity\nAlpha\nNE Difference vs Baseline\n-0.0005\n0.0000\n0.0005\n0.0010\n0.0015\n-30.0%\n-10.0%\n10.0%\n30.0%\n50.0%\n70.0%\n90.0%\n110.0%\n1.6 1.7 1.8 1.9 2\nAverage NE Across Tasks Percent Sparsity\nFigure 4. Impact of Stochastic Length (SL) on metrics. Left: n =\n4096. Right: n = 8192. Full results can be found in Appendix F.\nmore than 0.002 (0.2%). This evidence supports that SL, for\nsuitable \u03b1s, does not negatively impact model quality and\nallows for high sparsity to reduce training cost. We further\nverify in Appendix F.3 that SL significantly outperforms\nexisting length extrapolation techniques.\nEncoder Efficiency. Figure 5 compares the efficiency\nof HSTU and Transformer encoders in training and infer-\nence settings. For Transformers, we use the state-of-the-art\nFlashAttention-2 (Dao, 2023) implementation. We consider\nsequence lengths ranging from 1,024 to 8,192 and apply\nStochastic Length (SL) during training. In the evaluation,\nwe use the same configuration for HSTU and Transformer\n(d = 512, h = 8, dqk = 64) and ablate relative attention\nbias considering HSTU outperforms Transformers without\nrabp,t, as demonstrated in Section 4.1.2. We compare the\nencoder-level performance in bfloat16 on NVIDIA H100\nGPUs. Overall, HSTU is up to 15.2x and 5.6x more efficient\nthan Transformers in training and inference, respectively.\nAdditionally, the decrease in activation memory usage as\ndiscussed in Section 3.3 enables us to construct over 2x\ndeeper networks with HSTUs compared to Transformers.\nTable 6. Offline/Online Comparison of Retrieval Models.\nMethods Offline HR@K Online metrics\nK=100 K=500 E-Task C-Task\nDLRM 29.0% 55.5% +0% +0%\nDLRM (abl. features) 28.3% 54.3% \u2013\nGR (content-based) 11.6% 18.8% \u2013\nGR (interactions only) 35.6% 61.7% \u2013\nGR (new source) 36.9% 62.4% +6.2% +5.0%\nGR (replace source) +5.1% +1.9%\nTable 7. Offline/Online Comparison of Ranking Models.\nMethods Offline NEs Online metrics\nE-Task C-Task E-Task C-Task\nDLRM .4982 .7842 +0% +0%\nDLRM (DIN+DCN) .5053 .7899 \u2013 \u2013\nDLRM (abl. features) .5053 .7925 \u2013 \u2013\nGR (interactions only) .4851 .7903 \u2013 \u2013\nGR .4845 .7645 +12.4% +4.4%\n4.3. Generative Recommenders vs DLRMs in\nIndustrial-scale Streaming Settings\nLastly, we compare the end-to-end performance of GRs\nagainst state-of-the-art DLRM baselines in industrial-scale\nstreaming settings. Our GR implementation reflects a typ-\nical configuration used in production, whereas the DLRM\nsettings reflect iterations of hundreds of people over multi-\nple years. Given multiple generators are used in the retrieval\nstage of a recommendation system, we report both the online\nresult for adding GR (\u201cadd source\u201d) and replacing existing\nmain DLRM source (\u201creplace source\u201d). Table 6 and Table 7\nshow that GR not only significantly outperforms DLRMs\noffline, but also brings 12.4% wins in A/B tests.\nAs discussed in Section 2, GRs build upon raw categorical\nengagement features, while DLRMs are typically trained\nwith a significantly larger number of features, the majority\nof which are handcrafted from raw signals. If we give the\nsame set of features used in GRs to DLRMs (\u201cDLRM (abl.\nfeatures)\u201d), the performance of DLRMs is significantly de-\ngraded, which suggests GRs can meaningfully capture those\nfeatures via their architecture and unified feature space.\nWe further validate the GR formulation in Section 2.2 by\ncomparing it with a traditional sequential recommender\nsetup that only considers items user interacted with (Kang\n7\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\n(a) Training NE.\n(b) Training Speedup.\nSequence Length\nLatency (ms)\n0.00\n25.00\n50.00\n75.00\n100.00\n125.00\n1024 2048 4096 8192\nHSTU Transformers\n(c) Inference Speedup.\nFigure 5. Encoder-level efficiency: HSTU vs FlashAttention2-\nbased Transformers for Training (a, b) and Inference (c).\n& McAuley, 2018) (\u201cGR (interactions only)\u201d). The results\nare significantly worse, with its ranking variant underper-\nforming GRs by 2.6% in NE in the main consumption task.\nConsidering the popularity of content-based methods (in-\ncluding LMs), we also include a GR baseline with only con-\ntent features (\u201cGR (content-based)\u201d). The substantial gap in\nperformance of content-based baselines and DLRMs/GRs\nunderscores the significance of high cardinality user actions.\nWe finally compare the efficiency of GRs with our produc-\ntion DLRMs in Figure 6. Despite the GR model being 285x\nmore computationally complex, we achieved 1.50x/2.99x\nhigher QPS when scoring 1024/16384 candidates, due to\nHSTU and the novel M-FALCON algorithm in Section 3.4.\n4.3.1. SCALING LAW FOR RECOMMENDATION SYSTEMS\nIt is commonly known that in large-scale industrial settings,\nDLRMs saturate in quality at certain compute and params\nCandidates scored in M-FALCON (m)\nQPS\n0 \n250,000 \n500,000 \n750,000 \n1,000,000 \n1,250,000 \n32 64 128 256 512 1024\nGR (101x FLOPs) GR (285x FLOPs) DLRM (1x FLOPs)\nFigure 6. Comparison of inference throughput, in the most chal-\nlenging ranking setup. Full results can be found in Appendix H.1.\nregimes (Zhao et al., 2023). We compare the scalability of\nGRs and DLRMs to better understand this phenomenon.\nSince feature interaction layers are crucial for DLRM\u2019s per-\nformance (Mudigere et al., 2022), we experimented with\nTransformers (Vaswani et al., 2017), DHEN (Zhang et al.,\n2022), and a variant of DCN (Wang et al., 2021) augmented\nwith residual connections (He et al., 2015) used in our pro-\nduction settings to scale up the DLRM baseline in the rank-\ning setting. For the retrieval baseline, given our baseline\nused a residual setup, we scaled up hidden layer sizes, em-\nbedding dimensions, and number of layers. For HSTU-\nbased Generative Recommenders (GRs), we scaled up the\nmodel by adjusting the hyperparameters for HSTU, includ-\ning the number of residual layers, sequence length, em-\nbedding dimensions, number of attention heads, etc. We\nadditionally adjust the number of negatives for retrieval.\nResults are shown in Figure 7. In the low compute regime,\nDLRMs might outperform GRs due to handcrafted features,\ncorroborating the importance of feature engineering in tra-\nditional DLRMs. However, GRs demonstrate substantially\nbetter scalability with respect to FLOPs, whereas DLRM\nperformance plateaus, consistent with findings in prior work.\nWe also observe better scalability w.r.t. both embedding pa-\nrameters and non-embedding parameters, with GRs leading\nto 1.5 trillion parameter models, whereas DLRMs perfor-\nmance saturate at about 200 billion parameters.\nFinally, all of our main metrics, including Hit Rate@100 and\nHit Rate@500 for retrieval, and NE for ranking, empirically\nscale as a power law of compute used given appropriate\nhyperparameters. We observe this phenomenon across three\norders of magnitude, up till the largest models we were able\nto test (8,192 sequence length, 1,024 embedding dimension,\n24 layers of HSTU), at which point the total amount of com-\npute we used (normalized over 365 days as we use a stan-\ndard streaming training setting) is close to the total training\ncompute used by GPT-3 (Brown et al., 2020) and LLaMa-\n2 (Touvron et al., 2023b), as shown in Figure 1. Within\na reasonable range, the exact model hyperparameters play\nless important roles compared to the total amount of training\ncompute applied. In contrast to language modeling (Kaplan\net al., 2020), sequence length play a significantly more im-\nportant role in GRs, and it\u2019s important to scale up sequence\n8\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nTraining PetaFLOPs per day\nEval Hit Rate@100\n0.25\n0.30\n0.35\n0.40\n1000 5000 10000 50000 100000\nTraditional DLRMs Generative Recommenders (GRs)\nL = .15 + .0195 ln C\nTraining PetaFLOPs per day\nEval Hit Rate@500\n0.50\n0.55\n0.60\n0.65\n1000 5000 10000 50000 100000\nTraditional DLRMs Generative Recommenders (GRs)\nL = .395 + .0212 ln C\nTraining PetaFLOPs per day\nEval NE (lower is better)\n0.47\n0.48\n0.49\n0.50\n0.51\n1000 10000 100000 1000000\nTraditional DLRMs Generative Recommenders (GRs)\nL = .549 + -5.3E-03 ln C\nFigure 7. Scalability: DLRMs vs GRs in large-scale industrial\nsettings across retrieval (top, middle) and ranking (bottom). +0.005\nin HR and -0.001 in NE represent significant improvements.\nlength and other parameters in tandem. This is perhaps the\nmost important advantage of our proposed method, as we\u2019ve\nshown for the first time that scaling law from LLMs may\nalso apply to large-scale recommendation systems.",
          "translated": "**4.1. HSTU \uc778\uacfc\uc801 \uac00\uc124 \uac80\uc99d**\n\n4.1.1. \uc804\ud1b5\uc801\uc778 \uc21c\ucc28\uc801 \uc124\uc815\n\uc6b0\uc120 HSTU\uc758 \uc131\ub2a5\uc744 MovieLens \ubc0f Amazon Reviews \ub450 \uac00\uc9c0 \uc778\uae30 \uc788\ub294 \ucd94\ucc9c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \ud3c9\uac00\ud569\ub2c8\ub2e4. \ubb38\ud5cc\uc5d0 \ub530\ub978 \uc804\ud1b5\uc801\uc778 \uc21c\ucc28\uc801 \uc124\uc815(\uc804\uccb4 \uc154\ud50c \ubc0f \ub2e4\uc911 \uc5d0\ud3ec\ud06c \ud6c8\ub828 \ud3ec\ud568)\uc744 \ub530\ub985\ub2c8\ub2e4. \uae30\uc900\uc120\uc73c\ub85c, \ucd5c\ucca8\ub2e8 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uad6c\ud604\uc778 SASRec(Kang & McAuley, 2018)1\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Hit Rate@K \ubc0f NDCG@K\ub97c \uc804\uccb4 \ucf54\ud37c\uc2a4\uc5d0 \ub300\ud574 \ubcf4\uace0\ud558\uba70, \ucd5c\uadfc \uc791\uc5c5(Dallmann et al., 2021; Zhai et al., 2023a)\uc640 \uc77c\uad00\uc131\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. \uacb0\uacfc\ub294 \ud45c 4\uc5d0 \uc81c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \u201cSASRec (2023)\u201d\ub294 (Zhai et al., 2023a)\uc5d0\uc11c \ubcf4\uace0\ub41c \ucd5c\uc801\uc758 SASRec \ub808\uc2dc\ud53c\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n1 \ub2e4\ub978 \uae30\uc900\uc120\uc758 \uacb0\uacfc\ub294 \ubd80\ub85d D\uc5d0 \ubcf4\uace0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \u201cHSTU\u201d \ub808\uc774\ube14\uc774 \ubd99\uc740 \ud589\uc740 SAS-Rec(\ub3d9\uc77c\ud55c \ub808\uc774\uc5b4 \uc218, \ud5e4\ub4dc \uc218 \ub4f1)\uc640 \ub3d9\uc77c\ud55c \uad6c\uc131\uc73c\ub85c \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \u201cHSTU-large\u201d\ub294 4\ubc30 \ub808\uc774\uc5b4 \uc218 \ubc0f 2\ubc30 \ud5e4\ub4dc \uc218\uc758 \ub354 \ud070 HSTU \uc778\ucf54\ub354\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uacb0\uacfc\ub294 a) HSTU\ub294 \ucd94\ucc9c\uc5d0 \ucd5c\uc801\ud654\ub41c \uc124\uacc4\ub97c \ud1b5\ud574 \uae30\uc900\uc120\ubcf4\ub2e4 \ud6e8\uc52c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, b) HSTU\ub294 \ud655\uc7a5\ub420 \ub54c \uc131\ub2a5\uc744 \ub354\uc6b1 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.\n\n\uc911\uc694\ud55c \uc810\uc740 \uc774 \ud3c9\uac00 \ubc29\ubc95\ub860\uc774 \uc0b0\uc5c5 \uaddc\ubaa8 \uc124\uc815\uacfc \ud06c\uac8c \ub2e4\ub974\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc804\uccb4 \uc154\ud50c \ubc0f \ub2e4\uc911 \uc5d0\ud3ec\ud06c \ud6c8\ub828\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uc2a4\ud2b8\ub9ac\ubc0d \ud658\uacbd\uc5d0\uc11c \uc2e4\uc6a9\uc801\uc774\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4(Liu et al., 2022).\n\n4.1.2. \uc0b0\uc5c5 \uaddc\ubaa8 \uc2a4\ud2b8\ub9ac\ubc0d \uc124\uc815\n\ub2e4\uc74c\uc73c\ub85c HSTU, ablated HSTU, \uadf8\ub9ac\uace0 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uac00 \uc0b0\uc5c5 \uaddc\ubaa8 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uc2a4\ud2b8\ub9ac\ubc0d \uc124\uc815\uc5d0\uc11c \uc131\ub2a5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uc774 \uc139\uc158\uc758 \ub098\uba38\uc9c0 \ubd80\ubd84\uc5d0\uc11c \uc21c\uc704 \uacb0\uc815\uc5d0 \ub300\ud574 Normalized Entropy (NE) (He et al., 2014)\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 100B\uac1c\uc758 \uc608\uc81c(DLRM\uc5d0 \ud574\ub2f9\ud558\ub294)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ub418\uc5c8\uc73c\uba70, \uac01 \uc791\uc5c5\uc5d0 \ub300\ud574 64-256\uac1c\uc758 H100\uc774 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc21c\uc704 \uacb0\uc815\uc774 \ub2e4\uc911 \uc791\uc5c5 \uc124\uc815\uc5d0\uc11c \uc218\ud589\ub418\uae30 \ub54c\ubb38\uc5d0, \uc6b0\ub9ac\ub294 \uc8fc\uc694 \ucc38\uc5ec \uc774\ubca4\ud2b8 (\u201cE-Task\u201d) \ubc0f \uc8fc\uc694 \uc18c\ube44 \uc774\ubca4\ud2b8 (\u201cC-Task\u201d)\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \ub9e5\ub77d\uc5d0\uc11c 0.001 \uac10\uc18c\ub41c NE\ub97c \uc911\uc694\ud558\uac8c \uac04\uc8fc\ud558\uba70, \uc774\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc218\uc2ed\uc5b5 \uba85\uc758 \uc0ac\uc6a9\uc790\uc5d0\uac8c 0.5%\uc758 \ud0d1\ub77c\uc778 \uc9c0\ud45c \uac1c\uc120\uc744 \uac00\uc838\uc635\ub2c8\ub2e4. \uac80\uc0c9\uc758 \uacbd\uc6b0, \uc124\uc815\uc774 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1\uacfc \uc720\uc0ac\ud558\uae30 \ub54c\ubb38\uc5d0 log perplexity\ub97c \ubcf4\uace0\ud569\ub2c8\ub2e4. \uc778\ucf54\ub354 \ub9e4\uac1c\ubcc0\uc218\ub97c \ub354 \uc791\uc740 \uaddc\ubaa8 \uc124\uc815(l = 3, n = 2048, d = 512 \uc21c\uc704 \uacb0\uc815 \ubc0f l = 6, n = 512, d = 256 \uac80\uc0c9\uc744 \uc704\ud574)\uc5d0\uc11c \uace0\uc815\ud558\uace0, \ub9ac\uc18c\uc2a4 \uc81c\ud55c\uc73c\ub85c \uc778\ud574 \uadf8\ub9ac\ub4dc \uac80\uc0c9\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4.\n\n\ud45c 5\uc5d0 \uacb0\uacfc\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. \uccab\uc9f8, HSTU\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ubcf4\ub2e4 \ud6e8\uc52c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \ud2b9\ud788 \uc21c\uc704 \uacb0\uc815\uc5d0\uc11c \uadf8\ub807\uc2b5\ub2c8\ub2e4. \uc774\ub294 pointwise attention \ubc0f \uac1c\uc120\ub41c relative attention biases \ub54c\ubb38\uc77c \uac00\ub2a5\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4. \ub458\uc9f8, ablated HSTU\uc640 HSTU \uac04\uc758 \uac04\uaca9\uc740 \uc6b0\ub9ac\uc758 \ub514\uc790\uc778\uc758 \ud6a8\uacfc\ub97c \ud655\uc778\ud569\ub2c8\ub2e4. \ucd5c\uc801\uc758 \ud559\uc2b5\ub960\uc740 Softmax \uae30\ubc18 HSTU \ubc0f \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc5d0 \ub300\ud574 10\ubc30 \ub0ae\uc73c\uba70, \uc774\ub294 \ud6c8\ub828 \uc548\uc815\uc131 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc2ec\uc9c0\uc5b4 \ub0ae\uc740 \ud559\uc2b5\ub960\uacfc pre-norm residual connections (Xiong et al., 2020)\uc744 \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4 \ud45c\uc900 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub294 \uc21c\uc704 \uacb0\uc815\uc5d0\uc11c \ube48\ubc88\ud55c \uc190\uc2e4 \ud3ed\ubc1c\uc744 \uacaa\uc5c8\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, HSTU\ub294 LLM\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc778\uae30 \uc788\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubcc0\ud615\uc778 Transformer++ (Touvron et al., 2023a)\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. \uc774 \ubcc0\ud615\uc740 RoPE (Su et al., 2023), SwiGLU \ub4f1\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc804\ubc18\uc801\uc73c\ub85c \uc774 \uc791\uc740 \uaddc\ubaa8 \uc124\uc815\uc5d0\uc11c HSTU\ub294 1.5x-2x \ub354 \ube60\ub978 wall-clock \uc2dc\uac04\uacfc 50% \uc801\uc740 HBM \uc0ac\uc6a9\ub7c9\uc73c\ub85c \ub354 \ub098\uc740 \ud488\uc9c8\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n4.2. \uc778\ucf54\ub354 \ud6a8\uc728\uc131\n\uc2a4\ud1a0\uce74\uc2a4\ud2f0\ud06c \uae38\uc774. \uadf8\ub9bc 4 \ubc0f \uadf8\ub9bc 5 (a)\ub294 \ubaa8\ub378 \uc9c0\ud45c\uc5d0 \uc2a4\ud1a0\uce74\uc2a4\ud2f0\ud06c \uae38\uc774 (SL)\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \u03b1 = 1.6\uc77c \ub54c, \uae38\uc774\uac00 4096 \ud1a0\ud070\uc758 \uc2dc\ud000\uc2a4\ub294 \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0 776 \ud1a0\ud070\uc758 \uc2dc\ud000\uc2a4\ub85c \ubcc0\ud658\ub418\uba70, 80% \uc774\uc0c1 \ud1a0\ud070\uc744 \uc81c\uac70\ud569\ub2c8\ub2e4. \uc2a4\ud30c\ub77c\uc2dc\ud2f0 \ube44\uc728\uc774 64%-84%\uae4c\uc9c0 \uc99d\uac00\ud558\ub354\ub77c\ub3c4 \uc8fc\uc694 \uc791\uc5c5\uc5d0 \ub300\ud55c NE\uac00 6\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nTable 4. Evaluations of methods on public datasets in multi-pass, full-shuffle settings.\nMethod HR@10 HR@50 HR@200 NDCG@10 NDCG@200\nML-1M\nSASRec (2023) .2853 .5474 .7528 .1603 .2498\nHSTU .3097 (+8.6%) .5754 (+5.1%) .7716 (+2.5%) .1720 (+7.3%) .2606 (+4.3%)\nHSTU-large .3294 (+15.5%) .5935 (+8.4%) .7839 (+4.1%) .1893 (+18.1%) .2771 (+10.9%)\nML-20M\nSASRec (2023) .2906 .5499 .7655 .1621 .2521\nHSTU .3252 (+11.9%) .5885 (+7.0%) .7943 (+3.8%) .1878 (+15.9%) .2774 (+10.0%)\nHSTU-large .3567 (+22.8%) .6149 (+11.8%) .8076 (+5.5%) .2106 (+30.0%) .2971 (+17.9%)\nBooks\nSASRec (2023) .0292 .0729 .1400 .0156 .0350\nHSTU .0404 (+38.4%) .0943 (+29.5%) .1710 (+22.1%) .0219 (+40.6%) .0450 (+28.6%)\nHSTU-large .0469 (+60.6%) .1066 (+46.2%) .1876 (+33.9%) .0257 (+65.8%) .0508 (+45.1%)\nTable 5. Evaluation of HSTU, ablated HSTU, and Transformers\non industrial-scale datasets in one-pass streaming settings.\nArchitecture Retrieval Ranking (NE)\nlog pplx. E-Task C-Task\nTransformers 4.069 NaN NaN\nHSTU (-rabp,t, Softmax) 4.024 .5067 .7931\nHSTU (-rabp,t) 4.021 .4980 .7860\nTransformer++ 4.015 .4945 .7822\nHSTU (original rab) 4.029 .4941 .7817\nHSTU 3.978 .4937 .7805\nAlpha\nNE Difference vs Baseline\n-0.0010\n0.0000\n0.0010\n0.0020\n0.0030\n-25.0%\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\n1.6 1.7 1.8 1.9 2\nAverage NE Across Tasks Percent Sparsity\nAlpha\nNE Difference vs Baseline\n-0.0005\n0.0000\n0.0000\n0.0010\n0.0020\n-25.0%\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\n1.6 1.7 1.8 1.9 2\nAverage NE Across Tasks Percent Sparsity\nAlpha\nNE Difference vs Baseline\n-0.0005\n0.0000\n0.0000\n0.0010\n0.0020\n-25.0%\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\n1.6 1.7 1.8 1.9 2\nAverage NE Across Tasks Percent Sparsity"
        },
        {
          "name": "Related Work",
          "original": "Prior work on sequential recommenders reduces user interac-\ntions to a single homogeneous sequence over items (Hidasi\net al., 2016; Kang & McAuley, 2018). Industrial-scale ap-\nplications of sequential approaches are primarily pairwise\nattention (Zhou et al., 2018) or sequential encoders as part\nof DLRMs (Chen et al., 2019; Xia et al., 2023). Multi-stage\nattention has been explored in lieu of self-attention to im-\nprove efficiency (Chang et al., 2023). Generative approaches\nthat represent ids as a token series have been explored in\nretrieval (Zhuo et al., 2020). We give a more extensive\ndiscussion of prior work in Appendix B.1.\nEfficient attention has been a major research focus area due\nto self-attention\u2019s O(n2) scaling factor, with major work\nlike factorized attentions (Child et al., 2019), low-rank ap-\nproximations (Katharopoulos et al., 2020), etc. Recently,\nalternative formulations for sequential transduction settings\nhave been explored (Gu et al., 2022; Hua et al., 2022).\nHSTU\u2019s elementwise gating design, in particular, is inspired\nby FLASH (Hua et al., 2022). Recent hardware-aware for-\nmulations have been shown to significantly reduce memory\nusage (Rabe & Staats, 2021; Korthikanti et al., 2022; Zhai\net al., 2023b) and give significantly better wallclock time\nresults (Dao et al., 2022). Length extrapolation enables\nmodels trained on shorter sequences to generalize, though\nmost work focuses on finetuning or improving bias mech-\nanisms (Press et al., 2022). Our work instead introduces\nstochasticity in the length dimension, inspired by work on\nstochasticity in the depth dimension (Huang et al., 2016).\nInterests in large language models (LLMs) have motivated\nwork to treat various recommendation tasks as in-context\nlearning (Sileo et al., 2022), instruction tuning (Bao et al.,\n2023), or transfer learning (Li et al., 2023) on top of pre-\ntrained LLMs. World knowledge embedded in LLMs can\nbe transferred to downstream tasks (Cui et al., 2022) and\nimprove recommendations in zero-shot or few-shot cases.\nTextual representations of user behavior sequences have\nalso demonstrated good scaling behaviors on medium-scale\ndatasets (Shin et al., 2023). Most studies of LLMs for rec-\nommendation have been centered around low-data regimes;\nin large-scale settings, they have yet to outperform collabo-\nrative filtering on MovieLens (Hou et al., 2024).",
          "translated": "**\uad00\ub828 \uc5f0\uad6c**\n\n\uae30\uc874\uc758 \uc21c\ucc28 \ucd94\ucc9c \ubaa8\ub378\uc740 \ud56d\ubaa9\uc5d0 \ub300\ud55c \uc0ac\uc6a9\uc790 \uc0c1\ud638\uc791\uc6a9\uc744 \ub2e8\uc77c\ud558\uace0 \uade0\uc9c8\ud55c \uc2dc\ud000\uc2a4\ub85c \uc904\uc5ec\uc654\uc2b5\ub2c8\ub2e4 (Hidasi et al., 2016; Kang & McAuley, 2018). \uc21c\ucc28\uc801 \uc811\uadfc \ubc29\uc2dd\uc758 \uc0b0\uc5c5 \uaddc\ubaa8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 \uc8fc\ub85c \uc30d\ubcc4 \uc8fc\uc758 (Zhou et al., 2018) \ub610\ub294 DLRM\uc758 \uc77c\ubd80\ub85c \uc2dc\ud000\uc2a4 \uc778\ucf54\ub354\uc600\uc2b5\ub2c8\ub2e4 (Chen et al., 2019; Xia et al., 2023). \uc790\uccb4 \uc8fc\uc758\ub97c \ub300\uccb4\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uac1c\uc120\ud558\uae30 \uc704\ud574 \ub2e4\ub2e8\uacc4 \uc8fc\uc758\uac00 \ud0d0\uad6c\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (Chang et al., 2023). ID\ub97c \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub85c \ud45c\ud604\ud558\ub294 \uc0dd\uc131\uc801 \uc811\uadfc \ubc29\uc2dd\uc774 \uac80\uc0c9\uc5d0\uc11c \ud0d0\uad6c\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (Zhuo et al., 2020). \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubd80\ub85d B.1\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc790\uccb4 \uc8fc\uc758\uc758 O(n\u00b2) \uc2a4\ucf00\uc77c\ub9c1 \uc694\uc778\uc73c\ub85c \uc778\ud574 \ud6a8\uc728\uc801\uc778 \uc8fc\uc758\ub294 \uc8fc\uc694 \uc5f0\uad6c \ubd84\uc57c\ub85c \ubd80\uac01\ub418\uc5c8\uc73c\uba70, \uc778\uc790\ud654\ub41c \uc8fc\uc758 (Child et al., 2019), \uc800\ucc28\uc6d0 \uadfc\uc0ac (Katharopoulos et al., 2020) \ub4f1\uacfc \uac19\uc740 \uc8fc\uc694 \uc791\uc5c5\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ucd5c\uadfc\uc5d0\ub294 \uc21c\ucc28\uc801 \uc804\uc774 \uc124\uc815\uc5d0 \ub300\ud55c \ub300\uc548\uc801\uc778 \ud615\ud0dc\uac00 \ud0d0\uad6c\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (Gu et al., 2022; Hua et al., 2022). \ud2b9\ud788 HSTU\uc758 \uc694\uc18c\ubcc4 \uac8c\uc774\ud305 \ub514\uc790\uc778\uc740 FLASH (Hua et al., 2022)\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4. \ucd5c\uadfc \ud558\ub4dc\uc6e8\uc5b4\uc5d0 \ub300\ud55c \uc778\uc2dd \uae30\ubc18 \ud615\ud0dc\ub294 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ud06c\uac8c \uc904\uc774\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4 (Rabe & Staats, 2021; Korthikanti et al., 2022; Zhai et al., 2023b) \ubc0f \ubcbd\uc2dc\uacc4 \uc2dc\uac04 \uacb0\uacfc\uc5d0\uc11c \uc0c1\ub2f9\ud55c \uac1c\uc120\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4 (Dao et al., 2022). \uae38\uc774 \ud655\uc7a5\ubc95\uc744 \ud1b5\ud574 \ub354 \uc9e7\uc740 \uc2dc\ud000\uc2a4\ub85c \ud6c8\ub828\ub41c \ubaa8\ub378\uc774 \uc77c\ubc18\ud654\ub418\ub3c4\ub85d \ud560 \uc218 \uc788\uc9c0\ub9cc, \ub300\ubd80\ubd84\uc758 \uc791\uc5c5\uc740 \ubbf8\uc138 \uc870\uc815 \ub610\ub294 \ud3b8\ud5a5 \uba54\ucee4\ub2c8\uc998 \uac1c\uc120\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4 (Press et al., 2022). \ub300\uc2e0, \uc800\ud76c \uc5f0\uad6c\ub294 \uae4a\uc774 \ucc28\uc6d0\uc5d0\uc11c \ubb34\uc791\uc704\uc131\uc744 \ub3c4\uc785\ud558\uc5ec Huang et al., 2016\uc758 \uae4a\uc774 \ucc28\uc6d0\uc5d0\uc11c\uc758 \ubb34\uc791\uc704\uc131 \uc5f0\uad6c\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4. \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc5d0 \ub300\ud55c \uad00\uc2ec\uc740 \ub2e4\uc591\ud55c \ucd94\ucc9c \uc791\uc5c5\uc744 \uc778-\ucee8\ud14d\uc2a4\ud2b8 \ud559\uc2b5 (Sileo et al., 2022), \uc9c0\uc2dc \uc870\uc815 (Bao et al., 2023) \ub610\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c LLM \uc704\uc5d0 \uc804\uc774 \ud559\uc2b5 (Li et al., 2023)\uc73c\ub85c \ucde8\uae09\ud558\ub3c4\ub85d \ub3d9\uae30\ub97c \ubd80\uc5ec\ud588\uc2b5\ub2c8\ub2e4. LLM\uc5d0 \ub0b4\uc7ac\ub41c \uc138\uacc4 \uc9c0\uc2dd\uc740 \ud558\uc704 \uc791\uc5c5\uc73c\ub85c \uc804\uc1a1\ub418\uc5b4 \uc911\uac04 \uaddc\ubaa8 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uc81c\uc548 \uc0ac\ud56d\uc744 \uac1c\uc120\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 (Shin et al., 2023). LLM\uc5d0 \ub300\ud55c \ub300\ubd80\ubd84\uc758 \uc5f0\uad6c\ub294 \uc800 \ub370\uc774\ud130 \ud658\uacbd\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uc5c8\uc2b5\ub2c8\ub2e4. \ub300\uaddc\ubaa8 \uc124\uc815\uc5d0\uc11c\ub294 \ud611\uc5c5 \ud544\ud130\ub9c1\uc774 MovieLens\uc5d0\uc11c \uc131\ub2a5\uc744 \ub2a5\uac00\ud558\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4 (Hou et al., 2024)."
        },
        {
          "name": "Conclusions",
          "original": "We have proposed Generative Recommenders (GRs), a new\nparadigm that formulates ranking and retrieval as sequential\ntransduction tasks, allowing them to be trained in a gener-\native manner. This is made possible by the novel HSTU\nencoder design, which is 5.3x-15.2x faster than state-of-the-\nart Transformers on 8192 length sequences, and through\nthe use of new training and inference algorithms such as\nM-FALCON. With GRs, we deployed models that are 285x\nmore complex while using less inference compute. GRs and\nHSTU have led to 12.4% metric improvements in production\nand have shown superior scaling performance compared to\ntraditional DLRMs. Our results corroborate that user actions\nrepresent an underexplored modality in generative modeling\n\u2013 to echo our title, \u201cActions speak louder than words\u201d.\nThe dramatic simplification of features in our work paves the\nway for the first foundation models for recommendations,\nsearch, and ads by enabling a unified feature space to be\nused across domains. The fully sequential setup of GRs also\nenables recommendation to be formulated in an end-to-end,\ngenerative setting. Both of these enable recommendation\nsystems to better assist users holistically.\n9\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nIMPACT STATEMENTS\nWe believe that our work has broad positive implications.\nReducing reliance of recommendation, search, and ads sys-\ntems on the large number of heterogeneous features can\nmake these systems much more privacy friendly while im-\nproving user experiences. Enabling recommendation sys-\ntems to attribute users\u2019 long term outcomes to short-term\ndecisions via fully sequential formulations could reduce the\nprevalence of content that do not serve users\u2019 long term\ngoals (including clickbaits and fake news) across the web,\nand better align incentives of platforms with user values. Fi-\nnally, applications of foundation models and scaling law can\nhelp reduce carbon footprints incurred with model research\nand developments needed for recommendations, search, and\nrelated use cases.\nAcknowledgements\nThis work represents the joint efforts of hundreds of people,\nand would not be possible without work from the following\ncontributors (alphabetical order): Adnan Akhundov, Bugra\nAkyildiz, Shabab Ayub, Alex Bao, Renqin Cai, Jennifer\nCao, Guoqiang Jerry Chen, Lei Chen, Sean Chen, Xianjie\nChen, Huihui Cheng, Weiwei Chu, Ted Cui, Shiyan Deng,\nNimit Desai, Fei Ding, Francois Fagan, Lu Fang, Liang\nGuo, Liz Guo, Jeevan Gyawali, Yuchen Hao, Daisy Shi He,\nSamuel Hsia, Jie Hua, Yanzun Huang, Hongyi Jia, Rui Jian,\nJian Jin, Rahul Kindi, Changkyu Kim, Yejin Lee, Fu Li,\nHong Li, Shen Li, Wei Li, Zhijing Li, Xueting Liao, Emma\nLin, Hao Lin, Jingzhou Liu, Xingyu Liu, Kai Londenberg,\nLiang Luo, Linjian Ma, Matt Ma, Yun Mao, Bert Maher,\nMatthew Murphy, Satish Nadathur, Min Ni, Jongsoo Park,\nJing Qian, Lijing Qin, Alex Singh, Timothy Shi, Dennis\nvan der Staay, Xiao Sun, Colin Taylor, Shin-Yeh Tsai, Ro-\nhan Varma, Omkar Vichare, Alyssa Wang, Pengchao Wang,\nShengzhi Wang, Wenting Wang, Xiaolong Wang, Zhiyong\nWang, Wei Wei, Bin Wen, Carole-Jean Wu, Eric Xu, Bi Xue,\nZheng Yan, Chao Yang, Junjie Yang, Zimeng Yang, Chunx-\ning Yin, Daniel Yin, Yiling You, Keke Zhai, Yanli Zhao,\nZhuoran Zhao, Hui Zhang, Jingjing Zhang, Lu Zhang, Lujia\nZhang, Na Zhang, Rui Zhang, Xiong Zhang, Ying Zhang,\nZhiyun Zhang, Charles Zheng, Erheng Zhong, Xin Zhuang.\nWe would like to thank Shikha Kapoor, Rex Cheung, Lana\nDam, Ram Ramanathan, Nipun Mathur, Bo Feng, Yanhong\nWu, Zhaohui Guo, Hongjie Bai, Wen-Yun Yang, Zellux\nWang, Arun Singh, Bruce Deng, Yisong Song, Haotian Wu,\nMeihong Wang for product support, and Joseph Laria, Ak-\nshay Hegde, Abha Jain, Raj Ganapathy for assistance with\nprogram management. Finally, we would like to thank Ajit\nMathews, Shilin Ding, Hong Yan, Lars Backstrom for their\nleadership support, and insightful discussions with Andrew\nTulloch, Liang Xiong, Kaushik Veeraraghavan, and Gaofeng\nZhao.",
          "translated": "**\uacb0\ub860**\n\n\uc0dd\uc131 \ucd94\ucc9c \uc2dc\uc2a4\ud15c(GRs)\uc758 \uc0c8\ub85c\uc6b4 \ud328\ub7ec\ub2e4\uc784\uc744 \uc81c\uc548\ud588\uc73c\uba70, \uc21c\ucc28\uc801 \ucd94\ub860 \uc791\uc5c5\uc73c\ub85c \uc21c\uc704 \ubc0f \uac80\uc0c9\uc744 \ud3ec\uad04\uc801\uc73c\ub85c \uad6c\uc131\ud558\uc5ec \uc0dd\uc131\uc801\uc778 \ubc29\uc2dd\uc73c\ub85c \ud6c8\ub828\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub294 HSTU \uc778\ucf54\ub354 \uc124\uacc4\uc758 \ud601\uc2e0\uc801\uc778 \uae30\ub2a5 \ub355\ubd84\uc5d0 8192 \uae38\uc774 \uc2dc\ud000\uc2a4\uc5d0\uc11c \uae30\uc874\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ubcf4\ub2e4 5.3\ubc30\uc5d0\uc11c 15.2\ubc30 \ub354 \ube60\ub974\ub2e4\ub294 \uc810\uacfc M-FALCON\uacfc \uac19\uc740 \uc0c8\ub85c\uc6b4 \ud6c8\ub828 \ubc0f \ucd94\ub860 \uc54c\uace0\ub9ac\uc998\uc758 \ud65c\uc6a9 \ub355\ubd84\uc5d0 \uac00\ub2a5\ud569\ub2c8\ub2e4. GRs\ub97c \uc0ac\uc6a9\ud558\uba74 285\ubc30 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uba74\uc11c \ucd94\ub860 \ucef4\ud4e8\ud305 \ube44\uc6a9\uc744 \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. GRs\uc640 HSTU\ub294 \uc0dd\uc0b0 \ud658\uacbd\uc5d0\uc11c 12.4%\uc758 \uba54\ud2b8\ub9ad \uac1c\uc120\uc744 \uac00\uc838\uc654\uc73c\uba70, \uc804\ud1b5\uc801\uc778 DLRM\uc5d0 \ube44\ud574 \uc6b0\uc218\ud55c \ud655\uc7a5 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \uacb0\uacfc\ub294 \uc0ac\uc6a9\uc790 \ud589\ub3d9\uc774 \uc0dd\uc131 \ubaa8\ub378\ub9c1\uc5d0\uc11c \uc18c\uc678\ub41c \ubaa8\ub2e4\ub9ac\uc784\uc744 \ub4b7\ubc1b\uce68\ud558\uba70, \uc81c\ubaa9\ucc98\ub7fc \u201c\ud589\ub3d9\uc740 \ub9d0\ubcf4\ub2e4 \ub0ab\ub2e4\u201d\ub294 \uac83\uc744 \ubc18\uc601\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \uc791\uc5c5\uc5d0\uc11c \ud2b9\uc9d5\uc758 \uadf9\uc801\uc778 \ub2e8\uc21c\ud654\ub294 \ucd94\ucc9c, \uac80\uc0c9 \ubc0f \uad11\uace0\ub97c \uc704\ud55c \ucd5c\ucd08\uc758 \uae30\ubc18 \ubaa8\ub378\uc744 \uc704\ud55c \uae38\uc744 \uc5f4\uc5b4\uc90d\ub2c8\ub2e4. \uc774\ub294 \ub3c4\uba54\uc778 \uac04\uc5d0 \ub2e8\uc77c \ud2b9\uc9d5 \uacf5\uac04\uc744 \ud65c\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. GRs\uc758 \uc644\uc804\ud55c \uc21c\ucc28\uc801 \uc124\uc815\uc740 \ucd94\ucc9c\uc774 \uc5d4\ub4dc\ud22c\uc5d4\ub4dc, \uc0dd\uc131\uc801\uc778 \ubc29\uc2dd\uc73c\ub85c \uad6c\uc131\ub418\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uae30\ub2a5\ub4e4\uc740 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc774 \uc0ac\uc6a9\uc790\uc5d0\uac8c \ubcf4\ub2e4 \uc804\uccb4\uc801\uc73c\ub85c \ub3c4\uc6c0\uc744 \uc904 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n\n9\n\n\ud589\ub3d9\uc740 \ub9d0\ubcf4\ub2e4 \ub0ab\ub2e4: \uc0dd\uc131 \ucd94\ucc9c\uc744 \uc704\ud55c \uc21c\ucc28\uc801 \ud2b8\ub79c\uc2a4\ub4c0\uc11c\n\n\uc601\ud5a5 \ud3c9\uac00\n\n\uc800\ud76c \uc791\uc5c5\uc774 \ucd94\ucc9c, \uac80\uc0c9 \ubc0f \uad11\uace0 \uc2dc\uc2a4\ud15c\uc5d0 \ub300\ud55c \ub300\uaddc\ubaa8\uc758 \ub2e4\uc591\ud55c \ud2b9\uc9d5\uc5d0 \ub300\ud55c \uc758\uc874\uc131\uc744 \uc904\uc5ec \uc774\ub7ec\ud55c \uc2dc\uc2a4\ud15c\uc744 \ud6e8\uc52c \ub354 \uac1c\uc778 \uc815\ubcf4 \ubcf4\ud638 \uce5c\ud654\uc801\uc73c\ub85c \ub9cc\ub4e4\uace0 \uc0ac\uc6a9\uc790 \uacbd\ud5d8\uc744 \uac1c\uc120\ud560 \uc218 \uc788\ub2e4\uace0 \ubbff\uc2b5\ub2c8\ub2e4. \uc21c\ucc28\uc801 \uad6c\uc131\uc73c\ub85c \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc774 \uc0ac\uc6a9\uc790\uc758 \uc7a5\uae30\uc801\uc778 \uacb0\uacfc\ub97c \ub2e8\uae30\uc801\uc778 \uacb0\uc815\uc5d0 \uc5f0\uacb0\ud558\uc5ec \ucd94\uc801\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\uba74 \uc6f9 \uc804\uccb4\uc5d0\uc11c \uc0ac\uc6a9\uc790 \ubaa9\ud45c\ub97c \ucda9\uc871\ud558\uc9c0 \ubabb\ud558\ub294 \ucf58\ud150\uce20\uc758 \uc720\ud589\uc744 \uc904\uc774\uace0 \ud50c\ub7ab\ud3fc\uc758 \uc778\uc13c\ud2f0\ube0c\ub97c \uc0ac\uc6a9\uc790 \uac00\uce58\uc640 \ub354 \uc798 \uc77c\uce58\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uae30\ubc18 \ubaa8\ub378 \ubc0f \uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc758 \uc801\uc6a9\uc740 \ucd94\ucc9c, \uac80\uc0c9 \ubc0f \uad00\ub828 \uc0ac\ub840\uc5d0 \ub300\ud55c \ubaa8\ub378 \uc5f0\uad6c \ubc0f \uac1c\ubc1c\uacfc \uad00\ub828\ub41c \ud0c4\uc18c \ubc1c\uc790\uad6d\uc744 \uc904\uc774\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uac10\uc0ac\uc758 \uae00\n\n\uc774 \uc791\uc5c5\uc740 \uc218\ubc31 \uba85\uc758 \uacf5\ub3d9 \ub178\ub825\uc758 \uacb0\uacfc\uc774\uba70, \ub2e4\uc74c \uae30\uc5ec\uc790\uc758 \ub178\ub825 \uc5c6\uc774\ub294 \ubd88\uac00\ub2a5\ud588\uc744 \uac83\uc785\ub2c8\ub2e4(\uc54c\ud30c\ubcb3 \uc21c\uc11c): \uc544\ub4dc\ub09c \uc544\ud6c4\u09a8\u09cd\u09a6\uc624\ud504, \ubd80\uadf8\ub77c \uc544\ud0a4\ub51c\uc988, \uc0e4\ubc14\ube0c \uc544\uc774\uc720\ube0c, \uc54c\ub809\uc2a4 \ubc14\uc624, \ub80c\ud0a8 \uce74\uc774, \uc81c\ub2c8\ud37c \uce74\uc624, \uad88\uae30\uc559 \uc81c\uc2dc \uccb8, \ub808\uc774 \uccb8, \uc120 \uccb8, \uc158\uc9c0\uc608 \uccb8, \ud734\uc774\ud734 \uc468, \uc6e8\uc774\uc6e8 \ucd94, \ud14c\ub4dc \ucd94\uc774, \uc2dc\uc57c\uc5d4 \ub369, \ub2c8\ubc0b \ub370\uc0ac\uc774, \ud398\uc774 \ub529, \ud504\ub791\uc218\uc544 \ud30c\uac00, \ub8e8 \ud321, \ub9ac\uc7a5 \uad88, \ub9ac\uc988 \uad88, \uc9c0\uc644 \uac38\uc648\ub9ac, \uc720\uccb8 \ud558\uc624, \ub370\uc774\uc2dc \uc26c \ud5e4, \uc0ac\ubb34\uc5d8 \ud558\uc2a4\uc57c, \uc950\ud654 \ud558\uc624, \uc580\uc988\uc6b4 \ud669, \ud64d\uc774 \uc7c8, \ub8e8 \uc468, \uc7c8\uc778 \uc708, \ub784\ud750 \ud0a8\ub514, \uc468\ud0a4\uc6b0 \ud0b4, \uc608\uc9c4 \ub9ac, \ud478 \ub9ac, \ud64d \ub9ac, \uc154\uc57c \ub9ac, \uc6e8\uc774 \ub9ac, \uc988\uc9d5 \ub9ac, \uc464\uc5d0\ud305 \ub9ac\uc544\uc624, \uc5d0\ub9c8 \ub9b0, \ud558\uc624 \ub9b0, \uc9d5\uc8fc \ub958, \uc2f1\uc720 \ub9ac\uc6b0, \uce74\uc774 \ub860\ub374\ubca0\ub974\ud06c, \ub9ac\uc7a5 \ub8e8\uc624, \ub9b0\uc9c0\uc548 \ub9c8, \ub9f7 \ub9c8, \uc708 \ub9c8\uc624, \ubca0\ub974\ud2b8 \ub9c8\ud5c8, \ub9e4\ud29c \uba38\ud53c, \uc0ac\ud2f0\uc26c \ub098\ub2e4\ucd94\ub974, \ubbfc \ub2c8, \uc885\uc218 \ubc15, \uc9d5 \uccb8, \ub9ac\uc9d5 \ud038, \uc54c\ub809\uc2a4 \uc2f1, \ud0c0\uc774\ubaa8\uc2dc \uc26c, \ub370\ub2c8\uc2a4 \ubc18 \ub370\ub974 \uc2a4\ud0c0\uc774, \uc2dc\uc544\uc624 \uc36c, \ucf5c\ub9b0 \ud14c\uc77c\ub7ec, \uc2e0\uc608 \ucd5c, \ub85c\ud55c \ubc14\ub974\ub9c8, \uc634\uce74 \ube44\ucc28\ub808, \uc54c\ub9ac\uc0ac \uc655, \ud33d\ucc28\uc624 \uc655, \uc131\uc9c0 \uc655, \uc6ec\ud305 \uc655, \uc154\ub871 \uc655, \uc9c0\uc6a9 \uc655, \ubc14\uc774 \uc708, \uce74\ub97c-\uc468 \uc6b0, \uc5d0\ub9ad \uc464, \ube44 \uc464\uc5d0, \uc815 \uc60c, \ucd94\uc601 \uc591, \uc8fc\uc774\uc81c \uc591, \uc9c0\uba85 \uc591, \uce08\uc2f1 \uc708, \ub2e4\ub2c8\uc5d8 \uc708, \uc774\ub839 \uc720, \ucf00\ucf00 \uc9dc\uc774, \uc60c\ub9ac \uc9dc\uc624, \uc988\ub8e8\uc548 \uc9dc\uc624, \ud68c\uc7a5 \uc9dc\uc624, \uc9d5\uc9d5 \uc9dc\uc624, \ub8e8 \uc9dc\uc624,  lujia \uc9dc\uc624, na \uc9dc\uc624, Rui \uc9dc\uc624, xiong \uc9dc\uc624, ying \uc9dc\uc624, zhiyun \uc9dc\uc624, charles zheng, erheng zhong, xin zhuang.\n\nShikha Kapoor, Rex Cheung, Lana Dam, Ram Ramanathan, Nipun Mathur, Bo Feng, Yanhong Wu, Zhaohui Guo, Hongjie Bai, Wen-Yun Yang, Zellux Wang, Arun Singh, Bruce Deng, Yisong Song, Haotian Wu, Meihong Wang for product support, and Joseph Laria, Akshay Hegde, Abha Jain, Raj Ganapathy for assistance with program management. Finally, we would like to thank Ajit Mathews, Shilin Ding, Hong Yan, Lars Backstrom for their leadership support, and insightful discussions with Andrew Tulloch, Liang Xiong, Kaushik Veeraraghavan, and Gaofeng Zhao."
        },
        {
          "name": "References",
          "original": "Bao, K., Zhang, J., Zhang, Y ., Wang, W., Feng, F., and\nHe, X. Tallrec: An effective and efficient tuning frame-\nwork to align large language model with recommenda-\ntion. In Proceedings of the 17th ACM Conference on\nRecommender Systems, RecSys \u201923. ACM, September\n2023. doi: 10.1145/3604915.3608857. URL http:\n//dx.doi.org/10.1145/3604915.3608857.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. 2020.\nChang, J., Zhang, C., Fu, Z., Zang, X., Guan, L., Lu, J., Hui,\nY ., Leng, D., Niu, Y ., Song, Y ., and Gai, K. Twin: Two-\nstage interest network for lifelong user behavior modeling\nin ctr prediction at kuaishou, 2023.\nChen, Q., Zhao, H., Li, W., Huang, P., and Ou, W. Behavior\nsequence transformer for e-commerce recommendation in\nalibaba. In Proceedings of the 1st International Workshop\non Deep Learning Practice for High-Dimensional Sparse\nData, DLP-KDD \u201919, New York, NY , USA, 2019. Asso-\nciation for Computing Machinery. ISBN 9781450367837.\ndoi: 10.1145/3326937.3341261. URL https://doi.\norg/10.1145/3326937.3341261.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In Proceedings of the 37th International\nConference on Machine Learning, ICML\u201920, 2020.\nCheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T.,\nAradhye, H., Anderson, G., Corrado, G., Chai, W., Ispir,\nM., Anil, R., Haque, Z., Hong, L., Jain, V ., Liu, X., and\nShah, H. Wide & deep learning for recommender systems.\nIn Proceedings of the 1st Workshop on Deep Learning\nfor Recommender Systems, DLRS 2016, pp. 7\u201310, 2016.\nISBN 9781450347952.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gener-\nating long sequences with sparse transformers. CoRR,\nabs/1904.10509, 2019. URL http://arxiv.org/\nabs/1904.10509.\nCovington, P., Adams, J., and Sargin, E. Deep neural net-\nworks for youtube recommendations. In Proceedings\nof the 10th ACM Conference on Recommender Systems,\nRecSys \u201916, pp. 191\u2013198, 2016. ISBN 9781450340359.\n10\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nCui, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. M6-rec:\nGenerative pretrained language models are open-ended\nrecommender systems, 2022.\nDallmann, A., Zoller, D., and Hotho, A. A case study on\nsampling strategies for evaluating neural sequential item\nrecommendation models. In Proceedings of the 15th\nACM Conference on Recommender Systems, RecSys \u201921,\npp. 505\u2013514, 2021. ISBN 9781450384582.\nDao, T. Flashattention-2: Faster attention with better paral-\nlelism and work partitioning, 2023.\nDao, T., Fu, D. Y ., Ermon, S., Rudra, A., and R\u00b4e, C. FlashAt-\ntention: Fast and memory-efficient exact attention with\nIO-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding. In Burstein, J., Doran, C., and\nSolorio, T. (eds.), Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pp. 4171\u20134186.\nAssociation for Computational Linguistics, 2019. doi:\n10.18653/v1/n19-1423. URL https://doi.org/\n10.18653/v1/n19-1423.\nEksombatchai, C., Jindal, P., Liu, J. Z., Liu, Y ., Sharma, R.,\nSugnet, C., Ulrich, M., and Leskovec, J. Pixie: A system\nfor recommending 3+ billion items to 200+ million users\nin real-time. In Proceedings of the 2018 World Wide Web\nConference, WWW \u201918, pp. 1775\u20131784, 2018. ISBN\n9781450356398.\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\nlinear units for neural network function approximation\nin reinforcement learning. CoRR, abs/1702.03118, 2017.\nURL http://arxiv.org/abs/1702.03118.\nGao, W., Fan, X., Wang, C., Sun, J., Jia, K., Xiao, W., Ding,\nR., Bin, X., Yang, H., and Liu, X. Learning an end-to-end\nstructure for retrieval in large-scale recommendations. In\nProceedings of the 30th ACM International Conference\non Information and Knowledge Management, CIKM \u201921,\npp. 524\u2013533, 2021. ISBN 9781450384469.\nGillenwater, J., Kulesza, A., Fox, E., and Taskar, B.\nExpectation-maximization for learning determinantal\npoint processes. In Proceedings of the 27th International\nConference on Neural Information Processing Systems\n- Volume 2, NIPS\u201914, pp. 3149\u20133157, Cambridge, MA,\nUSA, 2014. MIT Press.\nGu, A., Goel, K., and R \u00b4e, C. Efficiently modeling long\nsequences with structured state spaces. In The Tenth\nInternational Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022 . OpenRe-\nview.net, 2022. URL https://openreview.net/\nforum?id=uYLFoz1vlAC.\nGuo, H., Tang, R., Ye, Y ., Li, Z., and He, X. Deepfm:\nA factorization-machine based neural network for ctr\nprediction. In Proceedings of the 26th International\nJoint Conference on Artificial Intelligence, IJCAI\u201917, pp.\n1725\u20131731, 2017. ISBN 9780999241103.\nGupta, M. R., Bengio, S., and Weston, J. Training highly\nmulticlass classifiers. J. Mach. Learn. Res. , 15(1):\n1461\u20131492, jan 2014. ISSN 1532-4435.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep resid-\nual learning for image recognition. arXiv preprint\narXiv:1512.03385, 2015.\nHe, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y .,\nAtallah, A., Herbrich, R., Bowers, S., and Candela, J. Q.\nPractical lessons from predicting clicks on ads at face-\nbook. In ADKDD\u201914: Proceedings of the Eighth Interna-\ntional Workshop on Data Mining for Online Advertising,\nNew York, NY , USA, 2014. Association for Computing\nMachinery. ISBN 9781450329996.\nHidasi, B., Karatzoglou, A., Baltrunas, L., and Tikk, D.\nSession-based recommendations with recurrent neural\nnetworks. In Bengio, Y . and LeCun, Y . (eds.),4th Inter-\nnational Conference on Learning Representations, ICLR\n2016, San Juan, Puerto Rico, May 2-4, 2016, Conference\nTrack Proceedings, 2016. URL http://arxiv.org/\nabs/1511.06939.\nHou, Y ., Zhang, J., Lin, Z., Lu, H., Xie, R., McAuley, J., and\nZhao, W. X. Large language models are zero-shot rankers\nfor recommender systems. In Advances in Information\nRetrieval - 46th European Conference on IR Research,\nECIR 2024, 2024.\nHua, W., Dai, Z., Liu, H., and Le, Q. V . Transformer qual-\nity in linear time. In Chaudhuri, K., Jegelka, S., Song,\nL., Szepesv\u00b4ari, C., Niu, G., and Sabato, S. (eds.), Inter-\nnational Conference on Machine Learning, ICML 2022,\n17-23 July 2022, Baltimore, Maryland, USA, volume 162\nof Proceedings of Machine Learning Research, pp. 9099\u2013\n9117. PMLR, 2022. URL https://proceedings.\nmlr.press/v162/hua22a.html.\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K.\nDeep networks with stochastic depth, 2016.\nJegou, H., Douze, M., and Schmid, C. Product quantization\nfor nearest neighbor search. IEEE Trans. Pattern Anal.\n11\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nMach. Intell., 33(1):117\u2013128, jan 2011. ISSN 0162-8828.\ndoi: 10.1109/TPAMI.2010.57. URL https://doi.\norg/10.1109/TPAMI.2010.57.\nKang, W.-C. and McAuley, J. Self-attentive sequential\nrecommendation. In 2018 International Conference on\nData Mining (ICDM), pp. 197\u2013206, 2018.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\nCoRR, abs/2001.08361, 2020. URL https://arxiv.\norg/abs/2001.08361.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning , ICML\u201920.\nJMLR.org, 2020.\nKhudia, D., Huang, J., Basu, P., Deng, S., Liu, H., Park,\nJ., and Smelyanskiy, M. Fbgemm: Enabling high-\nperformance low-precision deep learning inference.arXiv\npreprint arXiv:2101.05615, 2021.\nKlenitskiy, A. and Vasilev, A. Turning dross into gold loss:\nis bert4rec really better than sasrec? In Proceedings of the\n17th ACM Conference on Recommender Systems, RecSys\n\u201923, pp. 1120\u20131125, New York, NY , USA, 2023. Associa-\ntion for Computing Machinery. ISBN 9798400702419.\ndoi: 10.1145/3604915.3610644. URL https://doi.\norg/10.1145/3604915.3610644.\nKorthikanti, V ., Casper, J., Lym, S., McAfee, L., Andersch,\nM., Shoeybi, M., and Catanzaro, B. Reducing activation\nrecomputation in large transformer models, 2022.\nLi, C., Chang, E., Garcia-Molina, H., and Wiederhold,\nG. Clustering for approximate similarity search in high-\ndimensional spaces. IEEE Transactions on Knowledge\nand Data Engineering, 14(4):792\u2013808, 2002.\nLi, J., Wang, M., Li, J., Fu, J., Shen, X., Shang, J., and\nMcAuley, J. Text is all you need: Learning language\nrepresentations for sequential recommendation. In KDD,\n2023.\nLiu, Z., Zou, L., Zou, X., Wang, C., Zhang, B., Tang, D.,\nZhu, B., Zhu, Y ., Wu, P., Wang, K., and Cheng, Y . Mono-\nlith: Real time recommendation system with collisionless\nembedding table, 2022.\nMa, J., Zhao, Z., Yi, X., Chen, J., Hong, L., and Chi, E. H.\nModeling task relationships in multi-task learning with\nmulti-gate mixture-of-experts. KDD \u201918, 2018.\nMudigere, D., Hao, Y ., Huang, J., Jia, Z., Tulloch, A.,\nSridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo,\nL., Yang, J. A., Gao, L., Ivchenko, D., Basant, A., Hu,\nY ., Yang, J., Ardestani, E. K., Wang, X., Komuravelli,\nR., Chu, C.-H., Yilmaz, S., Li, H., Qian, J., Feng, Z.,\nMa, Y ., Yang, J., Wen, E., Li, H., Yang, L., Sun, C.,\nZhao, W., Melts, D., Dhulipala, K., Kishore, K., Graf,\nT., Eisenman, A., Matam, K. K., Gangidi, A., Chen,\nG. J., Krishnan, M., Nayak, A., Nair, K., Muthiah, B.,\nkhorashadi, M., Bhattacharya, P., Lapukhov, P., Nau-\nmov, M., Mathews, A., Qiao, L., Smelyanskiy, M., Jia,\nB., and Rao, V . Software-hardware co-design for fast\nand scalable training of deep learning recommendation\nmodels. In Proceedings of the 49th Annual Interna-\ntional Symposium on Computer Architecture, ISCA \u201922,\npp. 993\u20131011, New York, NY , USA, 2022. Associa-\ntion for Computing Machinery. ISBN 9781450386104.\ndoi: 10.1145/3470496.3533727. URL https://doi.\norg/10.1145/3470496.3533727.\nPeng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Ef-\nficient context window extension of large language mod-\nels. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.\nnet/forum?id=wHBfxhZu1u.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and\nDean, J. Efficiently scaling transformer inference, 2022.\nPress, O., Smith, N. A., and Lewis, M. Train short, test\nlong: Attention with linear biases enables input length\nextrapolation. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022. URL https:\n//openreview.net/forum?id=R8sQPpGCv0.\nRabe, M. N. and Staats, C. Self-attention does not need\no(n2) memory, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN\n1532-4435.\nRendle, S. Factorization machines. In 2010 IEEE Inter-\nnational Conference on Data Mining (ICDM), pp. 995\u2013\n1000, 2010. doi: 10.1109/ICDM.2010.127.\nRendle, S., Krichene, W., Zhang, L., and Anderson, J. Neu-\nral collaborative filtering vs. matrix factorization revisited.\nIn Fourteenth ACM Conference on Recommender Systems\n(RecSys\u201920), pp. 240\u2013248, 2020. ISBN 9781450375832.\nShazeer, N. Glu variants improve transformer, 2020.\n12\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nShin, K., Kwak, H., Kim, S. Y ., Ramstr\u00a8om, M. N., Jeong,\nJ., Ha, J.-W., and Kim, K.-M. Scaling law for recommen-\ndation models: towards general-purpose user representa-\ntions. In Proceedings of the Thirty-Seventh AAAI Con-\nference on Artificial Intelligence and Thirty-Fifth Confer-\nence on Innovative Applications of Artificial Intelligence\nand Thirteenth Symposium on Educational Advances\nin Artificial Intelligence , AAAI\u201923/IAAI\u201923/EAAI\u201923.\nAAAI Press, 2023. ISBN 978-1-57735-880-0. doi:\n10.1609/aaai.v37i4.25582. URL https://doi.org/\n10.1609/aaai.v37i4.25582.\nShrivastava, A. and Li, P. Asymmetric lsh (alsh) for sub-\nlinear time maximum inner product search (mips). In\nAdvances in Neural Information Processing Systems, vol-\nume 27, 2014.\nSileo, D., V ossen, W., and Raymaekers, R. Zero-shot rec-\nommendation as language modeling. In Hagen, M.,\nVerberne, S., Macdonald, C., Seifert, C., Balog, K.,\nN\u00f8rv\u02daag, K., and Setty, V . (eds.), Advances in Informa-\ntion Retrieval - 44th European Conference on IR Re-\nsearch, ECIR 2022, Stavanger, Norway, April 10-14,\n2022, Proceedings, Part II , volume 13186 of Lecture\nNotes in Computer Science, pp. 223\u2013230. Springer, 2022.\ndoi: 10.1007/978-3-030-99739-7 \\ 26. URL https://\ndoi.org/10.1007/978-3-030-99739-7_26 .\nSu, J., Lu, Y ., Pan, S., Murtadha, A., Wen, B., and Liu,\nY . Roformer: Enhanced transformer with rotary position\nembedding, 2023.\nSun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P.\nBert4rec: Sequential recommendation with bidirectional\nencoder representations from transformer. InProceedings\nof the 28th ACM International Conference on Information\nand Knowledge Management, CIKM \u201919, pp. 1441\u20131450,\n2019. ISBN 9781450369763.\nTang, H., Liu, J., Zhao, M., and Gong, X. Progres-\nsive layered extraction (ple): A novel multi-task learn-\ning (mtl) model for personalized recommendations.\nIn Proceedings of the 14th ACM Conference on Rec-\nommender Systems , RecSys \u201920, pp. 269\u2013278, New\nYork, NY , USA, 2020. Association for Computing\nMachinery. ISBN 9781450375832. doi: 10.1145/\n3383313.3412236. URL https://doi.org/10.\n1145/3383313.3412236.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,\nMao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Proceedings of the 31st International\nConference on Neural Information Processing Systems,\nNIPS\u201917, pp. 6000\u20136010, 2017. ISBN 9781510860964.\nWang, R., Shivanna, R., Cheng, D., Jain, S., Lin, D., Hong,\nL., and Chi, E. Dcn v2: Improved deep & cross network\nand practical lessons for web-scale learning to rank sys-\ntems. In Proceedings of the Web Conference 2021, WWW\n\u201921, pp. 1785\u20131797, New York, NY , USA, 2021. Associa-\ntion for Computing Machinery. ISBN 9781450383127.\ndoi: 10.1145/3442381.3450078. URL https://doi.\norg/10.1145/3442381.3450078.\nWang, Z., Zhao, L., Jiang, B., Zhou, G., Zhu, X., and Gai, K.\nCold: Towards the next generation of pre-ranking system,\n2020.\nXia, X., Eksombatchai, P., Pancha, N., Badani, D. D.,\nWang, P.-W., Gu, N., Joshi, S. V ., Farahpour, N., Zhang,\nZ., and Zhai, A. Transact: Transformer-based real-\ntime user action model for recommendation at pinterest.\nIn Proceedings of the 29th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining , KDD \u201923,\npp. 5249\u20135259, New York, NY , USA, 2023. Associa-\ntion for Computing Machinery. ISBN 9798400701030.\ndoi: 10.1145/3580305.3599918. URL https://doi.\norg/10.1145/3580305.3599918.\nXiao, J., Ye, H., He, X., Zhang, H., Wu, F., and Chua,\nT.-S. Attentional factorization machines: Learning the\nweight of feature interactions via attention networks. In\nProceedings of the 26th International Joint Conference on\nArtificial Intelligence, IJCAI\u201917, pp. 3119\u20133125. AAAI\nPress, 2017. ISBN 9780999241103.\nXiong, R., Yang, Y ., He, D., Zheng, K., Zheng, S., Xing, C.,\nZhang, H., Lan, Y ., Wang, L., and Liu, T.-Y . On layer\nnormalization in the transformer architecture. In Proceed-\nings of the 37th International Conference on Machine\nLearning, ICML\u201920. JMLR.org, 2020.\n13\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nYang, J., Yi, X., Zhiyuan Cheng, D., Hong, L., Li, Y ., Xi-\naoming Wang, S., Xu, T., and Chi, E. H. Mixed negative\nsampling for learning two-tower neural networks in rec-\nommendations. In Companion Proceedings of the Web\nConference 2020, WWW \u201920, pp. 441\u2013447, 2020. ISBN\n9781450370240.\nZhai, J., Lou, Y ., and Gehrke, J. Atlas: A probabilistic algo-\nrithm for high dimensional similarity search. In Proceed-\nings of the 2011 ACM SIGMOD International Conference\non Management of Data, SIGMOD \u201911, pp. 997\u20131008,\n2011. ISBN 9781450306614.\nZhai, J., Gong, Z., Wang, Y ., Sun, X., Yan, Z., Li, F., and\nLiu, X. Revisiting neural retrieval on accelerators. In\nProceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining , KDD \u201923, pp.\n5520\u20135531, New York, NY , USA, 2023a. Association for\nComputing Machinery. ISBN 9798400701030. doi: 10.\n1145/3580305.3599897. URL https://doi.org/\n10.1145/3580305.3599897.\nZhai, Y ., Jiang, C., Wang, L., Jia, X., Zhang, S., Chen,\nZ., Liu, X., and Zhu, Y . Bytetransformer: A high-\nperformance transformer boosted for variable-length\ninputs. In 2023 IEEE International Parallel and Dis-\ntributed Processing Symposium (IPDPS), pp. 344\u2013355,\nLos Alamitos, CA, USA, may 2023b. IEEE Computer\nSociety. doi: 10.1109/IPDPS54959.2023.00042. URL\nhttps://doi.ieeecomputersociety.org/\n10.1109/IPDPS54959.2023.00042.\nZhang, B., Luo, L., Liu, X., Li, J., Chen, Z., Zhang, W., Wei,\nX., Hao, Y ., Tsang, M., Wang, W., Liu, Y ., Li, H., Badr,\nY ., Park, J., Yang, J., Mudigere, D., and Wen, E. Dhen: A\ndeep and hierarchical ensemble network for large-scale\nclick-through rate prediction, 2022.\nZhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., and\nTang, J. Deep reinforcement learning for page-wise\nrecommendations. In Proceedings of the 12th ACM\nConference on Recommender Systems, RecSys \u201918, pp.\n95\u2013103, New York, NY , USA, 2018. Association for\nComputing Machinery. ISBN 9781450359016. doi: 10.\n1145/3240323.3240374. URL https://doi.org/\n10.1145/3240323.3240374.\nZhao, Z., Yang, Y ., Wang, W., Liu, C., Shi, Y ., Hu, W.,\nZhang, H., and Yang, S. Breaking the curse of quality\nsaturation with user-centric ranking, 2023.\nZhou, G., Zhu, X., Song, C., Fan, Y ., Zhu, H., Ma, X., Yan,\nY ., Jin, J., Li, H., and Gai, K. Deep interest network for\nclick-through rate prediction. KDD \u201918, 2018.\nZhou, K., Wang, H., Zhao, W. X., Zhu, Y ., Wang, S.,\nZhang, F., Wang, Z., and Wen, J.-R. S3-rec: Self-\nsupervised learning for sequential recommendation with\nmutual information maximization. In Proceedings of\nthe 29th ACM International Conference on Information\n& Knowledge Management, CIKM \u201920, pp. 1893\u20131902,\nNew York, NY , USA, 2020. Association for Comput-\ning Machinery. ISBN 9781450368599. doi: 10.1145/\n3340531.3411954. URL https://doi.org/10.\n1145/3340531.3411954.\nZhuo, J., Xu, Z., Dai, W., Zhu, H., Li, H., Xu, J., and Gai,\nK. Learning optimal tree models under beam search.\nIn Proceedings of the 37th International Conference on\nMachine Learning, ICML\u201920. JMLR.org, 2020.\n14\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nA. Notations\nWe summarize key notations used in this paper in Table 8 and Table 9.\nSymbol Description\n\u03a8k(tj)\nThe k-th training example (k is ordered globally) emitted by the feature logging system at time tj. In\na typical DLRM recommendation system, after the user consumes some content \u03a6i (by responding\nwith an action ai such as skip, video completion and share), the feature logging system joins the tuple\n(\u03a6i, ai) with the features used to rank \u03a6i, and emits (\u03a6i, ai, features for \u03a6i) as a training example\n\u03a8k(tj). As discussed in Section 2.3, DLRMs and GRs deal with different numbers of training\nexamples, with the number of examples in GRs typically being 1-2 orders of magnitude smaller.\nnc (nc,i) Number of contents that user has interacted with (of user/sample i).\n\u03a60, . . . ,\u03a6nc\u22121 List of contents that a user has interacted with, in the context of a recommendation system.\na0, . . . , anc\u22121\nList of user actions corresponding to \u03a6is. When all predicted events are binary, each action can be\nconsidered a multi-hot vector over (atomic) events such as like, share, comment, image view, video\ninitialization, video completion, hide, etc.\nE, F\nCategorical features in DLRMs, in Figure 2. E0, E1, . . ., E7, E8, and F0, F1, . . ., F7 represent\ntransformations of (\u03a60, a0, t0), . . . ,(\u03a6nc\u22121, anc\u22121, tnc\u22121) obtained at various points in time via\nfeature extraction (e.g., most recent 10 liked images, most similar 50 urls that the user clicked on in\nthe past compared to the current candidate, etc.). \u201cmerge & sequentialize\u201d denotes the (virtual)\nreverse process of obtaining the raw engagement series (\u03a60, a0, t0), . . . ,(\u03a6nc\u22121, anc\u22121, tnc\u22121).\nG, H\nCategorical features in DLRMs, in Figure 2 that are not related to user-content engagements. These\nfeatures (e.g., demographics or followed creators) are merged into the main time series (list of\ncontents user engaged with, e.g., \u03a60, a0, . . . ,\u03a6nc\u22121, anc\u22121), as discussed in Section 2.1 and\nillustrated in Figure 2.\nn (ni) Number of tokens in the sequential transduction task (of user/sample i). While O(n) = O(nc), n can\ndiffer from nc even without any non-interaction related categorical features; see e.g., Table 1.\nx0, . . . , xn\u22121 List of input tokens in the sequential transduction task.\ny0, . . . , yn\u22121 List of output tokens in the sequential transduction task.\nt0, . . . , tn\u22121 List of timestamps corresponding to when x0, . . . , xn\u22121 were observed.\nX, Xc V ocabulary of all input/output tokens (X) and its content subset (Xc).\nN, Nc maxi ni, maxi nc,i.\nut User representation at time t.\nsu(ni), \u02c6su(ni) Sampling rate for user i, used in generative training (Section 2.3).\nd Model dimension (embedding dimension).\ndqk Attention dimension size in HSTU and Transformers. This applies to Q(X) and K(X) in Equation (1).\ndv Value dimension size in HSTU. For Transformers, we typically havedqk = dv.\ndf f\nHidden dimension size in pointwise feedforward layers of Transformers. HSTU does not utilize\nfeedforward layers; see U(X) below.\nh Number of attention heads.\nl Number of layers in HSTU. For Transformers, attention and pointwise feedforward layers together\nconstitute a layer.\nTable 8. Table of Notations (continued on the next page).\nB. Generative Recommenders: Background and Formulations\nMany readers are likely more familiar with classical Deep Learning Recommendation Models (DLRMs) (Mudigere et al.,\n2022) given its popularity from YouTube DNN days (Covington et al., 2016) and its widespread usage in every single\nlarge online content and e-commerce platform (Cheng et al., 2016; Zhou et al., 2018; Wang et al., 2021; Chang et al.,\n2023; Xia et al., 2023; Zhai et al., 2023a). DLRMs operate on top of heterogeneous feature spaces using various neural\n15\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nSymbol Description\nX Input to an HSTU layer. In standard terminology (before batching), X \u2208 RN \u00d7d assuming we\nhave a input sequence containing N tokens.\nQ(X), K(X), V (X)\nQuery, key, value in HSTU obtained for a given inputX based on Equation (1). The definition\nis similar to Q, K, and V in standard Transformers. Q(X), K(X) \u2208 Rh\u00d7N \u00d7dqk , and\nV (X) \u2208 Rh\u00d7N \u00d7dv.\nU(X) HSTU uses U(X) to \u201cgate\u201d attention-pooled values (V (X)) in Equation (3), which together\nwith f2(\u00b7), enables HSTU to avoid feedforward layers altogether. U(X) \u2208 Rh\u00d7N \u00d7dv.\nA(X) Attention tensor obtained for input X. A(X) \u2208 Rh\u00d7N \u00d7N .\nY (X) Output of a HSTU layer obtained for the input X. Y (X) \u2208 Rd.\nSplit(\u00b7)\nThe operation that splits a tensor into chunks. \u03d51(f1(X))) \u2208 RN \u00d7(2hdqk+2hdv)\nin Equation (1); we obtain U(X), V (X) (both of shape h \u00d7 N \u00d7 dv), Q(X), K(X) (both of\nshape h \u00d7 N \u00d7 dqk) by splitting the larger tensor (and permuting dimensions) with\nU(X), V (X), Q(X), K(X) = Split(\u03d51(f1(X))).\nrabp,t\nrelative attention bias that incorporates both positional (Raffel et al., 2020) and temporal\ninformation (based on the time when the tokens are observed, t0, . . . , tn\u22121; one possible\nimplementation is to apply some bucketization function to (tj \u2212 ti) for (i, j)). In practice, we\nshare rabp,t across different attention heads within a layer, hence rabp,t \u2208 R1\u00d7N \u00d7N .\n\u03b1 Parameter controlling sparsity in the Stochastic Length algorithm used in HSTU (Section 3.2).\nR Register size on GPUs, in the context of the HSTU algorithm discussed in Section 3.2.\nm Number of candidates considered in a recommendation system\u2019s ranking stage.\nbm Microbatch size, in the M-FALCON algorithm discussed in Section 3.4.\nTable 9. Table of Notations (continued)\nnetworks including feature interaction modules (Guo et al., 2017; Xiao et al., 2017; Wang et al., 2021), sequential pooling or\ntarget-aware pairwise attention modules (Hidasi et al., 2016; Zhou et al., 2018; Chang et al., 2023) and advanced multi-expert\nmulti-task modules (Ma et al., 2018; Tang et al., 2020). We hence provided an overview of Generative Recommenders\n(GRs) by contrasting them with classical DLRMs explicitly in Section 2 and Section 3. In this section, we give the readers\nan alternative perspective starting from the classical sequential recommender literature.\nB.1. Background: Sequential Recommendations in Academia and Industry\nB.1.1. A CADEMIC RESEARCH (T RADITIONAL SEQUENTIAL RECOMMENDER SETTINGS )\nRecurrent neural networks (RNNs) were first applied to recommendation scenarios in GRU4Rec (Hidasi et al., 2016).\nHidasi et al. (2016) considered Gated Recurrent Units (GRUs) and applied them over two datasets, RecSys Challenge 20152\nand VIDEO (a proprietary dataset). In both cases, only positive events (clicked e-commerce items or videos where users\nspent at least a certain amount of time watching) were kept as part of the input sequence. We further observe that in a\nclassical industrial-scale two-stage recommendation system setup consisting of retrieval and ranking stages (Covington\net al., 2016), the task that Hidasi et al. (2016) solved primarily maps to the retrieval task.\nTransformers, sequential transduction architectures, and their variants. Advances in sequential transduction ar-\nchitectures in later years, in particular Transformers (Vaswani et al., 2017), have motivated similar advancements in\nrecommendation systems. SASRec (Kang & McAuley, 2018) first applied Transformers in an autoregressive setting. They\nconsidered the presence of a review or rating as positive feedback, thereby converting classical datasets like Amazon\nReviews 3 and MovieLens 4 to sequences of positive items, similar to GRU4Rec. A binary cross entropy loss was employed,\nwhere positive target is defined as the next \u201cpositive\u201d item (recall this is in essence just presence of a review or rating), and\nnegative target is randomly sampled from the item corpus X = Xc.\n2http://2015.recsyschallenge.com/\n3https://jmcauley.ucsd.edu/data/amazon/\n4https://grouplens.org/datasets/movielens/1m/, https://grouplens.org/datasets/movielens/20m/\n16\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nMost subsequent research were built upon similar settings as GRU4Rec (Hidasi et al., 2016) and SASRec (Kang & McAuley,\n2018) discussed above, such as BERT4Rec (Sun et al., 2019) applying bidirectional encoder setting from BERT (Devlin\net al., 2019), S3Rec (Zhou et al., 2020) introducing an explicit pre-training stage, and so on.\nB.1.2. I NDUSTRIAL APPLICATIONS AS PART OF DEEP LEARNING RECOMMENDATION MODELS (DLRM S).\nSequential approaches, including sequential encoders and pairwise attention modules, have been widely applied in industrial\nsettings due to their ability to enhance user representations as part of DLRMs. DLRMs commonly use relatively small\nsequence lengths, such as 20 in BST (Chen et al., 2019), 1,000 in DIN (Zhou et al., 2018), and 100 in TransAct (Xia et al.,\n2023). We observe that these are 1-3 orders of magnitude smaller compared with 8,192 in this work (Section 4.3).\nDespite using short sequence lengths, most DLRMs can successfully capture long-term user preferences. This can be\nattributed to two key aspects. First, precomputed user profiles/embeddings (Xia et al., 2023) or external vector stores (Chang\net al., 2023) are commonly used in modern DLRMs, both of which effectively extend lookback windows. Second, a\nsignificant number of contextual-, user-, and item-side features were generally employed (Zhou et al., 2018; Chen et al.,\n2019; Chang et al., 2023; Xia et al., 2023) and various heterogeneous networks, such as FMs (Xiao et al., 2017; Guo et al.,\n2017), DCNs (Wang et al., 2021), MoEs, etc. are used to transform representations and combine outputs.\nIn contrast to sequential settings discussed in Appendix B.1.1, all major industrial work defines loss over (user/request,\ncandidate item) pairs. In the ranking setting, a multi-task binary cross-entropy loss is commonly used. In the retrieval setting,\ntwo tower setting (Covington et al., 2016) remains the dominant approach. Recent work has investigated representing the\nnext item to recommend as a probability distribution over a sequence of (sub-)tokens, such as OTM (Zhuo et al., 2020),\nand DR (Gao et al., 2021) (note that in other recent work, the same setting is sometimes denoted as \u201cgenerative retrieval\u201d).\nThey commonly utilize beam search to decode the item from sub-tokens. Advanced learned similarity functions, such as\nmixture-of-logits (Zhai et al., 2023a), have also been proposed and deployed as an alternative to two-tower setting and beam\nsearch given proliferation of modern accelerators such as GPUs, custom ASICs, and TPUs.\nFrom a problem formulation perspective, we consider all work discussed above part of DLRMs (Mudigere et al., 2022)\ngiven the model architectures, features used, and losses used differ significantly from academic sequential recommender\nresearch discussed in Appendix B.1.1. It\u2019s also worth remarking that there have been no successful applications of fully\nsequential ranking settings in industry, especially not at billion daily active users (DAU) scale, prior to this work.\nB.2. Formulations: Ranking and Retrieval as Sequential Transduction Tasks in Generative Recommenders (GRs)\nWe next discuss three limitations in the traditional sequential recommender settings and DLRM settings, and how Generative\nRecommenders (GRs) address them from a problem formulation perspective.\nIgnorance of features other than user-interacted items. Past sequential formulations only consider contents (items)\nusers explicitly interacted with (Hidasi et al., 2016; Kang & McAuley, 2018; Sun et al., 2019; Zhou et al., 2020), while\nindustry-scale recommendation systems prior to GRs are trained over a vast number of features to enhance the representation\nof users and contents (Covington et al., 2016; Cheng et al., 2016; Zhou et al., 2018; Chen et al., 2019; Chang et al., 2023; Xia\net al., 2023; Zhai et al., 2023a). GR addresses this limitation by a) compressing other categorical features and merging them\nwith the main time series, and b) capturing numerical features through cross-attention interaction utilizing a target-aware\nformulation as discussed in Section 2.1 and Figure 2. We validate this by showing that the traditional \u201cinteraction-only\u201d\nformulation that ignores such features degrades model quality significantly; experiment results can be found in the rows\nlabeled \u201cGR (interactions only)\u201d in Table 7 and Table 6, where we show utilizing only interaction history led to a 1.3%\ndecrease in hit rate@100 for retrieval and a 2.6% NE decrease in ranking (recall a 0.1% change in NE is significant, as\ndiscussed in Sections 4.1.2 and 4.3.1).\nUser representations are computed in a target-independent setting. A second issue is most traditional sequential\nrecommenders, including GRU4Rec (Hidasi et al., 2016), SASRec (Kang & McAuley, 2018), BERT4Rec (Sun et al., 2019),\nS3Rec (Zhou et al., 2020), etc. are formulated in a target-independent fashion where for a target item \u03a6i, \u03a60, \u03a61, . . . ,\u03a6i\u22121\nare used as encoder input to compute user representations, which is then used to provide predictions. In contrast, most major\nDLRM approaches used in industrial settings formulated the sequential modules used in a target-aware fashion, with the\nability to incorporate \u201ctarget\u201d (ranking candidate) information into the user representations. These include DIN (Zhou et al.,\n2018) (Alibaba), BST (Chen et al., 2019) (Alibaba), TWIN (Chang et al., 2023) (Kwai), and TransAct (Xia et al., 2023)\n(Pinterest).\n17\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nGenerative Recommenders (GRs) combines the best of both worlds by interleaving the content and action sequences\n(Section 2.2) to enable applying target-aware attention in causal, autoregressive settings. We categorize and contrast prior\nwork and this work in Table 10 5.\nInput for target\nitem i\nExpected output\nfor target item i Architecture Training Procedure\nGRs \u03a60, a0, \u03a61, a1, . . . ,\u03a6i ai (target-aware) Self-attention (HSTU) Causal autoregressive\n(streaming/single-pass)\nGRU4Rec \u03a60, \u03a61, . . . ,\u03a6i\u22121 \u03a6i\nRNNs (GRUs) Causal autoregressive\n(multi-pass)SASRec Self-attention (Transformers)\nBERT4Rec \u03a60, \u03a61, . . . ,\u03a6i\u22121\n(at inference time) \u03a6i Self-attention (Transformers) Sequential multi-pass 6\nS3Rec\nDIN\n\u03a60, \u03a61, . . . ,\u03a6i\nai (target aware,\nimplicitly as part\nof DLRMs)\nPairwise attention\nPointwise (generally\nstreaming/single pass)\nBST Self-attention (Transformers)\nTWIN Two-stage pairwise attention\nTransAct (\u03a60, a0), . . . ,(\u03a6i\u22121, ai\u22121), \u03a6i Self-attention (Transformers)\nTable 10. Comparison of prior work on sequential recommenders and GRs, in the ranking setting, with DLRMs included for completeness.\nDiscriminative formulations restrict applicability of prior sequential recommender work to pointwise settings.Finally,\ntraditional sequential recommenders are discriminative by design. Existing sequential recommender literature, including\nseminal work such as GRU4Rec and SASRec, model p(\u03a6i|\u03a60, a0, . . . ,\u03a6i\u22121, ai\u22121), or the conditional distribution of the\nnext item to recommend given users\u2019 current states. On the other hand, we observe that there are two probabilistic processes\nin standard recommendation systems, namely the process of the recommendation system suggesting a content \u03a6i (e.g., some\nphoto or video) to the user, and the process of the user reacting to the suggested content \u03a6i via some action ai (which can\nbe a combination of like, video completion, skip, etc.).\nA generative approach needs to model the joint distribution over the sequence of suggested contents and user actions,\nor p(\u03a60, a0, \u03a61, a1, . . . ,\u03a6nc\u22121, anc\u22121), as discussed in Section 2.2. Our proposal of Generative Recommenders enables\nmodeling of such distributions, as shown in Table 11 (Figure 8). Note that the next action token ( ai) prediction task is\nexactly the GR ranking setting discussed in Table 1, whereas the next content (\u03a6i) prediction task is similar to the retrieval\nsetting adapted to the interleaved setting, with the target changed in order to learn the input data distribution.\nTask Specification (Inputs / Outputs / Length)\nNext action token (ai) prediction\nxis \u03a60, a0, \u03a61, a1, . . . ,\u03a6nc\u22122, anc\u22122, \u03a6nc\u22121, anc\u22121\nyis a0, \u2205, a1, \u2205, . . . , anc\u22122, \u2205, anc\u22121, \u2205\nn 2nc\nNext content token (\u03a6i) prediction\nxis \u03a60, a0, \u03a61, a1, . . . ,\u03a6nc\u22122, anc\u22122, \u03a6nc\u22121, anc\u22121\nyis \u2205, \u03a61, \u2205, \u03a62, . . . ,\u2205, \u03a6nc\u22121, \u2205, \u2205\nn 2nc\nTable 11. Generative modeling over p(\u03a60, a0, . . . ,\u03a6nc\u22121, anc\u22121). An illustration is provided in Figure 8.\nImportantly, this formulation not only enables proper modeling of data distribution but further enables sampling sequences\nof items to recommend to the user directly via e.g., beam search. We hypothesize that this will lead to a superior approach\ncompared with traditional listwise settings (e.g., DPP (Gillenwater et al., 2014) and RL (Zhao et al., 2018)), and we leave\nthe full formulation and evaluation of such systems (briefly discussed in Section 6) as a future work.\nC. Evaluation: Synthetic Data\nAs previously discussed in Section 3.1, standard softmax attention, due to its normalization factor, makes it challenging\nto capture intensity of user preferences which is important for user representation learning. This aspect is important in\nrecommendation scenarios as the system may need to predict the intensity of engagements (e.g., number of future positive\n5Most large-scale industrial recommenders need to be trained in a streaming/single-pass setting due to vast amount of logged data.\n6BERT4Rec leverages multi-pass training with a mixture of Cloze and pointwise (last item) supervision losses; S3Rec utilizes\nmulti-pass training with pre-training and finetuning as two separate stages.\n18\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\n\u03a60\n(a0)\n\u03a61\n(a1) ... ...\u03a6i\n(ai)\n\u03a6nc-1\n(anc-1)\n\u03a61 \u03a62 \u03a6i+1\nEmbedding Layers\n(with\u00a0optional MLPs)\nTraining Input\nToken Sequence\nSupervision (Expected\nNext Item\u00a0\u03a6i)\nSelf-Attention Block(s)\nPrediction Layer\n... ...\n... ...\n... ...\nEmbedding Layers\nTraining Input\nToken Sequence\nSupervision (Expected\nNext Token\u00a0\u03a6i/ai)\nSelf-Attention Block(s)\na0 \u03a61 a1 anc-1\n\u03a60 a0 \u03a61 ... \u03a6nc-1 anc-1\nPrediction Layer\n...\n...\n...\nFigure 8. Comparison of traditional sequential recommenders (left) and Generative Recommenders (right). We illustrate sequential\nrecommenders in causal autoregressive settings and GRs without contextual features to facilitate comparison. On the left hand side, the\naction types ais are either ignored or combined with item information \u03a6is using MLPs, before going into self-attention blocks.\nactions on a particular topic) in addition to the relative ordering of items.\nTo understand this behavior, we construct synthetic data following a Dirichlet Process that generates streaming data over a\ndynamic set of vocabulary. Dirichlet Process captures the behavior that \u2018rich gets richer\u2018 in user engagement histories. We\nset up the synthetic experiment as follows:\n\u2022 We randomly assign each one of 20,000 item ids to exactly one of 100 categories.\n\u2022 We generate 1,000,000 records of length 128 each, with the first 90% being used for training and the final 10% used\nfor testing. To simulate the streaming training setting, we make the initial 40% of item ids available initially and\nthe rest available progressively at equal intervals; i.e., at record 500,000, the maximum id that can be sampled is\n(40% + 60% \u2217 0.5) \u2217 20, 000 = 14, 000.\n\u2022 We randomly select up to 5 categories out of 100 for each record and randomly sample a prior Hc over these 5\ncategories. We sequentially sample category for each position following a Dirichlet process over possible categories as\nfollows:\n\u2013 for n > 1:\n* with probability \u03b1/(\u03b1 + n \u2212 1), draw category c from Hc.\n* with probability nc/(\u03b1 + n \u2212 1), draw category c, where nc is the number of previous items with category c.\n* randomly sample an assigned item matching category c subject to streaming constraints.\nwhere \u03b1 is uniformly sampled at random from (1.0, 500.0).\nThe results can be found in Table 2. We always ablaterabp,t for HSTU as this dataset does not have timestamps. We observe\nHSTU increasing Hit Rate@10 by more than 100% relative to standard Transformers. Importantly, replacing HSTU\u2019s\npointwise attention mechanism with softmax (\u201cHSTU w/ Softmax\u201d) also leads to a significant reduction in hit rate, verifying\nthe importance of pointwise attention-like aggregation mechanisms.\nD. Evaluation: Traditional Sequential Recommender Settings\nOur evaluations in Section 4.1.1 focused on comparing HSTU with a state-of-the-art Transformer baseline, SASRec, utilizing\nlatest training recipe. In this section, we further consider two other alternative approaches.\nRecurrent neural networks (RNNs). We consider the classical work on sequential recommender, GRU4Rec (Hidasi et al.,\n2016), to help readers understand how self-attention models, including Transformers and HSTU, compare to traditional\nRNNs, when all the latest modeling and training improvements are fully incorporated.\nSelf-supervised sequential approaches. We consider the most popular work, BERT4Rec (Sun et al., 2019), to understand\nhow bidirectional self-supervision (leveraged in BERT4Rec via a Cloze objective) compares with unidirectional causal\nautoregressive settings, such as SASRec and HSTU.\n19\n\nActions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\nMethod HR@10 HR@50 HR@200 NDCG@10 NDCG@200\nML-1M\nSASRec (2023) .2853 .5474 .7528 .1603 .2498\nBERT4Rec .2843 (-0.4%) \u2013 \u2013 .1537 (-4.1%) \u2013\nGRU4Rec .2811 (-1.5%) \u2013 \u2013 .1648 (+2.8%) \u2013\nHSTU .3097 (+8.6%) .5754 (+5.1%) .7716 (+2.5%) .1720 (+7.3%) .2606 (+4.3%)\nHSTU-large .3294 (+15.5%) .5935 (+8.4%) .7839 (+4.1%) .1893 (+18.1%) .2771 (+10.9%)\nML-20M\nSASRec (2023) .2906 .5499 .7655 .1621 .2521\nBERT4Rec .2816 (-3.4%) \u2013 \u2013 .1703 (+5.1%) \u2013\nGRU4Rec .2813 (-3.2%) \u2013 \u2013 .1730 (+6.7%) \u2013\nHSTU .3252 (+11.9%) .5885 (+7.0%) .7943 (+3.8%) .1878 (+15.9%) .2774 (+10.0%)\nHSTU-large .3567 (+22.8%) .6149 (+11.8%) .8076 (+5.5%) .2106 (+30.0%) .2971 (+17.9%)\nBooks\nSASRec (2023) .0292 .0729 .1400 .0156 .0350\nHSTU .0404 (+38.4%) .0943 (+29.5%) .1710 (+22.1%) .0219 (+40.6%) .0450 (+28.6%)\nHSTU-large .0469 (+60.6%) .1066 (+46.2%) .1876 (+33.9%) .0257 (+65.8%) .0508 (+45.1%)\nTable 12. Evaluations of methods on public datasets in traditional sequential recommender settings (multi-pass, full-shuffle). Compared\nwith Table 4, two other baselines (GRU4Rec and BERT4Rec) are included for completeness.\nResults are presented in Table 12. We reuse BERT4Rec results and GRU4Rec results on ML-1M and ML-20M as reported\nby Klenitskiy & Vasilev (2023). Given a sampled softmax loss is used, we hold the number of negatives used constant (128\nfor ML-1M, ML-20M and 512 for Amazon Books) to ensure a fair comparison between methods.\nThe results confirm that SASRec remains one of the most competitive approaches in traditional sequential recommendation\nsettings when sampled softmax loss is used (Zhai et al., 2023a; Klenitskiy & Vasilev, 2023), while HSTU significantly\noutperforms evaluated transformers, RNNs, and self-supervised bidirectional transformers.\nE. Evaluation: Traditional DLRM Baselines\nThe DLRM baseline configurations used in Section 4 reflect continued iterations of hundreds of researchers and engineers\nover multiple years and a close approximation of production configurations on a large internet platform with billions of daily\nactive users before HSTUs/GRs were deployed. We give a high level description of the models used below.\nRanking Setting. The baseline ranking model, as described in (Mudigere et al., 2022), employs approximately one thousand\ndense features and fifty sparse features. We incorporated various modeling techniques such as Mixture of Experts (Ma\net al., 2018), variants of Deep & Cross Network (Wang et al., 2021), various sequential recommendation modules including\ntarget-aware pairwise attention (one commonly used variant in industrial settings can be found in (Zhou et al., 2018)), and\nresidual connection over special interaction layers (He et al., 2015; Zhang et al., 2022). For the low FLOPs regime in the\nscaling law section (Section 4.3.1), some modules with high computational costs were simplified and/or replaced with other\nstate-of-the-art variants like DCNs to achieve desired FLOPs.\nWhile we cannot disclose the exact settings due to confidentiality considerations, to the best of our knowledge, our baseline\nrepresents one of the best known DLRM approaches when recent research are fully incorporated. To validate this claim and\nto facilitate readers\u2019 understanding, we report a typical setup based on identical features but only utilizing major published\nresults including DIN (Zhou et al., 2018), DCN (Wang et al., 2021), and MMoE (Ma et al., 2018) (\u201cDLRM (DIN+DCN)\u201d)\nin Table 7, with the combined architecture illustrated in Figure 9. This setup significantly underperformed our production\nDLRM setup by 0.71% in NE for the main E-Task and 0.57% in NE for the main C-Task (where 0.1% NE is significant).\nRetrieval Setting. The baseline retrieval model employs a standard two-tower neutral retrieval setting (Covington et al.,\n2016) with mixed in-batch and out-of-batch sampling. The input feature set consists of both high cardinality sparse features\n(e.g., item ids, user ids) and low cardinality sparse features (e.g. languages, topics, interest entities). A stack of feed forward\nlayers with residual connections (He et al., 2015) is used to compress the input features into user and item embeddings.\nFeatures and Sequence Length. The features used in both of the DLRM baselines, including main user interaction history\nthat is utilized by various sequential encoder/pairwise attention modules, are strict supersets of the features used in all GR\ncandidates. This applies to all studies conducted in this paper, including those used in the scaling studies (Section 4.3.1).\n20",
          "translated": "[\ucc38\uace0\ubb38\ud5cc \uc0dd\ub7b5]"
        }
      ]
    },
    {
      "id": "efb35aed-82e1-4303-a8d7-03afd89ef23f",
      "title": "LightKG: Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture",
      "authors": [
        "Yanhui Li",
        "Dongxia Wang",
        "Zhu Sun",
        "Haonan Zhang",
        "Huizhong Guo"
      ],
      "abstract": "Recently, Graph Neural Networks (GNNs) have become the dominant approach for Knowledge Graph-aware Recommender Systems (KGRSs) due to their proven effectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL) has been incorporated to address the sparity issue, leading to longer training time. However, through extensive experiments, we reveal that: (1)compared to other KGRSs, the existing GNN-based KGRSs fail to keep their superior performance under sparse interactions even with SSL. (2) More complex models tend to perform worse in sparse interaction scenarios and complex mechanisms, like attention mechanism, can be detrimental as they often increase learning difficulty. Inspired by these findings, we propose LightKG, a simple yet powerful GNN-based KGRS to address sparsity issues. LightKG includes a simplified GNN layer that encodes directed relations as scalar pairs rather than dense embeddings and employs a linear aggregation framework, greatly reducing the complexity of GNNs. Additionally, LightKG incorporates an efficient contrastive layer to implement SSL. It directly minimizes the node similarity in original graph, avoiding the time-consuming subgraph generation and comparison required in previous SSL methods. Experiments on four benchmark datasets show that LightKG outperforms 12 competitive KGRSs in both sparse and dense scenarios while significantly reducing training time. Specifically, it surpasses the best baselines by an average of 5.8% in recommendation accuracy and saves 84.3% of training time compared to KGRSs with SSL. Our code is available at https://github.com/1371149/LightKG.",
      "year": 2025,
      "arxiv_id": "2506.10347",
      "arxiv_url": "https://arxiv.org/abs/2506.10347",
      "doi": "10.1145/3711896.3737026",
      "paper_url": "https://dl.acm.org/doi/10.1145/3711896.3737026",
      "conference": "KDD'25",
      "category": "recsys",
      "tags": [
        {
          "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
          "name": "GCN"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-06-12",
      "created_at": "2026-01-29T18:45:57.419063",
      "updated_at": "2026-02-04T16:35:33.432750",
      "summary": {
        "one_line": "LightKG presents a simplified GNN-based Knowledge Graph Recommender System (KGRS) that achieves superior performance and efficiency in sparse interaction scenarios.",
        "contribution": "This paper demonstrates that complex GNN-based KGRSs fail to maintain performance under sparsity, highlighting the detrimental effects of intricate mechanisms like attention. LightKG introduces a simplified GNN layer and an efficient contrastive learning approach to directly minimize node similarity, addressing the limitations of existing methods.",
        "methodology": "LightKG utilizes a simplified GNN layer to encode directed relations as scalar pairs, reducing computational complexity. It employs a linear aggregation framework and incorporates a contrastive layer for self-supervised learning, directly minimizing node similarity within the original graph.",
        "results": "Experiments on benchmark datasets reveal that LightKG outperforms 12 competitive KGRSs in both sparse and dense scenarios, achieving a 5.8% average improvement in recommendation accuracy and reducing training time by 84.3% compared to SSL-based KGRSs."
      }
    },
    {
      "id": "ac2c1807-b63d-4510-af12-30d3d58d04e3",
      "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
      "authors": [
        "Xiangnan He",
        "Kuan Deng",
        "Xiang Wang",
        "Yan Li",
        "Yongdong Zhang",
        "Meng Wang"
      ],
      "abstract": "Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.   In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
      "year": 2020,
      "arxiv_id": "2002.02126",
      "arxiv_url": "https://arxiv.org/abs/2002.02126",
      "conference": "SIGIR'20",
      "category": "recsys",
      "tags": [
        {
          "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
          "name": "GCN"
        }
      ],
      "published_at": "2020-02-06",
      "created_at": "2026-01-30T11:16:47.921484",
      "updated_at": "2026-01-30T15:07:15.534327",
      "summary": {
        "one_line": "LightGCN simplifies GCN for recommendation by removing unnecessary components, achieving significant performance improvements through linear neighborhood aggregation.",
        "contribution": "This paper demonstrates that feature transformation and nonlinear activations in GCNs contribute minimally to collaborative filtering performance. LightGCN's core innovation is a linear neighborhood aggregation approach, resulting in a more concise and efficient model.",
        "methodology": "LightGCN learns user and item embeddings via linear propagation on the user-item graph, employing a weighted sum of layer embeddings as the final representation. This eliminates complex neural network operations, focusing solely on neighborhood aggregation for improved efficiency.",
        "results": "LightGCN achieves an average relative improvement of 16.0% over NGCF under identical experimental conditions, highlighting the effectiveness of its simplified design and linear propagation strategy."
      },
      "translation": {
        "title": "\uc81c\ubaa9: LightGCN: \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \ub124\ud2b8\uc6cc\ud06c\ub97c \ub2e8\uc21c\ud654\ud558\uace0 \ucd94\ucc9c\uc744 \uc704\ud55c \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95",
        "abstract": "\ucd08\ub85d: \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \ub124\ud2b8\uc6cc\ud06c(GCN)\ub294 \ud611\uc5c5 \ud544\ud130\ub9c1 \ubd84\uc57c\uc5d0\uc11c \uc0c8\ub85c\uc6b4 \ucd5c\ucca8\ub2e8 \uae30\uc220\ub85c \uc790\ub9ac \uc7a1\uc558\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ucd94\ucc9c\uc744 \uc704\ud55c \uadf8 \ud6a8\uacfc\uc758 \uc774\uc720\ub294 \ucda9\ubd84\ud788 \uc774\ud574\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \uae30\uc874\uc758 GCN\uc744 \ucd94\ucc9c\uc5d0 \uc801\uc6a9\ud55c \uc5f0\uad6c\ub4e4\uc740 GCN\uc758 \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc694\uc778\uc744 \uba85\ud655\ud558\uac8c \ubd84\uc11d\ud558\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4. GCN\uc740 \uc6d0\ub798 \uadf8\ub798\ud504 \ubd84\ub958 \uc791\uc5c5\uc5d0 \uc124\uacc4\ub418\uc5c8\uc73c\uba70, \ub9ce\uc740 \uc2e0\uacbd\ub9dd \uc5f0\uc0b0\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc6b0\ub9ac\ub294 GCN\uc758 \uac00\uc7a5 \ud754\ud55c \ub450 \uac00\uc9c0 \uc124\uacc4 \u2013 \ud2b9\uc9d5 \ubcc0\ud658 \ubc0f \ube44\uc120\ud615 \ud65c\uc131\ud654 \u2013 \uac00 \ud611\uc5c5 \ud544\ud130\ub9c1\uc758 \uc131\ub2a5\uc5d0 \ud06c\uac8c \uae30\uc5ec\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uacbd\ud5d8\uc801\uc73c\ub85c \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4. \ub354 \ub098\uc544\uac00 \uc774\ub7ec\ud55c \uc694\uc18c\ub97c \ud3ec\ud568\ud558\uba74 \ud559\uc2b5\uc758 \uc5b4\ub824\uc6c0\uc774 \uc99d\uac00\ud558\uace0 \ucd94\ucc9c \uc131\ub2a5\uc774 \uc800\ud558\ub429\ub2c8\ub2e4. \ubcf8 \uc5f0\uad6c\uc5d0\uc11c\ub294 GCN\uc758 \uc124\uacc4\ub97c \ub2e8\uc21c\ud654\ud558\uc5ec \ub354\uc6b1 \uac04\uacb0\ud558\uace0 \ucd94\ucc9c\uc5d0 \uc801\ud569\ud558\ub3c4\ub85d \ub9cc\ub4dc\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ud611\uc5c5 \ud544\ud130\ub9c1\uc744 \uc704\ud574 GCN\uc758 \uac00\uc7a5 \ud544\uc218\uc801\uc778 \uad6c\uc131 \uc694\uc18c\uc778 \u2018\uc774\uc6c3 \uc9d1\uacc4(neighborhood aggregation)\u2019\ub9cc\uc744 \ud3ec\ud568\ud558\ub294 \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc778 LightGCN\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, LightGCN\uc740 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638 \uc791\uc6a9 \uadf8\ub798\ud504\uc5d0\uc11c \uc120\ud615\uc801\uc73c\ub85c \uc0ac\uc6a9\uc790 \ubc0f \uc544\uc774\ud15c \uc784\ubca0\ub529\uc744 \ud559\uc2b5\ud558\uace0, \ubaa8\ub4e0 \ub808\uc774\uc5b4\uc5d0\uc11c \ud559\uc2b5\ub41c \uc784\ubca0\ub529\uc758 \uac00\uc911 \ud569\uc744 \ucd5c\uc885 \uc784\ubca0\ub529\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ub2e8\uc21c\ud558\uace0 \uc120\ud615\uc801\uc774\uba70 \uae54\ub054\ud55c \ubaa8\ub378\uc740 Neural Graph Collaborative Filtering (NGCF) \u2013 \ucd5c\ucca8\ub2e8 GCN \uae30\ubc18 \ucd94\ucc9c \ubaa8\ub378 \u2013 \ubcf4\ub2e4 \uad6c\ud604 \ubc0f \ud559\uc2b5\uc774 \ud6e8\uc52c \uc26c\uc6b0\uba70, \uc815\ud655\ud788 \ub3d9\uc77c\ud55c \uc2e4\ud5d8 \uc124\uc815 \ud558\uc5d0\uc11c \ud3c9\uade0\uc801\uc73c\ub85c \uc57d 16.0%\uc758 \uc0c1\ub300\uc801 \uac1c\uc120\uc744 \ubcf4\uc785\ub2c8\ub2e4. LightGCN\uc758 \ub2e8\uc21c\uc131\uc5d0 \ub300\ud55c \ubd84\uc11d\uc801 \ubc0f \uacbd\ud5d8\uc801 \uad00\uc810\uc5d0\uc11c \ud569\ub9ac\uc131\uc744 \ub4b7\ubc1b\uce68\ud558\ub294 \ucd94\uac00 \ubd84\uc11d\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4."
      },
      "full_translation": [
        {
          "name": "Abstract",
          "original": "LightGCN: Simplifying and Powering Graph Convolution\nNetwork for Recommendation\nXiangnan He\nUniversity of Science and Technology\nof China\nxiangnanhe@gmail.com\nKuan Deng\nUniversity of Science and Technology\nof China\ndengkuan@mail.ustc.edu.cn\nXiang Wang\nNational University of Singapore\nxiangwang@u.nus.edu\nYan Li\nBeijing Kuaishou Technology\nCo., Ltd.\nliyan@kuaishou.com\nYongdong Zhang\nUniversity of Science and Technology\nof China\nzhyd73@ustc.edu.cn\nMeng Wang\u2217\nHefei University of Technology\neric.mengwang@gmail.com",
          "translated": "**\uc694\uc57d**\n\nLightGCN: \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \ub124\ud2b8\uc6cc\ud06c\ub97c \ub2e8\uc21c\ud654\ud558\uace0 \uac15\ud654\ud558\uc5ec \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc744 \uad6c\ud604\ud55c\ub2e4.\n\nXiangnan He\nUniversity of Science and Technology of China\nxiangnanhe@gmail.com\n\nKuan Deng\nUniversity of Science and Technology of China\ndengkuan@mail.ustc.edu.cn\n\nXiang Wang\nNational University of Singapore\nxiangwang@u.nus.edu\n\nYan Li\nBeijing Kuaishou Technology Co., Ltd.\nliyan@kuaishou.com\n\nYongdong Zhang\nUniversity of Science and Technology of China\nzhyd73@ustc.edu.cn\n\nMeng Wang\u2217\nHefei University of Technology\neric.mengwang@gmail.com"
        },
        {
          "name": "Abstract",
          "original": "Graph Convolution Network (GCN) has become new state-of-\nthe-art for collaborative filtering. Nevertheless, the reasons of\nits effectiveness for recommendation are not well understood.\nExisting work that adapts GCN to recommendation lacks thorough\nablation analyses on GCN, which is originally designed for graph\nclassification tasks and equipped with many neural network\noperations. However, we empirically find that the two most\ncommon designs in GCNs \u2014 feature transformation and nonlinear\nactivation \u2014 contribute little to the performance of collaborative\nfiltering. Even worse, including them adds to the difficulty of\ntraining and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to\nmake it more concise and appropriate for recommendation. We\npropose a new model named LightGCN, including only the most\nessential component in GCN \u2014 neighborhood aggregation \u2014 for\ncollaborative filtering. Specifically, LightGCN learns user and\nitem embeddings by linearly propagating them on the user-item\ninteraction graph, and uses the weighted sum of the embeddings\nlearned at all layers as the final embedding. Such simple, linear,\nand neat model is much easier to implement and train, exhibiting\nsubstantial improvements (about 16.0% relative improvement on\naverage) over Neural Graph Collaborative Filtering (NGCF) \u2014 a\nstate-of-the-art GCN-based recommender model \u2014 under exactly\nthe same experimental setting. Further analyses are provided\ntowards the rationality of the simple LightGCN from both analytical\nand empirical perspectives. Our implementations are available in\nboth TensorFlow1 and PyTorch2.\n\u2217Meng Wang is the corresponding author.\n1https://github.com/kuandeng/LightGCN\n2https://github.com/gusye1234/pytorch-light-gcn\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR \u201920, July 25\u201330, 2020, Virtual Event, China\n\u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401063\nCCS CONCEPTS\n\u2022 Information systems \u2192 Recommender systems.\nKEYWORDS\nCollaborative Filtering, Recommendation, Embedding Propagation,\nGraph Neural Network\nACM Reference Format:\nXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution\nNetwork for Recommendation. In Proceedings of the 43rd International ACM\nSIGIR Conference on Research and Development in Information Retrieval\n(SIGIR \u201920), July 25\u201330, 2020, Virtual Event, China. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/3397271.3401063",
          "translated": "\ub124\ud2b8\uc6cc\ud06c \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 (GCN)\uc740 \ud611\uc5c5 \ud544\ud130\ub9c1 \ubd84\uc57c\uc5d0\uc11c \uc0c8\ub85c\uc6b4 \ucd5c\ucca8\ub2e8 \uae30\uc220\ub85c \uc790\ub9ac \uc7a1\uc558\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc5d0\uc11c\uc758 \ud6a8\uacfc\uc801\uc778 \uc131\ub2a5\uc5d0 \ub300\ud55c \uba85\ud655\ud55c \uc124\uba85\uc774 \ubd80\uc871\ud569\ub2c8\ub2e4. \uae30\uc874\uc758 GCN\uc744 \ucd94\ucc9c\uc5d0 \uc801\uc6a9\ud55c \uc5f0\uad6c\ub4e4\uc740 GCN\uc758 \uc6d0\ub798 \ubaa9\uc801\uc774\uc5c8\ub358 \uadf8\ub798\ud504 \ubd84\ub958 \uc791\uc5c5\uc5d0 \uc801\ud569\ud55c \ub9ce\uc740 \uc2e0\uacbd\ub9dd \uc5f0\uc0b0\uc744 \uac16\ucd98 \uac83\uc5d0 \ub300\ud55c \ucda9\ubd84\ud55c \uc2e4\ud5d8\uc801 \ubd84\uc11d\uc744 \uc218\ud589\ud558\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc6b0\ub9ac\ub294 GCN\uc5d0\uc11c \uac00\uc7a5 \ud754\ud55c \ub450 \uac00\uc9c0 \uc124\uacc4, \uc989 \ud2b9\uc9d5 \ubcc0\ud658 \ubc0f \ube44\uc120\ud615 \ud65c\uc131 \ud568\uc218\uac00 \ud611\uc5c5 \ud544\ud130\ub9c1 \uc131\ub2a5\uc5d0 \uac70\uc758 \uae30\uc5ec\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uacbd\ud5d8\uc801\uc73c\ub85c \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4. \ub354\uc6b1\uc774 \uc774\ub7ec\ud55c \uc694\uc18c\ub97c \ud3ec\ud568\ud558\uba74 \ud559\uc2b5\uc758 \uc5b4\ub824\uc6c0\uc774 \uc99d\uac00\ud558\uace0 \ucd94\ucc9c \uc131\ub2a5\uc774 \uc800\ud558\ub429\ub2c8\ub2e4.\n\n\ubcf8 \uc5f0\uad6c\uc5d0\uc11c\ub294 GCN\uc758 \uc124\uacc4\ub97c \ub2e8\uc21c\ud654\ud558\uc5ec \ub354\uc6b1 \uac04\uacb0\ud558\uace0 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc5d0 \uc801\ud569\ud558\ub3c4\ub85d \ub9cc\ub4dc\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \ud611\uc5c5 \ud544\ud130\ub9c1\uc744 \uc704\ud574 GCN\uc758 \uac00\uc7a5 \ud544\uc218\uc801\uc778 \uad6c\uc131 \uc694\uc18c\uc778 \uc774\uc6c3 \uc9d1\uacc4\ub9cc\uc744 \ud3ec\ud568\ud558\ub294 \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc778 LightGCN\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, LightGCN\uc740 \uc0ac\uc6a9\uc790 \ubc0f \uc544\uc774\ud15c \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638 \uc791\uc6a9 \uadf8\ub798\ud504\uc5d0\uc11c \uc120\ud615\uc801\uc73c\ub85c \uc804\ud30c\ud558\uc5ec \ud559\uc2b5\ud558\uace0, \ubaa8\ub4e0 \ub808\uc774\uc5b4\uc5d0\uc11c \ud559\uc2b5\ub41c \uc784\ubca0\ub529\uc758 \uac00\uc911 \ud569\uc744 \ucd5c\uc885 \uc784\ubca0\ub529\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ub2e8\uc21c\ud558\uace0 \uc120\ud615\uc801\uc774\uba70 \uae54\ub054\ud55c \ubaa8\ub378\uc740 \uad6c\ud604 \ubc0f \ud559\uc2b5\uc774 \ud6e8\uc52c \uc26c\uc6b0\uba70, \uc2e0\uacbd \uadf8\ub798\ud504 \ud611\uc5c5 \ud544\ud130\ub9c1 (NGCF) \u2013 \ucd5c\ucca8\ub2e8 GCN \uae30\ubc18 \ucd94\ucc9c \ubaa8\ub378 \u2013 \uacfc \ube44\uad50\ud558\uc5ec \ub3d9\uc77c\ud55c \uc2e4\ud5d8 \uc124\uc815 \ud558\uc5d0\uc11c \ud3c9\uade0\uc801\uc73c\ub85c \uc57d 16.0%\uc758 \uc0c1\ub300\uc801 \uac1c\uc120\uc744 \ubcf4\uc785\ub2c8\ub2e4. LightGCN\uc758 \ub2e8\uc21c\uc131\uc5d0 \ub300\ud55c \ubd84\uc11d\uc801 \ubc0f \uacbd\ud5d8\uc801 \uad00\uc810 \ubaa8\ub450\uc5d0\uc11c \ucd94\uac00\uc801\uc778 \ubd84\uc11d\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4. TensorFlow1 \ubc0f PyTorch2\uc5d0\uc11c \uad6c\ud604\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4.\n\n*\uba4d \uc655(Meng Wang)\uc774 \ud574\ub2f9 \uc800\uc790\uc785\ub2c8\ub2e4.\n1https://github.com/kuandeng/LightGCN\n2https://github.com/gusye1234/pytorch-light-gcn\nSIGIR \u201920, 7\uc6d4 25\uc77c-30\uc77c, 2020, \uac00\uc0c1 \ud589\uc0ac, \uc911\uad6d\n\u00a9 2020 \uc800\uc791\uad8c \uc18c\uc720\uc790/\uc800\uc790. ACM \ucd9c\ud310\uad8c \ub77c\uc774\uc120\uc2a4. ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401063\n\nCCS CONCEPTS\n\u2022 \uc815\ubcf4 \uc2dc\uc2a4\ud15c \u2192 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\n\ud0a4\uc6cc\ub4dc\n\ud611\uc5c5 \ud544\ud130\ub9c1, \ucd94\ucc9c, \uc784\ubca0\ub529 \uc804\ud30c, \uadf8\ub798\ud504 \uc2e0\uacbd\ub9dd"
        },
        {
          "name": "Introduction",
          "original": "To alleviate information overload on the web, recommender system\nhas been widely deployed to perform personalized information\nfiltering [7, 45, 46]. The core of recommender system is to predict\nwhether a user will interact with an item, e.g., click, rate, purchase,\namong other forms of interactions. As such, collaborative filtering\n(CF), which focuses on exploiting the past user-item interactions to\nachieve the prediction, remains to be a fundamental task towards\neffective personalized recommendation [10, 19, 28, 39].\nThe most common paradigm for CF is to learn latent features\n(a.k.a. embedding) to represent a user and an item, and perform\nprediction based on the embedding vectors [ 6, 19]. Matrix\nfactorization is an early such model, which directly projects the\nsingle ID of a user to her embedding [26]. Later on, several research\nfind that augmenting user ID with the her interaction history as\nthe input can improve the quality of embedding. For example,\nSVD++ [25] demonstrates the benefits of user interaction history\nin predicting user numerical ratings, and Neural Attentive Item\nSimilarity (NAIS) [ 18] differentiates the importance of items in\nthe interaction history and shows improvements in predicting\nitem ranking. In view of user-item interaction graph, these\nimprovements can be seen as coming from using the subgraph\nstructure of a user \u2014 more specifically, her one-hop neighbors \u2014 to\nimprove the embedding learning.\nTo deepen the use of subgraph structure with high-hop\nneighbors, Wang et al. [39] recently proposes NGCF and achieves\nstate-of-the-art performance for CF. It takes inspiration from the\nGraph Convolution Network (GCN) [14, 23], following the same\narXiv:2002.02126v4  [cs.IR]  7 Jul 2020\n\npropagation rule to refine embeddings: feature transformation,\nneighborhood aggregation, and nonlinear activation. Although\nNGCF has shown promising results, we argue that its designs\nare rather heavy and burdensome \u2014 many operations are directly\ninherited from GCN without justification. As a result, they are not\nnecessarily useful for the CF task. To be specific, GCN is originally\nproposed for node classification on attributed graph, where each\nnode has rich attributes as input features; whereas in user-item\ninteraction graph for CF, each node (user or item) is only described\nby a one-hot ID, which has no concrete semantics besides being\nan identifier. In such a case, given the ID embedding as the input,\nperforming multiple layers of nonlinear feature transformation \u2014\nwhich is the key to the success of modern neural networks [ 16]\n\u2014 will bring no benefits, but negatively increases the difficulty for\nmodel training.\nTo validate our thoughts, we perform extensive ablation studies\non NGCF. With rigorous controlled experiments (on the same data\nsplits and evaluation protocol), we draw the conclusion that the\ntwo operations inherited from GCN \u2014 feature transformation and\nnonlinear activation \u2014 has no contribution on NGCF\u2019s effectiveness.\nEven more surprising, removing them leads to significant accuracy\nimprovements. This reflects the issues of adding operations that\nare useless for the target task in graph neural network, which not\nonly brings no benefits, but rather degrades model effectiveness.\nMotivated by these empirical findings, we present a new model\nnamed LightGCN, including the most essential component of\nGCN \u2014 neighborhood aggregation \u2014 for collaborative filtering.\nSpecifically, after associating each user (item) with an ID embedding,\nwe propagate the embeddings on the user-item interaction graph\nto refine them. We then combine the embeddings learned at\ndifferent propagation layers with a weighted sum to obtain the final\nembedding for prediction. The whole model is simple and elegant,\nwhich not only is easier to train, but also achieves better empirical\nperformance than NGCF and other state-of-the-art methods like\nMult-VAE [28].\nTo summarize, this work makes the following main contributions:\n\u2022 We empirically show that two common designs in GCN,\nfeature transformation and nonlinear activation, have no\npositive effect on the effectiveness of collaborative filtering.\n\u2022 We propose LightGCN, which largely simplifies the model\ndesign by including only the most essential components in\nGCN for recommendation.\n\u2022 We empirically compare LightGCN with NGCF by following\nthe same setting and demonstrate substantial improvements.\nIn-depth analyses are provided towards the rationality of\nLightGCN from both technical and empirical perspectives.\n2 PRELIMINARIES\nWe first introduce NGCF [39], a representative and state-of-the-art\nGCN model for recommendation. We then perform ablation studies\non NGCF to judge the usefulness of each operation in NGCF. The\nnovel contribution of this section is to show that the two common\ndesigns in GCNs, feature transformation and nonlinear activation,\nhave no positive effect on collaborative filtering.\nTable 1: Performance of NGCF and its three variants.\nGowalla Amazon-Book\nrecall ndcg recall ndcg\nNGCF 0.1547 0.1307 0.0330 0.0254\nNGCF-f 0.1686 0.1439 0.0368 0.0283\nNGCF-n 0.1536 0.1295 0.0336 0.0258\nNGCF-fn 0.1742 0.1476 0.0399 0.0303\n2.1 NGCF Brief\nIn the initial step, each user and item is associated with an ID\nembedding. Let e(0)\nu denote the ID embedding of user u and e(0)\ni\ndenote the ID embedding of item i. Then NGCF leverages the user-\nitem interaction graph to propagate embeddings as:\ne(k+1)\nu =\u03c3\n\u0010\nW1e(k)\nu +\nX\ni \u2208Nu\n1p\n|Nu ||Ni |\n(W1e(k)\ni + W2(e(k)\ni \u2299 e(k)\nu ))\n\u0011\n,\ne(k+1)\ni =\u03c3\n\u0010\nW1e(k)\ni +\nX\nu \u2208Ni\n1p\n|Nu ||Ni |\n(W1e(k)\nu + W2(e(k)\nu \u2299 e(k)\ni ))\n\u0011\n,\n(1)\nwhere e(k)\nu and e(k)\ni respectively denote the refined embedding of\nuser u and item i after k layers propagation, \u03c3is the nonlinear\nactivation function, Nu denotes the set of items that are interacted\nby user u, Ni denotes the set of users that interact with item i,\nand W1 and W2 are trainable weight matrix to perform feature\ntransformation in each layer. By propagatingL layers, NGCF obtains\nL + 1 embeddings to describe a user (e(0)\nu , e(1)\nu , ..., e(L)\nu ) and an item\n(e(0)\ni , e(1)\ni , ..., e(L)\ni ). It then concatenates these L + 1 embeddings to\nobtain the final user embedding and item embedding, using inner\nproduct to generate the prediction score.\nNGCF largely follows the standard GCN [23], including the use\nof nonlinear activation function \u03c3(\u00b7) and feature transformation\nmatrices W1 and W2. However, we argue that the two operations\nare not as useful for collaborative filtering. In semi-supervised\nnode classification, each node has rich semantic features as input,\nsuch as the title and abstract words of a paper. Thus performing\nmultiple layers of nonlinear transformation is beneficial to feature\nlearning. Nevertheless, in collaborative filtering, each node of user-\nitem interaction graph only has an ID as input which has no\nconcrete semantics. In this case, performing multiple nonlinear\ntransformations will not contribute to learn better features; even\nworse, it may add the difficulties to train well. In the next subsection,\nwe provide empirical evidence on this argument.\n2.2 Empirical Explorations on NGCF\nWe conduct ablation studies on NGCF to explore the effect of\nnonlinear activation and feature transformation. We use the codes\nreleased by the authors of NGCF 3, running experiments on the\nsame data splits and evaluation protocol to keep the comparison as\nfair as possible. Since the core of GCN is to refine embeddings by\npropagation, we are more interested in the embedding quality under\nthe same embedding size. Thus, we change the way of obtaining\nfinal embedding from concatenation (i.e., e\u2217u = e(0)\nu \u2225\u00b7 \u00b7 \u00b7 \u2225 e(L)\nu ) to\nsum (i.e., e\u2217u = e(0)\nu + \u00b7 \u00b7 \u00b7+ e(L)\nu ). Note that this change has little effect\n3https://github.com/xiangwang1223/neural_graph_collaborative_filtering\n\n0 100 200 300 400 500\nEpoch\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030trainning loss\nGowalla\n(a) Training loss on Gowalla\n0 100 200 300 400 500\nEpoch\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18recall@20\nGowalla\n (b) Testing recall on Gowalla\n0 25 50 75 100 125 150 175\nEpoch\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030trainning loss\nAmazon-Book\n (c) Training loss on Amazon-Book\n0 25 50 75 100 125 150 175\nEpoch\n0.0200\n0.0225\n0.0250\n0.0275\n0.0300\n0.0325\n0.0350\n0.0375recall@20\nAmazon-Book\n (d) Testing recall on Amazon-Book\nFigure 1: Training curves (training loss and testing recall) of NGCF and its three simplified variants.\non NGCF\u2019s performance, but makes the following ablation studies\nmore indicative of the embedding quality refined by GCN.\nWe implement three simplified variants of NGCF:\n\u2022 NGCF-f, which removes the feature transformation matrices W1\nand W2.\n\u2022 NGCF-n, which removes the non-linear activation function\u03c3.\n\u2022 NGCF-fn, which removes both the feature transformation\nmatrices and non-linear activation function.\nFor the three variants, we keep all hyper-parameters (e.g.,\nlearning rate, regularization coefficient, dropout ratio, etc.) same as\nthe optimal settings of NGCF. We report the results of the 2-layer\nsetting on the Gowalla and Amazon-Book datasets in Table 1. As\ncan be seen, removing feature transformation (i.e., NGCF-f) leads\nto consistent improvements over NGCF on all three datasets. In\ncontrast, removing nonlinear activation does not affect the accuracy\nthat much. However, if we remove nonlinear activation on the basis\nof removing feature transformation (i.e., NGCF-fn), the performance\nis improved significantly. From these observations, we conclude\nthe findings that:\n(1) Adding feature transformation imposes negative effect on\nNGCF, since removing it in both models of NGCF and NGCF-n\nimproves the performance significantly;\n(2) Adding nonlinear activation affects slightly when feature\ntransformation is included, but it imposes negative effect when\nfeature transformation is disabled.\n(3) As a whole, feature transformation and nonlinear activation\nimpose rather negative effect on NGCF, since by removing them\nsimultaneously, NGCF-fn demonstrates large improvements over\nNGCF (9.57% relative improvement on recall).\nTo gain more insights into the scores obtained in Table 1 and\nunderstand why NGCF deteriorates with the two operations, we\nplot the curves of model status recorded by training loss and testing\nrecall in Figure 1. As can be seen, NGCF-fn achieves a much lower\ntraining loss than NGCF, NGCF-f, and NGCF-n along the whole\ntraining process. Aligning with the curves of testing recall, we\nfind that such lower training loss successfully transfers to better\nrecommendation accuracy. The comparison between NGCF and\nNGCF-f shows the similar trend, except that the improvement\nmargin is smaller.\nFrom these evidences, we can draw the conclusion that\nthe deterioration of NGCF stems from the training difficulty,\nrather than overfitting. Theoretically speaking, NGCF has higher\nrepresentation power than NGCF-f, since setting the weight\nmatrix W1 and W2 to identity matrix I can fully recover\nthe NGCF-f model. However, in practice, NGCF demonstrates\nhigher training loss and worse generalization performance than\nNGCF-f. And the incorporation of nonlinear activation further\naggravates the discrepancy between representation power and\ngeneralization performance. To round out this section, we claim\nthat when designing model for recommendation, it is important\nto perform rigorous ablation studies to be clear about the impact\nof each operation. Otherwise, including less useful operations will\ncomplicate the model unnecessarily, increase the training difficulty,\nand even degrade model effectiveness.",
          "translated": "To alleviate information overload on the web, recommender system has been widely deployed to perform personalized information filtering [7, 45, 46]. The core of recommender system is to predict whether a user will interact with an item, e.g., click, rate, purchase, among other forms of interactions. As such, collaborative filtering (CF), which focuses on exploiting the past user-item interactions to achieve the prediction, remains to be a fundamental task towards effective personalized recommendation [10, 19, 28, 39].\nThe most common paradigm for CF is to learn latent features (a.k.a. embedding) to represent a user and an item, and perform prediction based on the embedding vectors [ 6, 19]. Matrix factorization is an early such model, which directly projects the single ID of a user to her embedding [26]. Later on, several research find that augmenting user ID with the her interaction history as the input can improve the quality of embedding. For example, SVD++ [25] demonstrates the benefits of user interaction history in predicting user numerical ratings, and Neural Attentive Item Similarity (NAIS) [ 18] differentiates the importance of items in the interaction history and shows improvements in predicting item ranking. In view of user-item interaction graph, these improvements can be seen as coming from using the subgraph structure of a user \u2014 more specifically, her one-hop neighbors \u2014 to improve the embedding learning.\nTo deepen the use of subgraph structure with high-hop neighbors, Wang et al. [39] recently proposes NGCF and achieves state-of-the-art performance for CF. It takes inspiration from the Graph Convolution Network (GCN) [14, 23], following the same propagation rule to refine embeddings: feature transformation, neighborhood aggregation, and nonlinear activation. Although NGCF has shown promising results, we argue that its designs are rather heavy and burdensome \u2014 many operations are directly inherited from GCN without justification. As a result, they are not necessarily useful for the CF task. To be specific, GCN is originally proposed for node classification on attributed graph, where each node has rich attributes as input features; whereas in user-item interaction graph for CF, each node (user or item) is only described by a one-hot ID, which has no concrete semantics besides being an identifier. In such a case, given the ID embedding as the input, performing multiple layers of nonlinear feature transformation \u2014 which is the key to the success of modern neural networks [ 16] \u2014 will bring no benefits, but negatively increases the difficulty for model training.\nTo validate our thoughts, we perform extensive ablation studies on NGCF. With rigorous controlled experiments (on the same data splits and evaluation protocol), we draw the conclusion that the two operations inherited from GCN \u2014 feature transformation and nonlinear activation \u2014 has no contribution on NGCF\u2019s effectiveness. Even more surprising, removing them leads to significant accuracy improvements. This reflects the issues of adding operations that are useless for the target task in graph neural network, which not only brings no benefits, but rather degrades model effectiveness.\nMotivated by these empirical findings, we present a new model named LightGCN, including the most essential component of GCN \u2014 neighborhood aggregation \u2014 for collaborative filtering. Specifically, after associating each user (item) with an ID embedding, we propagate the embeddings on the user-item interaction graph to refine them. We then combine the embeddings learned at different propagation layers with a weighted sum to obtain the final embedding for prediction. The whole model is simple and elegant, which not only is easier to train, but also achieves better empirical performance than NGCF and other state-of-the-art methods like Mult-VAE [28].\nTo summarize, this work makes the following main contributions:\n\u2022 We empirically show that two common designs in GCN, feature transformation and nonlinear activation, have no positive effect on the effectiveness of collaborative filtering.\n\u2022 We propose LightGCN, which largely simplifies the model design by including only the most essential components in GCN for recommendation.\n\u2022 We empirically compare LightGCN with NGCF by following the same setting and demonstrate substantial improvements. In-depth analyses are provided towards the rationality of LightGCN from both technical and empirical perspectives.\n2 PRELIMINARIES\nWe first introduce NGCF [39], a representative and state-of-the-art GCN model for recommendation. We then perform ablation studies on NGCF to judge the usefulness of each operation in NGCF. The novel contribution of this section is to show that the two common designs in GCNs, feature transformation and nonlinear activation, have no positive effect on collaborative filtering.\nTable 1: Performance of NGCF and its three variants.\nGowalla Amazon-Book\nrecall ndcg recall\n\n[... \uae38\uc774 \uc81c\ud55c\uc73c\ub85c \uc0dd\ub7b5 ...]"
        },
        {
          "name": "Method",
          "original": "The former section demonstrates that NGCF is a heavy and\nburdensome GCN model for collaborative filtering. Driven by\nthese findings, we set the goal of developing a light yet effective\nmodel by including the most essential ingredients of GCN for\nrecommendation. The advantages of being simple are several-\nfold \u2014 more interpretable, practically easy to train and maintain,\ntechnically easy to analyze the model behavior and revise it towards\nmore effective directions, and so on.\nIn this section, we first present our designed Light Graph\nConvolution Network (LightGCN) model, as illustrated in Figure 2.\nWe then provide an in-depth analysis of LightGCN to show the\nrationality behind its simple design. Lastly, we describe how to do\nmodel training for recommendation.\n3.1 LightGCN\nThe basic idea of GCN is to learning representation for nodes by\nsmoothing features over the graph [ 23, 40]. To achieve this, it\nperforms graph convolution iteratively, i.e., aggregating the features\nof neighbors as the new representation of a target node. Such\nneighborhood aggregation can be abstracted as:\ne(k+1)\nu = AGG(e(k)\nu , {e(k)\ni : i \u2208 Nu }). (2)\nThe AGG is an aggregation function \u2014 the core of graph convolution\n\u2014 that considers the k-th layer\u2019s representation of the target node\nand its neighbor nodes. Many work have specified the AGG, such\nas the weighted sum aggregator in GIN [42], LSTM aggregator in\nGraphSAGE [14], and bilinear interaction aggregator in BGNN [48]\netc. However, most of the work ties feature transformation or\nnonlinear activation with the AGG function. Although they perform\nwell on node or graph classification tasks that have semantic input\nfeatures, they could be burdensome for collaborative filtering (see\npreliminary results in Section 2.2).\n\n\u0ada\n\u123a\u0adb\u123b\n\u0ada\n\u0add\n\u123a\u0ad9\u123b\n\u0ada\n\u123a\u0ada\u123b\n\u0ada\n\u123a\u0adc\u123b\n\u0add\n\u123a\u0ada\u123b\n\u0add\n\u123a\u0adb\u123b\n\u0add\n\u123a\u0adc\u123b\nLight Graph Convolution (LGC)\n\u0d4c\u0adaNormalized Sum\n\u0ada\u0adb\n\u0ada\n\u0adb\n\u0b3f\u0ada\u123b\n\u0ada\n\u0d4c\u0ada\n\u0adb\n\u0adc\n\u0b3f\u0ada\u123b\n\u0adb\n\u0adc\nLayer Combination (weighted sum)\n\u0ada\n\u0adb\n\u0adc\n\u0adb\u0adc\nNormalized Sum\n\u0adc\n\u0adc\n\u0b3f\u0ada\u123b\nPrediction\nneighbors of u1 neighbors of i4\nFigure 2: An illustration of LightGCN model architecture.\nIn LGC, only the normalized sum of neighbor embeddings\nis performed towards next layer; other operations like\nself-connection, feature transformation, and nonlinear\nactivation are all removed, which largely simplifies GCNs.\nIn Layer Combination, we sum over the embeddings at each\nlayer to obtain the final representations.\n3.1.1 Light Graph Convolution (LGC). In LightGCN, we adopt the\nsimple weighted sum aggregator and abandon the use of feature\ntransformation and nonlinear activation. The graph convolution\noperation (a.k.a., propagation rule [39]) in LightGCN is defined as:\ne(k+1)\nu =\nX\ni \u2208Nu\n1p\n|Nu |\np\n|Ni |\ne(k)\ni ,\ne(k+1)\ni =\nX\nu \u2208Ni\n1p\n|Ni |\np\n|Nu |\ne(k)\nu .\n(3)\nThe symmetric normalization term 1\u221a\n| Nu |\n\u221a\n| Ni |\nfollows the design\nof standard GCN [23], which can avoid the scale of embeddings\nincreasing with graph convolution operations; other choices can\nalso be applied here, such as the L1 norm, while empirically we\nfind this symmetric normalization has good performance (see\nexperiment results in Section 4.4.2).\nIt is worth noting that in LGC, we aggregate only the connected\nneighbors and do not integrate the target node itself (i.e., self-\nconnection). This is different from most existing graph convolution\noperations [14, 23, 36, 39, 48] that typically aggregate extended\nneighbors and need to handle the self-connection specially.\nThe layer combination operation, to be introduced in the next\nsubsection, essentially captures the same effect as self-connections.\nThus, there is no need in LGC to include self-connections.\n3.1.2 Layer Combination and Model Prediction. In LightGCN, the\nonly trainable model parameters are the embeddings at the 0-th\nlayer, i.e., e(0)\nu for all users and e(0)\ni for all items. When they are\ngiven, the embeddings at higher layers can be computed via LGC\ndefined in Equation (3). AfterK layers LGC, we further combine the\nembeddings obtained at each layer to form the final representation\nof a user (an item):\neu =\nKX\nk=0\n\u03b1k e(k)\nu ; ei =\nKX\nk=0\n\u03b1k e(k)\ni , (4)\nwhere\u03b1k \u2265 0 denotes the importance of the k-th layer embedding\nin constituting the final embedding. It can be treated as a hyper-\nparameter to be tuned manually, or as a model parameter (e.g.,\noutput of an attention network [3]) to be optimized automatically.\nIn our experiments, we find that setting\u03b1k uniformly as 1/(K + 1)\nleads to good performance in general. Thus we do not design\nspecial component to optimize\u03b1k , to avoid complicating LightGCN\nunnecessarily and to keep its simplicity. The reasons that we\nperform layer combination to get final representations are three-\nfold. (1) With the increasing of the number of layers, the embeddings\nwill be over-smoothed [ 27]. Thus simply using the last layer is\nproblematic. (2) The embeddings at different layers capture different\nsemantics. E.g., the first layer enforces smoothness on users and\nitems that have interactions, the second layer smooths users (items)\nthat have overlap on interacted items (users), and higher-layers\ncapture higher-order proximity [39]. Thus combining them will\nmake the representation more comprehensive. (3) Combining\nembeddings at different layers with weighted sum captures the\neffect of graph convolution with self-connections, an important\ntrick in GCNs (proof sees Section 3.2.1).\nThe model prediction is defined as the inner product of user and\nitem final representations:\n\u02c6yui = eT\nu ei , (5)\nwhich is used as the ranking score for recommendation generation.\n3.1.3 Matrix Form. We provide the matrix form of LightGCN to\nfacilitate implementation and discussion with existing models. Let\nthe user-item interaction matrix be R \u2208 RM \u00d7N where M and N\ndenote the number of users and items, respectively, and each entry\nRui is 1 if u has interacted with item i otherwise 0. We then obtain\nthe adjacency matrix of the user-item graph as\nA =\n\u0012 0 R\nRT 0\n\u0013\n, (6)\nLet the 0-th layer embedding matrix be E(0) \u2208 R(M+N )\u00d7T , where T\nis the embedding size. Then we can obtain the matrix equivalent\nform of LGC as:\nE(k+1) = (D\u2212 1\n2 AD\u2212 1\n2 )E(k), (7)\nwhere D is a (M + N ) \u00d7(M + N ) diagonal matrix, in which each entry\nDii denotes the number of nonzero entries in the i-th row vector\nof the adjacency matrix A (also named as degree matrix). Lastly,\nwe get the final embedding matrix used for model prediction as:\nE =\u03b10E(0) +\u03b11E(1) +\u03b12E(2) + ... +\u03b1K E(K)\n=\u03b10E(0) +\u03b11 \u02dcAE(0) +\u03b12 \u02dcA2E(0) + ... +\u03b1K \u02dcAK E(0),\n(8)\nwhere \u02dcA = D\u2212 1\n2 AD\u2212 1\n2 is the symmetrically normalized matrix.\n3.2 Model Analysis\nWe conduct model analysis to demonstrate the rationality behind\nthe simple design of LightGCN. First we discuss the connection\nwith the Simplified GCN (SGCN) [ 40], which is a recent linear\n\nGCN model that integrates self-connection into graph convolution;\nthis analysis shows that by doing layer combination, LightGCN\nsubsumes the effect of self-connection thus there is no need for\nLightGCN to add self-connection in adjacency matrix. Then we\ndiscuss the relation with the Approximate Personalized Propagation\nof Neural Predictions (APPNP) [24], which is recent GCN variant\nthat addresses oversmoothing by inspiring from Personalized\nPageRank [15]; this analysis shows the underlying equivalence\nbetween LightGCN and APPNP, thus our LightGCN enjoys\nthe sames benefits in propagating long-range with controllable\noversmoothing. Lastly we analyze the second-layer LGC to show\nhow it smooths a user with her second-order neighbors, providing\nmore insights into the working mechanism of LightGCN.\n3.2.1 Relation with SGCN. In [ 40], the authors argue the\nunnecessary complexity of GCN for node classfication and propose\nSGCN, which simplifies GCN by removing nonlinearities and\ncollapsing the weight matrices to one weight matrix. The graph\nconvolution in SGCN is defined as4:\nE(k+1) = (D + I)\u2212 1\n2 (A + I)(D + I)\u2212 1\n2 E(k), (9)\nwhere I \u2208 R(M+N )\u00d7(M+N ) is an identity matrix, which is added on\nA to include self-connections. In the following analysis, we omit the\n(D + I)\u2212 1\n2 terms for simplicity, since they only re-scale embeddings.\nIn SGCN, the embeddings obtained at the last layer are used for\ndownstream prediction task, which can be expressed as:\nE(K) = (A + I)E(K \u22121) = (A + I)K E(0)\n=\n\u0012K\n0\n\u0013\nE(0) +\n\u0012K\n1\n\u0013\nAE(0) +\n\u0012K\n2\n\u0013\nA2E(0) + ... +\n\u0012K\nK\n\u0013\nAK E(0).\n(10)\nThe above derivation shows that, inserting self-connection into A\nand propagating embeddings on it, is essentially equivalent to a\nweighted sum of the embeddings propagated at each LGC layer.\n3.2.2 Relation with APPNP. In a recent work [ 24], the authors\nconnect GCN with Personalized PageRank [ 15], inspiring from\nwhich they propose a GCN variant named APPNP that can\npropagate long range without the risk of oversmoothing. Inspired by\nthe teleport design in Personalized PageRank, APPNP complements\neach propagation layer with the starting features (i.e., the 0-th layer\nembeddings), which can balance the need of preserving locality\n(i.e., staying close to the root node to alleviate oversmoothing)\nand leveraging the information from a large neighborhood. The\npropagation layer in APPNP is defined as:\nE(k+1) =\u03b2E(0) + (1 \u2212\u03b2) \u02dcAE(k), (11)\nwhere \u03b2is the teleport probability to control the retaining of\nstarting features in the propagation, and \u02dcA denotes the normalized\nadjacency matrix. In APPNP, the last layer is used for final\nprediction, i.e.,\nE(K) =\u03b2E(0) + (1 \u2212\u03b2) \u02dcAE(K \u22121),\n=\u03b2E(0) +\u03b2(1 \u2212\u03b2) \u02dcAE(0) + (1 \u2212\u03b2)2 \u02dcA2E(K \u22122)\n=\u03b2E(0) +\u03b2(1 \u2212\u03b2) \u02dcAE(0) +\u03b2(1 \u2212\u03b2)2 \u02dcA2E(0) + ... + (1 \u2212\u03b2)K \u02dcAK E(0).\n(12)\n4The weight matrix in SGCN can be absorbed into the 0-th layer embedding parameters,\nthus it is omitted in the analysis.\nAligning with Equation (8), we can see that by setting \u03b1k\naccordingly, LightGCN can fully recover the prediction embedding\nused by APPNP. As such, LightGCN shares the strength of APPNP\nin combating oversmoothing \u2014 by setting the \u03b1properly, we\nallow using a large K for long-range modeling with controllable\noversmoothing.\nAnother minor difference is that APPNP adds self-connection\ninto the adjacency matrix. However, as we have shown before, this\nis redundant due to the weighted sum of different layers.\n3.2.3 Second-Order Embedding Smoothness. Owing to the linearity\nand simplicity of LightGCN, we can draw more insights into how\ndoes it smooth embeddings. Here we analyze a 2-layer LightGCN\nto demonstrate its rationality. Taking the user side as an example,\nintuitively, the second layer smooths users that have overlap on\nthe interacted items. More concretely, we have:\ne(2)\nu =\nX\ni \u2208Nu\n1p\n|Nu |\np\n|Ni |\ne(1)\ni =\nX\ni \u2208Nu\n1\n|Ni |\nX\nv \u2208Ni\n1p\n|Nu |\np\n|Nv |\ne(0)\nv .\n(13)\nWe can see that, if another userv has co-interacted with the target\nuser u, the smoothness strength of v on u is measured by the\ncoefficient (otherwise 0):\ncv\u2212>u = 1p\n|Nu |\np\n|Nv |\nX\ni \u2208Nu \u2229Nv\n1\n|Ni | . (14)\nThis coefficient is rather interpretable: the influence of a second-\norder neighbor v on u is determined by 1) the number of co-\ninteracted items, the more the larger; 2) the popularity of the\nco-interacted items, the less popularity (i.e., more indicative of\nuser personalized preference) the larger; and 3) the activity of v,\nthe less active the larger. Such interpretability well caters for the\nassumption of CF in measuring user similarity [2, 37] and evidences\nthe reasonability of LightGCN. Due to the symmetric formulation\nof LightGCN, we can get similar analysis on the item side.\n3.3 Model Training\nThe trainable parameters of LightGCN are only the embeddings of\nthe 0-th layer, i.e.,\u0398 = {E(0)}; in other words, the model complexity\nis same as the standard matrix factorization (MF). We employ the\nBayesian Personalized Ranking (BPR) loss [32], which is a pairwise\nloss that encourages the prediction of an observed entry to be\nhigher than its unobserved counterparts:\nLBP R = \u2212\nMX\nu=1\nX\ni \u2208Nu\nX\nj /\u2208Nu\nln\u03c3( \u02c6yui \u2212 \u02c6yu j) +\u03bb|| E(0)|| 2 (15)\nwhere\u03bbcontrols the L2 regularization strength. We employ the\nAdam [ 22] optimizer and use it in a mini-batch manner. We\nare aware of other advanced negative sampling strategies which\nmight improve the LightGCN training, such as the hard negative\nsampling [31] and adversarial sampling [9]. We leave this extension\nin the future since it is not the focus of this work.\nNote that we do not introduce dropout mechanisms, which are\ncommonly used in GCNs and NGCF. The reason is that we do not\nhave feature transformation weight matrices in LightGCN, thus\nenforcing L2 regularization on the embedding layer is sufficient\nto prevent overfitting. This showcases LightGCN\u2019s advantages of\nbeing simple \u2014 it is easier to train and tune than NGCF which\n\nTable 2: Statistics of the experimented data.\nDataset User # Item # Interaction # Density\nGowalla 29, 858 40, 981 1, 027, 370 0.00084\nYelp2018 31, 668 38, 048 1, 561, 406 0.00130\nAmazon-Book 52, 643 91, 599 2, 984, 108 0.00062\nadditionally requires to tune two dropout ratios (node dropout and\nmessage dropout) and normalize the embedding of each layer to\nunit length.\nMoreover, it is technically viable to also learn the layer\ncombination coefficients {\u03b1k }K\nk=0, or parameterize them with an\nattention network. However, we find that learning\u03b1on training\ndata does not lead improvement. This is probably because the\ntraining data does not contain sufficient signal to learn good\u03b1that\ncan generalize to unknown data. We have also tried to learn\u03b1from\nvalidation data, as inspired by [5] that learns hyper-parameters on\nvalidation data. The performance is slightly improved (less than1%).\nWe leave the exploration of optimal settings of\u03b1(e.g., personalizing\nit for different users and items) as future work.",
          "translated": "\ubcf8 \uc139\uc158\uc5d0\uc11c\ub294 NGCF\uac00 \ud611\uc5c5 \ud544\ud130\ub9c1\uc744 \uc704\ud55c \ubb34\uac81\uace0 \uc9d0\uc774 \ub418\ub294 GCN \ubaa8\ub378\uc784\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacb0\uacfc\uc5d0 \ub530\ub77c, \uc6b0\ub9ac\ub294 \ucd94\ucc9c\uc744 \uc704\ud574 \uac00\uc7a5 \uc911\uc694\ud55c GCN\uc758 \uc131\ubd84\uc744 \ud3ec\ud568\ud558\uc5ec \uac00\ubccd\uc9c0\ub9cc \ud6a8\uacfc\uc801\uc778 \ubaa8\ub378\uc744 \uac1c\ubc1c\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \ub2e8\uc21c\ud568\uc758 \uc7a5\uc810\uc740 \uc5ec\ub7ec \uac00\uc9c0\uc785\ub2c8\ub2e4 \u2013 \ub354 \ud574\uc11d \uac00\ub2a5\ud558\uace0, \uc2e4\uc9c8\uc801\uc73c\ub85c \ud6c8\ub828 \ubc0f \uc720\uc9c0 \uad00\ub9ac\uac00 \uc27d\uace0, \ubaa8\ub378 \ub3d9\uc791\uc744 \uae30\uc220\uc801\uc73c\ub85c \ubd84\uc11d\ud558\uace0 \ubcf4\ub2e4 \ud6a8\uacfc\uc801\uc778 \ubc29\ud5a5\uc73c\ub85c \uc218\uc815\ud560 \uc218 \uc788\uc73c\uba70, \ub4f1\ub4f1\uc785\ub2c8\ub2e4.\n\n\uc774 \uc139\uc158\uc5d0\uc11c\ub294 \uba3c\uc800 \uc124\uacc4\ub41c Light Graph Convolution Network (LightGCN) \ubaa8\ub378\uc744 \uc81c\uc2dc\ud558\uba70, Figure 2\uc5d0 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uadf8 \ub2e4\uc74c\uc73c\ub85c LightGCN\uc758 \uc2ec\uce35\uc801\uc778 \ubd84\uc11d\uc744 \ud1b5\ud574 \ub2e8\uc21c\ud55c \uc124\uacc4\uc758 \ud0c0\ub2f9\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \ucd94\ucc9c\uc744 \uc704\ud55c \ubaa8\ub378 \ud6c8\ub828 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n3.1 LightGCN\nGCN\uc758 \uae30\ubcf8 \uc544\uc774\ub514\uc5b4\ub294 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \ud2b9\uc9d5\uc744 \ud3c9\ud65c\ud654\ud558\uc5ec \ub178\ub4dc\uc5d0 \ub300\ud55c \ud45c\ud604\uc744 \ud559\uc2b5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4 [23, 40]. \uc774\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158\uc744 \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ud558\uc5ec, \uc0c8\ub85c\uc6b4 \ub300\uc0c1 \ub178\ub4dc\uc758 \ud45c\ud604\uc73c\ub85c \uc774\uc6c3\uc758 \ud2b9\uc9d5\uc744 \uc9d1\uacc4\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc774\uc6c3 \uc9d1\uacc4\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ucd94\uc0c1\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\ne(k+1)\nu = AGG(e(k)\nu , {e(k)\ni : i \u2208 Nu }). (2)\nAGG\ub294 \ub300\uc0c1 \ub178\ub4dc\uc640 \uadf8 \uc774\uc6c3 \ub178\ub4dc\uc5d0 \ub300\ud55c k-1 \uce35\uc758 \ud45c\ud604\uc744 \uace0\ub824\ud558\ub294 \uc9d1\uacc4 \ud568\uc218\uc785\ub2c8\ub2e4. \ub9ce\uc740 \uc5f0\uad6c\uc5d0\uc11c AGG\ub97c \uc9c0\uc815\ud588\uc73c\uba70, GIN [42]\uc758 \uac00\uc911 \ud569 \uc9d1\uacc4\uae30, GraphSAGE [14]\uc758 LSTM \uc9d1\uacc4\uae30, BGNN [48]\uc758 \uc30d\uc120\ud615 \uc0c1\ud638 \uc791\uc6a9 \uc9d1\uacc4\uae30 \ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ub300\ubd80\ubd84\uc758 \uc5f0\uad6c\ub294 AGG \ud568\uc218\uc640 \ud2b9\uc9d5 \ubcc0\ud658 \ub610\ub294 \ube44\uc120\ud615 \ud65c\uc131\ud654\ub97c \uacb0\ud569\ud569\ub2c8\ub2e4. \uc774\ub294 \uc758\ubbf8 \uc788\ub294 \uc785\ub825 \ud2b9\uc9d5\uc744 \uac00\uc9c4 \ub178\ub4dc \ub610\ub294 \uadf8\ub798\ud504 \ubd84\ub958 \uc791\uc5c5\uc5d0\uc11c \uc798 \uc218\ud589\ub418\uc9c0\ub9cc, \ud611\uc5c5 \ud544\ud130\ub9c1 (Section 2.2\uc758 \uc608\ube44 \uacb0\uacfc \ucc38\uc870)\uc5d0 \ubd80\ub2f4\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\u0ada\n\u123a\u0adb\u123b\n\u0ada\n\u0add\n\u123a\u0ad9\u123b\n\u0ada\n\u123a\u0ada\u123b\n\u0ada\n\u123a\u0adc\u123b\n\u0add\n\u123a\u0ada\u123b\n\u0add\n\u123a\u0adb\u123b\n\u0add\n\u123a\u0adc\u123b\nLight Graph Convolution (LGC)\n\u0d4c\u0adaNormalized Sum\n\u0ada\u0adb\n\u0ada\n\u0adb\n\u0b3f\u0ada\u123b\n\u0ada\n\u0d4c\u0ada\n\u0adb\n\u0adc\n\u0b3f\u0ada\u123b\n\u0adb\n\u0adc\nLayer Combination (weighted sum)\n\u0ada\n\u0adb\n\u0adc\n\u0adb\u0adc\nNormalized Sum\n\u0adc\n\u0adc\n\u0b3f\u0ada\u123b\nPrediction\nneighbors of u1 neighbors of i4\nFigure 2: An illustration of LightGCN model architecture.\nLGC\uc5d0\uc11c, \ub2e4\uc74c \uce35\uc73c\ub85c \ud2b9\uc9d5\uc744 \ud3c9\ud65c\ud654\ud558\uae30 \uc704\ud574, \uc774\uc6c3 \uc784\ubca0\ub529\uc758 \uc815\uaddc\ud654 \ud569\ub9cc \uc218\ud589\ud558\uace0, \uc790\uae30 \uc5f0\uacb0, \ud2b9\uc9d5 \ubcc0\ud658, \ube44\uc120\ud615 \ud65c\uc131\ud654\uc640 \uac19\uc740 \ub2e4\ub978 \uc5f0\uc0b0\uc740 \ubaa8\ub450 \uc81c\uac70\ud569\ub2c8\ub2e4. \uc774\ub294 GCN\uc744 \ud06c\uac8c \ub2e8\uc21c\ud654\ud569\ub2c8\ub2e4. Layer Combination\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uac01 \uce35\uc758 \uc784\ubca0\ub529\uc744 \ud569\ud558\uc5ec \ucd5c\uc885 \ud45c\ud604\uc744 \uc5bb\uc2b5\ub2c8\ub2e4.\n3.1.1 Light Graph Convolution (LGC). LightGCN\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uac04\ub2e8\ud55c \uac00\uc911 \ud569 \uc9d1\uacc4\uae30\ub97c \ucc44\ud0dd\ud558\uace0 \ud2b9\uc9d5 \ubcc0\ud658 \ubc0f \ube44\uc120\ud615 \ud65c\uc131\ud654\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \ud3ec\uae30\ud569\ub2c8\ub2e4. LightGCN\uc758 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0 (a.k.a., propagation rule [39])\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758\ub429\ub2c8\ub2e4:\ne(k+1)\nu =\nX\ni \u2208Nu\n1p\n|Nu |\np\n|Ni |\ne(k)\ni ,\ne(k+1)\ni =\nX\nu \u2208Ni\n1p\n|Ni |\np\n|Nu |\ne(k)\nu .\n(3)\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4 (Section 4.4.2\uc758 \uc2e4\ud5d8 \uacb0\uacfc \ucc38\uc870).\n|Nu |\n\u221a\n| Ni |\n\uc218\uc2dd\uc5d0 \ub530\ub978 \ub300\uce6d \uc815\uaddc\ud654 \ud56d 1\u221a\n\ub294 \ud45c\uc900 GCN [23]\uc758 \uc124\uacc4\ub97c \ub530\ub974\uba70, \uc774\ub294 \uadf8\ub798\ud504 \ucee8\ubcfc\ub8e8\uc158 \uc5f0\uc0b0\uc73c\ub85c \uc778\ud574 \uc784\ubca0\ub529\uc758 \uc2a4\ucf00\uc77c\uc774 \uc99d\uac00\ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub978 \uc120\ud0dd\ub3c4 \uc5ec\uae30\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, L1 norm\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacbd\ud5d8\uc801\uc73c\ub85c \uc774 \ub300\uce6d \uc815\uaddc\ud654\uac00 \uc88b\uc740 \uc131\ub2a5\uc744"
        },
        {
          "name": "Experiments",
          "original": "We first describe experimental settings, and then conduct detailed\ncomparison with NGCF [39], the method that is most relevant with\nLightGCN but more complicated (Section 4.2). We next compare\nwith other state-of-the-art methods in Section 4.3. To justify the\ndesigns in LightGCN and reveal the reasons of its effectiveness, we\nperform ablation studies and embedding analyses in Section 4.4.\nThe hyper-parameter study is finally presented in Section 4.5.\n4.1 Experimental Settings\nTo reduce the experiment workload and keep the comparison fair,\nwe closely follow the settings of the NGCF work [39]. We request\nthe experimented datasets (including train/test splits) from the\nauthors, for which the statistics are shown in Table 2. The Gowalla\nand Amazon-Book are exactly the same as the NGCF paper used, so\nwe directly use the results in the NGCF paper. The only exception\nis the Yelp2018 data, which is a revised version. According to the\nauthors, the previous version did not filter out cold-start items in\nthe testing set, and they shared us the revised version only. Thus\nwe re-run NGCF on the Yelp2018 data. The evaluation metrics are\nrecall@20 and ndcg@20 computed by the all-ranking protocol \u2014\nall items that are not interacted by a user are the candidates.\n4.1.1 Compared Methods. The main competing method is NGCF,\nwhich has shown to outperform several methods including GCN-\nbased models GC-MC [35] and PinSage [45], neural network-based\nmodels NeuMF [19] and CMN [10], and factorization-based models\nMF [32] and HOP-Rec [43]. As the comparison is done on the same\ndatasets under the same evaluation protocol, we do not further\ncompare with these methods. In addition to NGCF, we further\ncompare with two relevant and competitive CF methods:\n\u2022 Mult-VAE [28]. This is an item-based CF method based on the\nvariational autoencoder (VAE). It assumes the data is generated\nfrom a multinomial distribution and using variational inference\nfor parameter estimation. We run the codes released by the\nauthors5, tuning the dropout ratio in [0, 0.2, 0.5], and the \u03b2in\n[0.2, 0.4, 0.6, 0.8]. The model architecture is the suggested one in\nthe paper: 600 \u2192 200 \u2192 600.\n\u2022 GRMF [30]. This method smooths matrix factorization by adding\nthe graph Laplacian regularizer. For fair comparison on item\nrecommendation, we change the rating prediction loss to BPR\nloss. The objective function of GRMF is:\nL = \u2212\nMX\nu=1\nX\ni \u2208Nu\n\u0010 X\nj /\u2208Nu\nln\u03c3(eT\nu ei \u2212 eT\nu ej ) +\u03bb\u0434|| eu \u2212 ei || 2\n\u0011\n+\u03bb|| E|| 2,\n(16)\nwhere \u03bb\u0434is searched in the range of [1e\u22125, 1e\u22124, ..., 1e\u22121].\nMoreover, we compare with a variant that adds normalization\nto graph Laplacian: \u03bb\u0434|| eu\u221a\n| Nu |\n\u2212 ei\u221a\n| Ni |\n|| 2, which is termed\nas GRMF-norm. Other hyper-parameter settings are same as\nLightGCN. The two GRMF methods benchmark the performance\nof smoothing embeddings via Laplacian regularizer, while our\nLightGCN achieves embedding smoothing in the predictive\nmodel.\n4.1.2 Hyper-parameter Settings. Same as NGCF, the embedding\nsize is fixed to 64 for all models and the embedding parameters are\ninitialized with the Xavier method [ 12]. We optimize LightGCN\nwith Adam [22] and use the default learning rate of 0.001 and default\nmini-batch size of 1024 (on Amazon-Book, we increase the mini-\nbatch size to 2048 for speed). The L2 regularization coefficient\u03bbis\nsearched in the range of {1e\u22126, 1e\u22125, ..., 1e\u22122}, and in most cases\nthe optimal value is 1e\u22124. The layer combination coefficient\u03b1k is\nuniformly set to 1\n1+K where K is the number of layers. We testK in\nthe range of 1 to 4, and satisfactory performance can be achieved\nwhen K equals to 3. The early stopping and validation strategies\nare the same as NGCF. Typically, 1000 epochs are sufficient for\nLightGCN to converge. Our implementations are available in both\nTensorFlow6 and PyTorch7.\n4.2 Performance Comparison with NGCF\nWe perform detailed comparison with NGCF, recording the\nperformance at different layers (1 to 4) in Table 4, which also shows\nthe percentage of relative improvement on each metric. We further\nplot the training curves of training loss and testing recall in Figure 3\nto reveal the advantages of LightGCN and to be clear of the training\nprocess. The main observations are as follows:\n\u2022 In all cases, LightGCN outperforms NGCF by a large margin. For\nexample, on Gowalla the highest recall reported in the NGCF\npaper is 0.1570, while our LightGCN can reach 0.1830 under\nthe 4-layer setting, which is 16.56% higher. On average, the\nrecall improvement on the three datasets is 16.52% and the ndcg\nimprovement is 16.87%, which are rather significant.\n\u2022 Aligning Table 4 with Table 1 in Section 2, we can see that\nLightGCN performs better than NGCF-fn, the variant of NGCF\nthat removes feature transformation and nonlinear activation. As\nNGCF-fn still contains more operations than LightGCN (e.g., self-\nconnection, the interaction between user embedding and item\n5https://github.com/dawenl/vae_cf\n6https://github.com/kuandeng/LightGCN\n7https://github.com/gusye1234/pytorch-light-gcn\n\nTable 3: Performance comparison between NGCF and LightGCN at different layers.\nDataset Gowalla Yelp2018 Amazon-Book\nLayer # Method recall ndcg recall ndcg recall ndcg\n1 Layer NGCF 0.1556 0.1315 0.0543 0.0442 0.0313 0.0241\nLightGCN 0.1755(+12.79%) 0.1492(+13.46%) 0.0631(+16.20%) 0.0515(+16.51%) 0.0384(+22.68%) 0.0298(+23.65%)\n2 Layers NGCF 0.1547 0.1307 0.0566 0.0465 0.0330 0.0254\nLightGCN 0.1777(+14.84%) 0.1524(+16.60%) 0.0622(+9.89%) 0.0504(+8.38%) 0.0411(+24.54%) 0.0315(+24.02%)\n3 Layers NGCF 0.1569 0.1327 0.0579 0.0477 0.0337 0.0261\nLightGCN 0.1823(+16.19%) 0.1555(+17.18%) 0.0639(+10.38%) 0.0525(+10.06%) 0.0410(+21.66%) 0.0318(+21.84%)\n4 Layers NGCF 0.1570 0.1327 0.0566 0.0461 0.0344 0.0263\nLightGCN 0.1830(+16.56%) 0.1550(+16.80%) 0.0649(+14.58%) 0.0530(+15.02%) 0.0406(+17.92%) 0.0313(+18.92%)\n*The scores of NGCF on Gowalla and Amazon-Book are directly copied from Table 3 of the NGCF paper (https://arxiv.org/abs/1905.08108)\n0 200 400 600 800\nEpoch\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06Training-Loss\nGowalla\n0 200 400 600 800\nEpoch\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18recall@20\nGowalla\n0 100 200 300 400 500\nEpoch\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05Training-Loss\nAmazon-Book\n0 100 200 300 400 500\nEpoch\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040recall@20\nAmazon-Book\nFigure 3: Training curves of LightGCN and NGCF, which are evaluated by training loss and testing recall per 20 epochs on\nGowalla and Amazon-Book (results on Yelp2018 show exactly the same trend which are omitted for space).\nembedding in graph convolution, and dropout), this suggests that\nthese operations might also be useless for NGCF-fn.\n\u2022 Increasing the number of layers can improve the performance, but\nthe benefits diminish. The general observation is that increasing\nthe layer number from 0 (i.e., the matrix factorization model,\nresults see [39]) to 1 leads to the largest performance gain, and\nusing a layer number of 3 leads to satisfactory performance in\nmost cases. This observation is consistent with NGCF\u2019s finding.\n\u2022 Along the training process, LightGCN consistently obtains\nlower training loss, which indicates that LightGCN fits the\ntraining data better than NGCF. Moreover, the lower training\nloss successfully transfers to better testing accuracy, indicating\nthe strong generalization power of LightGCN. In contrast, the\nhigher training loss and lower testing accuracy of NGCF reflect\nthe practical difficulty to train such a heavy model it well. Note\nthat in the figures we show the training process under the optimal\nhyper-parameter setting for both methods. Although increasing\nthe learning rate of NGCF can decrease its training loss (even\nlower than that of LightGCN), the testing recall could not be\nimproved, as lowering training loss in this way only finds trivial\nsolution for NGCF.\n4.3 Performance Comparison with\nState-of-the-Arts\nTable 4 shows the performance comparison with competing\nmethods. We show the best score we can obtain for each method.\nWe can see that LightGCN consistently outperforms other methods\non all three datasets, demonstrating its high effectiveness with\nsimple yet reasonable designs. Note that LightGCN can be further\nimproved by tuning the \u03b1k (see Figure 4 for an evidence), while\nhere we only use a uniform setting of 1\nK+1 to avoid over-tuning it.\nAmong the baselines, Mult-VAE exhibits the strongest performance,\nwhich is better than GRMF and NGCF. The performance of GRMF is\non a par with NGCF, being better than MF, which admits the utility\nof enforcing embedding smoothness with Laplacian regularizer.\nBy adding normalization into the Laplacian regularizer, GRMF-\nnorm betters than GRMF on Gowalla, while brings no benefits on\nYelp2018 and Amazon-Book.\nTable 4: The comparison of overall performance among\nLightGCN and competing methods.\nDataset Gowalla Yelp2018 Amazon-Book\nMethod recall ndcg recall ndcg recall ndcg\nNGCF 0.1570 0.1327 0.0579 0.0477 0.0344 0.0263\nMult-VAE 0.1641 0.1335 0.0584 0.0450 0.0407 0.0315\nGRMF 0.1477 0.1205 0.0571 0.0462 0.0354 0.0270\nGRMF-norm 0.1557 0.1261 0.0561 0.0454 0.0352 0.0269\nLightGCN 0.1830 0.1554 0.0649 0.0530 0.0411 0.0315\n4.4 Ablation and Effectiveness Analyses\nWe perform ablation studies on LightGCN by showing how\nlayer combination and symmetric sqrt normalization affect its\nperformance. To justify the rationality of LightGCN as analyzed\nin Section 3.2.3, we further investigate the effect of embedding\nsmoothness \u2014 the key reason of LightGCN\u2019s effectiveness.\n4.4.1 Impact of Layer Combination. Figure 4 shows the results of\nLightGCN and its variant LightGCN-single that does not use layer\ncombination (i.e., E(K) is used for final prediction for a K-layer\nLightGCN). We omit the results on Yelp2018 due to space limitation,\nwhich show similar trend with Amazon-Book. We have three main\nobservations:\n\u2022 Focusing on LightGCN-single, we find that its performance first\nimproves and then drops when the layer number increases from\n\n1 2 3 4\nNumber of Layers\n0.15\n0.16\n0.17\n0.18\n0.19\n0.20recall@20\nGowalla\n1 2 3 4\nNumber of Layers\n0.13\n0.14\n0.15\n0.16\n0.17ndcg@20\nGowalla\n1 2 3 4\nNumber of Layers\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055recall@20\nAmazon-Book\n1 2 3 4\nNumber of Layers\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045ndcg@20\nAmazon-Book\nFigure 4: Results of LightGCN and the variant that does not use layer combination (i.e., LightGCN-single) at different layers\non Gowalla and Amazon-Book (results on Yelp2018 shows the same trend with Amazon-Book which are omitted for space).\nTable 5: Performance of the 3-layer LightGCN with different\nchoices of normalization schemes in graph convolution.\nDataset Gowalla Yelp2018 Amazon-Book\nMethod recall ndcg recall ndcg recall ndcg\nLightGCN-L1-L 0.1724 0.1414 0.0630 0.0511 0.0419 0.0320\nLightGCN-L1-R 0.1578 0.1348 0.0587 0.0477 0.0334 0.0259\nLightGCN-L1 0.159 0.1319 0.0573 0.0465 0.0361 0.0275\nLightGCN-L 0.1589 0.1317 0.0619 0.0509 0.0383 0.0299\nLightGCN-R 0.1420 0.1156 0.0521 0.0401 0.0252 0.0196\nLightGCN 0.1830 0.1554 0.0649 0.0530 0.0411 0.0315\nMethod notation: -L means only the left-side norm is used, -R means only\nthe right-side norm is used, and -L1 means the L1 norm is used.\n1 to 4. The peak point is on layer 2 in most cases, while after that\nit drops quickly to the worst point of layer 4. This indicates that\nsmoothing a node\u2019s embedding with its first-order and second-\norder neighbors is very useful for CF, but will suffer from over-\nsmoothing issues when higher-order neighbors are used.\n\u2022 Focusing on LightGCN, we find that its performance gradually\nimproves with the increasing of layers. Even using 4 layers,\nLightGCN\u2019s performance is not degraded. This justifies the\neffectiveness of layer combination for addressing over-smoothing,\nas we have technically analyzed in Section 3.2.2 (relation with\nAPPNP).\n\u2022 Comparing the two methods, we find that LightGCN consistently\noutperforms LightGCN-single on Gowalla, but not on Amazon-\nBook and Yelp2018 (where the 2-layer LightGCN-single performs\nthe best). Regarding this phenomenon, two points need to be\nnoted before we draw conclusion: 1) LightGCN-single is special\ncase of LightGCN that sets\u03b1K to 1 and other\u03b1k to 0; 2) we do\nnot tune the\u03b1k and simply set it as 1\nK+1 uniformly for LightGCN.\nAs such, we can see the potential of further enhancing the\nperformance of LightGCN by tuning\u03b1k .\n4.4.2 Impact of Symmetric Sqrt Normalization. In LightGCN,\nwe employ symmetric sqrt normalization 1\u221a\n| Nu |\n\u221a\n| Ni |\non each\nneighbor embedding when performing neighborhood aggregation\n(cf. Equation (3)). To study its rationality, we explore different\nchoices here. We test the use of normalization only at the left\nside (i.e., the target node\u2019s coefficient) and the right side (i.e., the\nneighbor node\u2019s coefficient). We also test L1 normalization, i.e.,\nremoving the square root. Note that if removing normalization,\nthe training becomes numerically unstable and suffers from not-\na-value (NAN) issues, so we do not show this setting. Table 5\nTable 6: Smoothness loss of the embeddings learned by\nLightGCN and MF (the lower the smoother).\nDataset Gowalla Yelp2018 Amazon-book\nSmoothness of User Embeddings\nMF 15449.3 16258.2 38034.2\nLightGCN-single 12872.7 10091.7 32191.1\nSmoothness of Item Embeddings\nMF 12106.7 16632.1 28307.9\nLightGCN-single 5829.0 6459.8 16866.0\nshows the results of the 3-layer LightGCN. We have the following\nobservations:\n\u2022 The best setting in general is using sqrt normalization at both\nsides (i.e., the current design of LightGCN). Removing either side\nwill drop the performance largely.\n\u2022 The second best setting is using L1 normalization at the left side\nonly (i.e., LightGCN-L1-L). This is equivalent to normalize the\nadjacency matrix as a stochastic matrix by the in-degree.\n\u2022 Normalizing symmetrically on two sides is helpful for the\nsqrt normalization, but will degrade the performance of L1\nnormalization.\n4.4.3 Analysis of Embedding Smoothness. As we have analyzed\nin Section 3.2.3, a 2-layer LightGCN smooths a user\u2019s embedding\nbased on the users that have overlap on her interacted items, and\nthe smoothing strength between two users cv\u2192u is measured in\nEquation (14). We speculate that such smoothing of embeddings is\nthe key reason of LightGCN\u2019s effectiveness. To verify this, we first\ndefine the smoothness of user embeddings as:\nSU =\nMX\nu=1\nMX\nv=1\ncv\u2192u( eu\n|| eu || 2 \u2212 ev\n|| ev || 2 )2, (17)\nwhere the L2 norm on embeddings is used to eliminate the\nimpact of the embedding\u2019s scale. Similarly we can obtained the\ndefinition for item embeddings. Table 6 shows the smoothness\nof two models, matrix factorization (i.e., using the E(0) for model\nprediction) and the 2-layer LightGCN-single (i.e., using the E(2) for\nprediction). Note that the 2-layer LightGCN-single outperforms\nMF in recommendation accuracy by a large margin. As can be\nseen, the smoothness loss of LightGCN-single is much lower\nthan that of MF. This indicates that by conducting light graph\nconvolution, the embeddings become smoother and more suitable\nfor recommendation.\n\n0 1e-6 1e-5 1e-4 1e-3 1e-2\nRegularization\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\n0.060recall@20\n0 1e-6 1e-5 1e-4 1e-3 1e-2\nRegularization\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050ndcg@20\nFigure 5: Performance of 2-layer LightGCN w.r.t. different\nregularization coefficient\u03bbon Yelp and Amazon-Book.\n4.5 Hyper-parameter Studies\nWhen applying LightGCN to a new dataset, besides the standard\nhyper-parameter learning rate, the most important hyper-parameter\nto tune is the L2 regularization coefficient\u03bb. Here we investigate\nthe performance change of LightGCN w.r.t.\u03bb.\nAs shown in Figure 5, LightGCN is relatively insensitive to \u03bb\n\u2014 even when \u03bbsets to 0, LightGCN is better than NGCF, which\nadditionally uses dropout to prevent overfitting8. This shows that\nLightGCN is less prone to overfitting \u2014 since the only trainable\nparameters in LightGCN are ID embeddings of the 0-th layer,\nthe whole model is easy to train and to regularize. The optimal\nvalue for Yelp2018, Amazon-Book, and Gowalla is 1e\u22123, 1e\u22124, and\n1e\u22124, respectively. When \u03bbis larger than 1e\u22123, the performance\ndrops quickly, which indicates that too strong regularization will\nnegatively affect model normal training and is not encouraged.",
          "translated": "**\uc2e4\ud5d8**\n\n\uc6b0\uc120 \uc2e4\ud5d8 \uc124\uc815\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uace0, \uadf8 \ub2e4\uc74c NGCF [39]\uc640 \ube44\uad50\ub97c \uc0c1\uc138\ud558\uac8c \uc218\ud589\ud569\ub2c8\ub2e4. NGCF\ub294 LightGCN\ubcf4\ub2e4 \ub354 \ubcf5\uc7a1\ud558\uc9c0\ub9cc \uac00\uc7a5 \uad00\ub828\uc131\uc774 \ub192\uc740 \ubc29\ubc95\uc785\ub2c8\ub2e4 (\uc139\uc158 4.2). \ub610\ud55c \uc139\uc158 4.3\uc5d0\uc11c \ub2e4\ub978 \ucd5c\ucca8\ub2e8 \ubc29\ubc95\ub4e4\uacfc \ube44\uad50\ud569\ub2c8\ub2e4. LightGCN\uc758 \uc124\uacc4 \uc774\uc720\uc640 \ud6a8\uacfc\uc758 \uc6d0\uc778\uc744 \ubc1d\ud788\uae30 \uc704\ud574 \uc139\uc158 4.4\uc5d0\uc11c \uc801\ub300 \uc5f0\uad6c\uc640 \uc784\ubca0\ub529 \ubd84\uc11d\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc139\uc158 4.5\uc5d0\uc11c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc5f0\uad6c\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.\n\n4.1 \uc2e4\ud5d8 \uc124\uc815\n\uc2e4\ud5d8 \ubd80\ud558\ub97c \uc904\uc774\uace0 \ube44\uad50\ub97c \uacf5\uc815\ud558\uac8c \uc720\uc9c0\ud558\uae30 \uc704\ud574 NGCF [39]\uc758 \uc124\uc815\uc744 \ucd5c\ub300\ud55c \ub530\ub985\ub2c8\ub2e4. \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ub41c \ub370\uc774\ud130 \uc138\ud2b8 (\ud6c8\ub828/\ud14c\uc2a4\ud2b8 \ubd84\ud560 \ud3ec\ud568)\ub97c \uc800\uc790\ub85c\ubd80\ud130 \uc694\uccad\ubc1b\uc558\uc73c\uba70, \ud1b5\uacc4\ub294 \ud45c 2\uc5d0 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. Gowalla\uc640 Amazon-Book\uc740 NGCF \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uac83\uacfc \ub3d9\uc77c\ud558\ubbc0\ub85c NGCF \ub17c\ubb38\uc5d0\uc11c \uc5bb\uc740 \uacb0\uacfc\ub97c \uc9c1\uc811 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc720\uc77c\ud55c \uc608\uc678\ub294 Yelp2018 \ub370\uc774\ud130\ub85c, \uc218\uc815\ub41c \ubc84\uc804\uc785\ub2c8\ub2e4. \uc800\uc790\uc5d0 \ub530\ub974\uba74 \uc774\uc804 \ubc84\uc804\uc5d0\uc11c\ub294 \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \ub0c9\uac01 \uc2dc\uc791 \ud56d\ubaa9\uc744 \ud544\ud130\ub9c1\ud558\uc9c0 \uc54a\uc558\uc73c\uba70, \uc218\uc815\ub41c \ubc84\uc804\uc744 \uc800\ud76c\uc5d0\uac8c\ub9cc \uacf5\uc720\ud588\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c NGCF\ub97c Yelp2018 \ub370\uc774\ud130\uc5d0 \uc7ac\uc2e4\ud589\ud569\ub2c8\ub2e4. \ud3c9\uac00 \uc9c0\ud45c\ub294 recall@20\uc640 ndcg@20\ub85c all-ranking \ud504\ub85c\ud1a0\ucf5c\uc5d0 \uc758\ud574 \uacc4\uc0b0\ub429\ub2c8\ub2e4. \uc989, \uc0ac\uc6a9\uc790\uac00 \uc0c1\ud638 \uc791\uc6a9\ud558\uc9c0 \uc54a\ub294 \ubaa8\ub4e0 \ud56d\ubaa9\uc774 \ud6c4\ubcf4 \ud56d\ubaa9\uc774 \ub429\ub2c8\ub2e4.\n\n4.1.1 \ube44\uad50 \ubc29\ubc95\n\uc8fc\uc694 \uacbd\uc7c1 \ubc29\ubc95\uc740 NGCF\uc774\uba70, GCN \uae30\ubc18 \ubaa8\ub378\uc778 GC-MC [35], PinSage [45], \uc2e0\uacbd\ub9dd \uae30\ubc18 \ubaa8\ub378 NeuMF [19], CMN [10], \uadf8\ub9ac\uace0 MF [32] \ubc0f HOP-Rec [43]\uc640 \uac19\uc740 \uc694\uc778 \ubd84\uc11d \uae30\ubc18 \ubaa8\ub378\uc744 \ud3ec\ud568\ud558\uc5ec \uc5ec\ub7ec \ubc29\ubc95\ubcf4\ub2e4 \uc131\ub2a5\uc774 \uc6b0\uc218\ud568\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \ube44\uad50\ub294 \ub3d9\uc77c\ud55c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \ub3d9\uc77c\ud55c \ud3c9\uac00 \ud504\ub85c\ud1a0\ucf5c\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc218\ud589\ub418\uc5c8\uc73c\ubbc0\ub85c, \uc774\ub7ec\ud55c \ubc29\ubc95\ub4e4\uacfc \ucd94\uac00\uc801\uc73c\ub85c \ube44\uad50\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. NGCF \uc678\uc5d0\ub3c4 \ub450 \uac00\uc9c0 \uad00\ub828\ub418\uace0 \uacbd\uc7c1\uc801\uc778 CF \ubc29\ubc95\uc744 \ucd94\uac00\ub85c \ube44\uad50\ud569\ub2c8\ub2e4.\n\u2022 Mult-VAE [28]. \uc774 \ubc29\ubc95\uc740 \ubcc0\ubd84 \uc624\ud1a0\uc778\ucf54\ub354 (VAE)\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 \ud56d\ubaa9 \uae30\ubc18 CF \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 \ub370\uc774\ud130\uac00 \ub2e4\ud56d \ubd84\ud3ec\uc5d0\uc11c \uc0dd\uc131\ub41c\ub2e4\uace0 \uac00\uc815\ud558\uace0 \ubcc0\ubd84 \ucd94\ub860\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub9e4\uac1c\ubcc0\uc218\ub97c \ucd94\uc815\ud569\ub2c8\ub2e4. \uc800\uc790\uac00 \ubc1c\ud45c\ud55c \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uace0 \ub4dc\ub86d\uc544\uc6c3 \ube44\uc728\uc744 [0, 0.2, 0.5] \ubc94\uc704\uc5d0\uc11c, \u03b2in [0.2, 0.4, 0.6, 0.8] \ubc94\uc704\uc5d0\uc11c \uc870\uc815\ud569\ub2c8\ub2e4. \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub294 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ub41c \uac83\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4: 600 \u2192 200 \u2192 600.\n\u2022 GRMF [30]. \uc774 \ubc29\ubc95\uc740 \uadf8\ub798\ud504 \ub77c\ud50c\ub77c\uc2dc\uc548 \uc815\uaddc\ud654 \ucd94\uac00\ub85c \uc694\uc778 \ubd84\uc11d \ub9e4\ud2b8\ub9ad\uc2a4\ub97c \ud3c9\ud65c\ud654\ud569\ub2c8\ub2e4. \ud56d\ubaa9 \ucd94\ucc9c \uc131\ub2a5 \uce21\uba74\uc5d0\uc11c \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \ud3c9\uac00\uc9c0\ud45c\ub97c BPR \uc190\uc2e4\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4. GRMF\uc758 \ubaa9\uc801 \ud568\uc218\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\nL = \u2212\nMX\nu=1\nX\ni \u2208Nu\n\u0010 X\nj /\u2208Nu\nln\u03c3(eT\nu ei \u2212 eT\nu ej ) +\u03bb\u0434|| eu \u2212 ei || 2\n\u0011\n+\u03bb|| E|| 2,\n(16)\n\uc5ec\uae30\uc11c \u03bb\u0434\ub294 [1e\u22125, 1e\u22124, ..., 1e\u22121] \ubc94\uc704\uc5d0\uc11c \uac80\uc0c9\ub429\ub2c8\ub2e4. \ub610\ud55c \uadf8\ub798\ud504 \ub77c\ud50c\ub77c\uc2dc\uc548\uc5d0 \uc815\uaddc\ud654\ub97c \ucd94\uac00\ud55c \ubcc0\ud615\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \uc989, \u03bb\u0434|| eu\u221a\n| Nu |\n\u2212 ei\u221a\n| Ni |\n|| 2, \uc774\ub294 GRMF-norm\uc73c\ub85c \uba85\uba85\ub429\ub2c8\ub2e4. \ub2e4\ub978 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc740 LightGCN\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4. \ub450 GRMF \ubc29\ubc95\uc740 \ub77c\ud50c\ub77c\uc2dc\uc548 \uc815\uaddc\ud654 \uc815\ub82c\uc744 \ud1b5\ud55c \uc784\ubca0\ub529 \ud3c9\ud65c\ud654\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4. \ubc18\uba74 LightGCN\uc740 \uc608\uce21 \ubaa8\ub378\uc5d0\uc11c \uc784\ubca0\ub529 \ud3c9\ud65c\ud654\ub97c \ub2ec\uc131\ud569\ub2c8\ub2e4.\n\n4.1.2 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\nNGCF\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ubaa8\ub4e0 \ubaa8\ub378\uc5d0\uc11c \uc784\ubca0\ub529 \ud06c\uae30\ub294 64\ub85c \uace0\uc815\ub418\uc5b4 \uc788\uc73c\uba70, \uc784\ubca0\ub529 \ub9e4\uac1c\ubcc0\uc218\ub294 Xavier \ubc29\ubc95 [ 12]\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. LightGCN\uc740 Adam [22]\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd5c\uc801\ud654\ud558\uace0 \uae30\ubcf8 \ud559\uc2b5\ub960 0.001\uacfc \uae30\ubcf8 \ubc30\uce58 \ud06c\uae30 1024\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4 (Amazon-Book\uc5d0\uc11c\ub294 \uc18d\ub3c4\ub97c \uc704\ud574 \ubc30\uce58 \ud06c\uae30\ub97c 2048\ub85c \uc99d\uac00\uc2dc\ud0b5\ub2c8\ub2e4). L2 \uc815\uaddc\ud654 \uacc4\uc218 \u03bb\ub294 {1e\u22126, 1e\u22125, ..., 1e\u22122} \ubc94\uc704\uc5d0\uc11c \uac80\uc0c9\ub418\uba70, \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0 \ucd5c\uc801 \uac12\uc740 1e\u22124\uc785\ub2c8\ub2e4. \ub808\uc774\uc5b4 \uc870\ud569 \uacc4\uc218 \u03b1k\ub294 1\n1+K \uc5ec\uae30\uc11c K\ub294 \ub808\uc774\uc5b4\uc758 \uc218\ub85c \uade0\ub4f1\ud558\uac8c \uc124\uc815\ub429\ub2c8\ub2e4. K\uc758 \ubc94\uc704\ub294 1\uc5d0\uc11c 4\uae4c\uc9c0 \ud14c\uc2a4\ud2b8\ud558\uace0 K\uac00 3\uc77c \ub54c \ub9cc\uc871\uc2a4\ub7ec\uc6b4 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc870\uae30 \uc911\uc9c0 \ubc0f \uac80\uc99d \uc804\ub7b5\uc740 NGCF\uc640 \ub3d9\uc77c\ud569\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c LightGCN\uc774 \uc218\ub834\ud558\ub294 \ub370 1000\uac1c\uc758 \uc5d0\ud3ec\ud06c\uac00 \ucda9\ubd84\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \uad6c\ud604\uc740 TensorFlow6 \ubc0f PyTorch7\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n4.2 NGCF\uc640\uc758 \uc131\ub2a5 \ube44\uad50\nNGCF\uc640\uc758 \uc0c1\uc138\ud55c \ube44\uad50\ub97c \uc218\ud589\ud558\uace0 \ud45c 4\uc5d0 \uc131\ub2a5\uc744 \uae30\ub85d\ud558\uba70, \uc774\ub294 \ub610\ud55c \uac01 \uc9c0\ud45c\uc5d0 \ub300\ud55c \uc0c1\ub300\uc801\uc778 \uac1c\uc120\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub610\ud55c \uadf8\ub9bc 3\uc5d0\uc11c LightGCN\uc758 \ud6c8\ub828 \uc190\uc2e4\uacfc \ud14c\uc2a4\ud2b8 recall \uace1\uc120\uc744 \ud50c\ub86f\ud558\uc5ec LightGCN\uc758 \uc7a5\uc810\uc744 \ubc1d\ud788\uace0 \ud6c8\ub828 \uacfc\uc815\uc744 \uba85\ud655\ud558\uac8c \ud569\ub2c8\ub2e4. \uc8fc\uc694 \uad00\ucc30 \uc0ac\ud56d\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\u2022 \ubaa8\ub4e0 \uacbd\uc6b0\uc5d0 LightGCN\uc740 NGCF\ubcf4\ub2e4 \ud6e8\uc52c \ud070 \ud3ed\uc73c\ub85c \uc6b0\uc218\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 Gowalla\uc5d0\uc11c NGCF \ub17c\ubb38\uc5d0\uc11c \ubcf4\uace0\ud55c \ucd5c\uace0 recall \uac12\uc740 0.1570\uc778 \ubc18\uba74 LightGCN\uc740 4-\ub808\uc774\uc5b4 \uc124\uc815\uc744 \uc0ac\uc6a9\ud558\uc5ec 0.1830\uc73c\ub85c \uc774\ub97c 16.56% \ub192\uc785\ub2c8\ub2e4. \ud3c9\uade0\uc801\uc73c\ub85c \uc138 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c recall \uac1c\uc120\uc740 16.52%\uc774\uace0 ndcg \uac1c\uc120\uc740 16.87%\ub85c \uc0c1\ub2f9\ud788 \ud07d\ub2c8\ub2e4.\n\u2022 \ud45c 4\ub97c \uc139\uc158 2\uc758 \ud45c 1\uacfc \ud568\uaed8 \ubcf4\uba74 LightGCN\uc740 \ud2b9\uc9d5 \ubcc0\ud658 \ubc0f \ube44\uc120\ud615 \ud65c\uc131\ud654 \uc5c6\uc774 \uc81c\uac70\ub41c NGCF-fn \ubcc0\ud615\ubcf4\ub2e4 \uc6b0\uc218\ud569\ub2c8\ub2e4. NGCF-fn\uc740 \uc5ec\uc804\ud788 LightGCN\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \uc5f0\uc0b0\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4 (\uc608: \uc790\uae30 \uc5f0\uacb0, \uc0ac\uc6a9\uc790 \uc784\ubca0\ub529\uacfc \ud56d\ubaa9 \uac04\uc758 \uc0c1\ud638 \uc791\uc6a9).\n... (\uc0dd\ub7b5) ..."
        },
        {
          "name": "Related Work",
          "original": "5.1 Collaborative Filtering\nCollaborative Filtering (CF) is a prevalent technique in modern\nrecommender systems [ 7, 45]. One common paradigm of CF\nmodel is to parameterize users and items as embeddings, and\nlearn the embedding parameters by reconstructing historical user-\nitem interactions. For example, earlier CF models like matrix\nfactorization (MF) [ 26, 32] project the ID of a user (or an item)\ninto an embedding vector. The recent neural recommender models\nlike NCF [19] and LRML [34] use the same embedding component,\nwhile enhance the interaction modeling with neural networks.\nBeyond merely using ID information, another type of CF methods\nconsiders historical items as the pre-existing features of a user,\ntowards better user representations. For example, FISM [21] and\nSVD++ [ 25] use the weighted average of the ID embeddings\nof historical items as the target user\u2019s embedding. Recently,\nresearchers realize that historical items have different contributions\nto shape personal interest. Towards this end, attention mechanisms\nare introduced to capture the varying contributions, such as\nACF [3] and NAIS [ 18], to automatically learn the importance\nof each historical item. When revisiting historical interactions as\na user-item bipartite graph, the performance improvements can\nbe attributed to the encoding of local neighborhood \u2014 one-hop\nneighbors \u2014 that improves the embedding learning.\n8Note that Gowalla shows the same trend with Amazon-Book, so its curves are not\nshown to better highlight the trend of Yelp2018 and Amazon-Book.\n5.2 Graph Methods for Recommendation\nAnother relevant research line is exploiting the user-item graph\nstructure for recommendation. Prior efforts like ItemRank [ 13],\nuse the label propagation mechanism to directly propagate user\npreference scores over the graph,i.e., encouraging connected nodes\nto have similar labels. Recently emerged graph neural networks\n(GNNs) shine a light on modeling graph structure, especially high-\nhop neighbors, to guide the embedding learning [ 14, 23]. Early\nstudies define graph convolution on the spectral domain, such as\nLaplacian eigen-decomposition [1] and Chebyshev polynomials [8],\nwhich are computationally expensive. Later on, GraphSage [14] and\nGCN [23] re-define graph convolution in the spatial domain, i.e.,\naggregating the embeddings of neighbors to refine the target node\u2019s\nembedding. Owing to its interpretability and efficiency, it quickly\nbecomes a prevalent formulation of GNNs and is being widely\nused [11, 29, 47]. Motivated by the strength of graph convolution,\nrecent efforts like NGCF [39], GC-MC [35], and PinSage [45] adapt\nGCN to the user-item interaction graph, capturing CF signals in\nhigh-hop neighbors for recommendation.\nIt is worth mentioning that several recent efforts provide deep\ninsights into GNNs [ 24, 27, 40], which inspire us developing\nLightGCN. Particularly, Wu et al. [ 40] argues the unnecessary\ncomplexity of GCN, developing a simplified GCN (SGCN) model\nby removing nonlinearities and collapsing multiple weight\nmatrices into one. One main difference is that LightGCN and\nSGCN are developed for different tasks, thus the rationality of\nmodel simplification is different. Specifically, SGCN is for node\nclassification, performing simplification for model interpretability\nand efficiency. In contrast, LightGCN is on collaborative filtering\n(CF), where each node has an ID feature only. Thus, we do\nsimplification for a stronger reason: nonlinearity and weight\nmatrices are useless for CF, and even hurt model training. For node\nclassification accuracy, SGCN is on par with (sometimes weaker\nthan) GCN. While for CF accuracy, LightGCN outperforms GCN by\na large margin (over 15% improvement over NGCF). Lastly, another\nwork conducted in the same time [4] also finds that the nonlinearity\nis unnecessary in NGCF and develops linear GCN model for CF. In\ncontrast, our LightGCN makes one step further \u2014 we remove all\nredundant parameters and retain only the ID embeddings, making\nthe model as simple as MF.\n6 CONCLUSION AND FUTURE WORK\nIn this work, we argued the unnecessarily complicated design of\nGCNs for collaborative filtering, and performed empirical studies\nto justify this argument. We proposed LightGCN which consists\nof two essential components \u2014 light graph convolution and\nlayer combination. In light graph convolution, we discard feature\ntransformation and nonlinear activation \u2014 two standard operations\nin GCNs but inevitably increase the training difficulty. In layer\ncombination, we construct a node\u2019s final embedding as the weighted\nsum of its embeddings on all layers, which is proved to subsume the\neffect of self-connections and is helpful to control oversmoothing.\nWe conduct experiments to demonstrate the strengths of LightGCN\nin being simple: easier to be trained, better generalization ability,\nand more effective.\n\nWe believe the insights of LightGCN are inspirational to future\ndevelopments of recommender models. With the prevalence of\nlinked graph data in real applications, graph-based models are\nbecoming increasingly important in recommendation; by explicitly\nexploiting the relations among entities in the predictive model, they\nare advantageous to traditional supervised learning scheme like\nfactorization machines [17, 33] that model the relations implicitly.\nFor example, a recent trend is to exploit auxiliary information such\nas item knowledge graph [38], social network [41] and multimedia\ncontent [44] for recommendation, where GCNs have set up the\nnew state-of-the-art. However, these models may also suffer from\nthe similar issues of NGCF since the user-item interaction graph is\nalso modeled by same neural operations that may be unnecessary.\nWe plan to explore the idea of LightGCN in these models. Another\nfuture direction is to personalize the layer combination weights\u03b1k ,\nso as to enable adaptive-order smoothing for different users (e.g.,\nsparse users may require more signal from higher-order neighbors\nwhile active users require less). Lastly, we will explore further the\nstrengths of LightGCN\u2019s simplicity, studying whether fast solution\nexists for non-sampling regression loss [20] and streaming it for\nonline industrial scenarios.\nAcknowledgement. The authors thank Bin Wu, Jianbai Ye,\nand Yingxin Wu for contributing to the implementation and\nimprovement of LightGCN. This work is supported by the\nNational Natural Science Foundation of China (61972372, U19A2079,\n61725203).",
          "translated": "**5.1 Collaborative Filtering**\n\nCollaborative Filtering (CF)\ub294 \ud604\ub300 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc5d0\uc11c \ud754\ud788 \uc0ac\uc6a9\ub418\ub294 \uae30\uc220 [ 7, 45]. CF \ubaa8\ub378\uc758 \uc77c\ubc18\uc801\uc778 \ud328\ub7ec\ub2e4\uc784 \uc911 \ud558\ub098\ub294 \uc0ac\uc6a9\uc790 \ubc0f \uc544\uc774\ud15c\uc744 \uc784\ubca0\ub529\uc73c\ub85c \ud30c\ub77c\ubbf8\ud130\ud654\ud558\uace0, \uc5ed\uc0ac\uc801 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638\uc791\uc6a9\uc744 \uc7ac\uad6c\uc131\ud558\uc5ec \uc784\ubca0\ub529 \ud30c\ub77c\ubbf8\ud130\ub97c \ud559\uc2b5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc774\uc804 CF \ubaa8\ub378\uc778 \ud589\ub82c \ubd84\ud574 (MF) [ 26, 32]\ub294 \uc0ac\uc6a9\uc790 (\ub610\ub294 \uc544\uc774\ud15c)\uc758 ID\ub97c \uc784\ubca0\ub529 \ubca1\ud130\ub85c \ud22c\uc601\ud569\ub2c8\ub2e4. \ucd5c\uadfc  \uc2e0\uacbd \ucd94\ucc9c \ubaa8\ub378\uc778 NCF [19] \ubc0f LRML [34]\ub294 \ub3d9\uc77c\ud55c \uc784\ubca0\ub529 \uad6c\uc131 \uc694\uc18c\ub97c \uc0ac\uc6a9\ud558\uba74\uc11c \uc2e0\uacbd\ub9dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c1\ud638\uc791\uc6a9 \ubaa8\ub378\ub9c1\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4. ID \uc815\ubcf4\ub97c \ub2e8\uc21c\ud788 \uc0ac\uc6a9\ud558\ub294 \uac83 \uc678\uc5d0\ub3c4, \ub2e4\ub978 \uc720\ud615\uc758 CF \ubc29\ubc95\uc740 \uc5ed\uc0ac\uc801 \uc544\uc774\ud15c\uc744 \uc0ac\uc6a9\uc790\uc758 \uae30\uc874 \ud2b9\uc9d5\uc73c\ub85c \uace0\ub824\ud558\uc5ec \uc0ac\uc6a9\uc790 \ud45c\ud604\uc744 \uac1c\uc120\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, FISM [21] \ubc0f SVD++ [ 25]\ub294 \uc5ed\uc0ac\uc801 \uc544\uc774\ud15c\uc758 \uac00\uc911 \ud3c9\uade0\uc744 \ub300\uc0c1 \uc0ac\uc6a9\uc790\uc758 \uc784\ubca0\ub529\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ucd5c\uadfc \uc5f0\uad6c\uc790\ub4e4\uc740 \uc5ed\uc0ac\uc801 \uc544\uc774\ud15c\uc774 \uac1c\uc778\uc801\uc778 \uad00\uc2ec\uc744 \ud615\uc131\ud558\ub294 \ub370 \uc11c\ub85c \ub2e4\ub978 \uae30\uc5ec\ub97c \ud55c\ub2e4\ub294 \uac83\uc744 \uc778\uc2dd\ud558\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574, ACF [3] \ubc0f NAIS [ 18]\uc640 \uac19\uc740 \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc774 \uc11c\ub85c \ub2e4\ub978 \uae30\uc5ec\ub97c \uc790\ub3d9\uc73c\ub85c \ud559\uc2b5\ud558\uae30 \uc704\ud574 \ub3c4\uc785\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc774\ubd84 \uadf8\ub798\ud504\ub85c \uc5ed\uc0ac\uc801 \uc0c1\ud638\uc791\uc6a9\uc744 \uc7ac\ubc29\ubb38\ud560 \ub54c \uc131\ub2a5 \uac1c\uc120\uc740 \ud55c-\ud649 \uc774\uc6c3\uc744 \uc778\ucf54\ub529\ud558\uc5ec \uc784\ubca0\ub529 \ud559\uc2b5\uc744 \uac1c\uc120\ud588\uae30 \ub54c\ubb38\uc5d0 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.\n\n8Note that Gowalla shows the same trend with Amazon-Book, so its curves are not\nshown to better highlight the trend of Yelp2018 and Amazon-Book.\n\n**5.2 Graph Methods for Recommendation**\n\n\ub610 \ub2e4\ub978 \uad00\ub828 \uc5f0\uad6c \ubd84\uc57c\ub294 \ucd94\ucc9c\uc744 \uc704\ud574 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uadf8\ub798\ud504 \uad6c\uc870\ub97c \ud65c\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. ItemRank [ 13]\uc640 \uac19\uc740 \uc774\uc804 \ub178\ub825\uc740 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790 \uc120\ud638\ub3c4 \uc810\uc218\ub97c \uc9c1\uc811 \uc804\ud30c\ud558\ub294 \ub808\uc774\ube14 \uc804\ud30c \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc989, \uc5f0\uacb0\ub41c \ub178\ub4dc\uac00 \uc720\uc0ac\ud55c \ub808\uc774\ube14\uc744 \uac16\ub3c4\ub85d \uc7a5\ub824\ud569\ub2c8\ub2e4. \ucd5c\uadfc \ub4f1\uc7a5\ud55c \uadf8\ub798\ud504 \uc2e0\uacbd\ub9dd (GNNs)\ub294 \ud2b9\ud788 \uace0-\ud649 \uc774\uc6c3\uc744 \ubaa8\ub378\ub9c1\ud558\uc5ec \uc784\ubca0\ub529 \ud559\uc2b5\uc744 \uc548\ub0b4\ud558\ub294 \uadf8\ub798\ud504 \uad6c\uc870, \ud2b9\ud788 \ubaa8\ub378\ub9c1\uc5d0 \ud070 \ube5b\uc744 \ube44\ucda5\ub2c8\ub2e4 [ 14, 23]. \ucd08\uae30 \uc5f0\uad6c\uc5d0\uc11c\ub294 \uc2a4\ud399\ud2b8\ub7fc \uc601\uc5ed\uc5d0\uc11c \uadf8\ub798\ud504 \uacf5\ubcc0\ud654\ub97c \uc815\uc758\ud588\uc2b5\ub2c8\ub2e4. \uc989, \ub77c\ud50c\ub77c\uc2dc\uc548 \uc790\uba85\uac12 \ubd84\ud574 [1] \ubc0f \uccb4\ube44\uc170\ud504 \ub2e4\ud56d\uc2dd [8]\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc0b0 \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4ed\ub2c8\ub2e4. \ub098\uc911\uc5d0, GraphSage [14] \ubc0f GCN [23]\ub294 \uacf5\uac04 \uc601\uc5ed\uc5d0\uc11c \uadf8\ub798\ud504 \uacf5\ubcc0\ud654\ub97c \uc7ac\uc815\uc758\ud588\uc2b5\ub2c8\ub2e4. \uc989, \ub300\uc0c1 \ub178\ub4dc\uc758 \uc784\ubca0\ub529\uc744 \uac1c\uc120\ud558\uae30 \uc704\ud574 \uc774\uc6c3\uc758 \uc784\ubca0\ub529\uc744 \uc9d1\uacc4\ud569\ub2c8\ub2e4. \ud574\uc11d \uac00\ub2a5\uc131\uacfc \ud6a8\uc728\uc131 \ub54c\ubb38\uc5d0, GNN\uc758 \uc77c\ubc18\uc801\uc778 \ud615\ud0dc\ub85c \ube60\ub974\uac8c \uc790\ub9ac \uc7a1\uc558\uc73c\uba70 \ub110\ub9ac \uc0ac\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4 [11, 29, 47]. \uadf8\ub798\ud504 \uacf5\ubcc0\ud654\uc758 \uac15\ub3c4\ub97c \ubc14\ud0d5\uc73c\ub85c, NGCF [39], GC-MC [35], \ubc0f PinSage [45]\uc640 \uac19\uc740 \ucd5c\uadfc \ub178\ub825\uc740 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638\uc791\uc6a9 \uadf8\ub798\ud504\ub97c \uc0ac\uc6a9\ud558\uc5ec CF \uc2e0\ud638\ub97c \uace0-\ud649 \uc774\uc6c3\uc5d0\uc11c \ucea1\ucc98\ud558\uc5ec \ucd94\ucc9c\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.\n\n\uc5ec\ub7ec \ucd5c\uadfc \ub178\ub825\uc774 GNN\uc5d0 \ub300\ud55c \uae4a\uc740 \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud55c\ub2e4\ub294 \uc810\uc5d0 \uc8fc\ubaa9\ud560 \ub9cc\ud569\ub2c8\ub2e4 [ 24, 27, 40], \uc774\ub294 LightGCN\uc744 \uac1c\ubc1c\ud558\ub294 \ub370 \uc601\uac10\uc744 \uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, Wu et al. [ 40]\ub294 GCN\uc758 \ubd88\ud544\uc694\ud55c \ubcf5\uc7a1\uc131\uc744 \uc8fc\uc7a5\ud558\uace0 \ube44\uc120\ud615\uc131 \ubc0f \uc5ec\ub7ec \uac00\uc911 \ud589\ub82c\uc744 \ud558\ub098\uc758 \ud589\ub82c\ub85c \ud569\uc0b0\ud558\uc5ec \ubaa8\ub378 \ub2e8\uc21c\ud654\ub97c \ud1b5\ud574 SGCN \ubaa8\ub378\uc744 \uac1c\ubc1c\ud588\uc2b5\ub2c8\ub2e4. \uc8fc\uc694 \ucc28\uc774\uc810\uc740 LightGCN\uacfc SGCN\uc774 \uc11c\ub85c \ub2e4\ub978 \uc791\uc5c5\uc5d0 \ub300\ud574 \uac1c\ubc1c\ub418\uc5c8\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubaa8\ub378 \ub2e8\uc21c\ud654\uc758 \ud0c0\ub2f9\uc131\uc740 \ub2e4\ub985\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, SGCN\uc740 \ub178\ub4dc \ubd84\ub958\ub97c \uc704\ud55c \uac83\uc774\uba70, \ubaa8\ub378 \ud574\uc11d \uac00\ub2a5\uc131\uacfc \ud6a8\uc728\uc131\uc744 \uc704\ud574 \ub2e8\uc21c\ud654\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \ubc18\ub300\ub85c, LightGCN\uc740 \ud611\uc5c5 \ud544\ud130\ub9c1 (CF)\uc5d0 \uc788\uc73c\uba70, \uac01 \ub178\ub4dc\ub294 ID \uae30\ub2a5\ub9cc \uac16\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc6b0\ub9ac\ub294 \ub354 \uac15\ub825\ud55c \uc774\uc720\ub85c \ub2e8\uc21c\ud654\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \ube44\uc120\ud615\uc131\uacfc \uac00\uc911 \ud589\ub82c\uc740 CF\uc5d0 \uc4f8\ubaa8\uc5c6\uace0 \uc2ec\uc9c0\uc5b4 \ubaa8\ub378 \ud6c8\ub828\uc744 \uc800\ud574\ud569\ub2c8\ub2e4. \ub178\ub4dc \ubd84\ub958 \uc815\ud655\ub3c4\uc5d0 \ub300\ud574, SGCN\uc740 (\ub54c\ub85c\ub294 GCN\ubcf4\ub2e4 \ub0ae\uc744 \uc218 \uc788\ub294) GCN\uacfc \uc77c\uce58\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 CF \uc815\ud655\ub3c4\uc5d0 \ub300\ud574, LightGCN\uc740 NGCF\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ud070 \ub9c8\uc9c4 (NGCF\ubcf4\ub2e4 15% \uc774\uc0c1\uc758 \uac1c\uc120)\uc73c\ub85c \uc131\ub2a5\uc744 \ub2a5\uac00\ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uac19\uc740 \uc2dc\uae30\uc5d0 \uc218\ud589\ub41c \ub2e4\ub978 \uc791\uc5c5 [4]\ub3c4 NGCF\uc5d0\uc11c \ube44\uc120\ud615\uc131\uc774 \ubd88\ud544\uc694\ud558\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud558\uace0 CF\uc5d0 \ub300\ud55c \uc120\ud615 GCN \ubaa8\ub378\uc744 \uac1c\ubc1c\ud588\uc2b5\ub2c8\ub2e4. \ub300\uc870\uc801\uc73c\ub85c, LightGCN\uc740 \ud55c \uac78\uc74c \ub354 \ub098\uc544\uac00 \ubaa8\ub4e0 \uc911\ubcf5 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc81c\uac70\ud558\uace0 ID \uc784\ubca0\ub529\ub9cc \uc720\uc9c0\ud558\uc5ec \ubaa8\ub378\uc744 MF\uc640 \uac19\uc774 \ub2e8\uc21c\ud558\uac8c \ub9cc\ub4ed\ub2c8\ub2e4.\n\n6 CONCLUSION AND FUTURE WORK\n\n\uc774 \uc791\uc5c5\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ud611\uc5c5 \ud544\ud130\ub9c1\uc5d0 \ub300\ud55c GCN\uc758 \ubd88\ud544\uc694\ud558\uac8c \ubcf5\uc7a1\ud55c \uc124\uacc4\ub97c \uc8fc\uc7a5\ud558\uace0, \uc774\ub97c \uc815\ub2f9\ud654\ud558\uae30 \uc704\ud574 \uacbd\ud5d8\uc801 \uc5f0\uad6c\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 LightGCN\uc744 \uc81c\uc548\ud588\ub294\ub370, \uc774\ub294 \uac00\ubcbc\uc6b4 \uadf8\ub798\ud504 \uacf5\ubcc0\ud654 \ubc0f \ub808\uc774\uc5b4 \uacb0\ud569\uc758 \ub450 \uac00\uc9c0 \ud544\uc218 \uad6c\uc131 \uc694\uc18c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uac00\ubcbc\uc6b4 \uadf8\ub798\ud504 \uacf5\ubcc0\ud654\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ud45c\uc900 GCN\uc5d0\uc11c \uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \uae30\ub2a5 \ubcc0\ud658 \ubc0f \ube44\uc120\ud615 \ud65c\uc131\ud654\uc640 \uac19\uc774 \ub450 \uac00\uc9c0 \ud45c\uc900 \uc791\uc5c5\uc744 \uc81c\uac70\ud558\uc5ec \ud6c8\ub828 \ub09c\uc774\ub3c4\ub97c \uc99d\uac00\uc2dc\ud0b5\ub2c8\ub2e4. \ub808\uc774\uc5b4 \uacb0\ud569\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ubaa8\ub4e0 \ub808\uc774\uc5b4\uc758 \uc784\ubca0\ub529\uc744 \uac00\uc911 \ud3c9\uade0\ud558\uc5ec \ub300\uc0c1 \ub178\ub4dc\uc758 \uc784\ubca0\ub529\uc73c\ub85c \uad6c\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 \uc790\uac00 \uc5f0\uacb0 \ud6a8\uacfc\ub97c subsume\ud558\uace0 \uacfc\ub3c4\ud55c \ub9e4\ub044\ub7ec\uc6c0 \ud1b5\uc81c\ub97c \ub3d5\ub294 \uac83\uc73c\ub85c \uc99d\uba85\ub418\uc5c8\uc73c\uba70, \uc774\ub294 \ub354 \ub098\uc740 \uc77c\ubc18\ud654 \ub2a5\ub825\uacfc \ub354 \ud6a8\uacfc\uc801\uc778 \ud559\uc2b5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 LightGCN\uc774 \ub2e8\uc21c\ud568 (\ub354 \uc27d\uac8c \ud6c8\ub828\ud560 \uc218 \uc788\uc74c, \ub354 \ub098\uc740 \uc77c\ubc18\ud654 \ub2a5\ub825, \ub354 \ud6a8\uacfc\uc801\uc784)\uc5d0\uc11c \uac15\uc810\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \ubbff\uc2b5\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 LightGCN\uc758 \ud1b5\ucc30\ub825\uc774 \ucd94\ucc9c \ubaa8\ub378\uc758 \ubbf8\ub798 \uac1c\ubc1c\uc5d0 \uc601\uac10\uc744 \uc904 \uac83\uc774\ub77c\uace0 \ubbff\uc2b5\ub2c8\ub2e4. \ub9c1\ud06c\ub41c gr"
        },
        {
          "name": "References",
          "original": "[1] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. Spectral\nNetworks and Locally Connected Networks on Graphs. In ICLR.\n[2] Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, and Yi-Hsuan Yang. 2019.\nCollaborative Similarity Embedding for Recommender Systems. In WWW. 2637\u2013\n2643.\n[3] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-\nSeng Chua. 2017. Attentive Collaborative Filtering: Multimedia Recommendation\nwith Item- and Component-Level Attention. In SIGIR. 335\u2013344.\n[4] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting\nGraph based Collaborative Filtering: A Linear Residual Graph Convolutional\nNetwork Approach. In AAAI.\n[5] Yihong Chen, Bei Chen, Xiangnan He, Chen Gao, Yong Li, Jian-Guang Lou, and\nYue Wang. 2019.\u03bbOpt: Learn to Regularize Recommender Models in Finer Levels.\nIn KDD. 978\u2013986.\n[6] Zhiyong Cheng, Ying Ding, Lei Zhu, and Mohan S. Kankanhalli. 2018. Aspect-\nAware Latent Factor Model: Rating Prediction with Ratings and Reviews. In\nWWW. 639\u2013648.\n[7] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for\nYouTube Recommendations. In RecSys. 191\u2013198.\n[8] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016.\nConvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\nIn NeurIPS. 3837\u20133845.\n[9] Jingtao Ding, Yuhan Quan, Xiangnan He, Yong Li, and Depeng Jin. 2019.\nReinforced Negative Sampling for Recommendation with Exposure Data. In\nIJCAI. 2230\u20132236.\n[10] Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collaborative Memory Network for\nRecommendation Systems. In SIGIR. 515\u2013524.\n[11] Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, and Tat-Seng Chua.\n2019. Temporal Relational Ranking for Stock Prediction. TOIS 37, 2 (2019),\n27:1\u201327:30.\n[12] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training\ndeep feedforward neural networks. In AISTATS. 249\u2013256.\n[13] Marco Gori and Augusto Pucci. 2007. ItemRank: A Random-Walk Based Scoring\nAlgorithm for Recommender Engines. In IJCAI. 2766\u20132771.\n[14] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive\nRepresentation Learning on Large Graphs. In NeurIPS. 1025\u20131035.\n[15] Taher H Haveliwala. 2002. Topic-sensitive pagerank. In WWW. 517\u2013526.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In CVPR. 770\u2013778.\n[17] Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization Machines for Sparse\nPredictive Analytics. In SIGIR. 355\u2013364.\n[18] Xiangnan He, Zhankui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang,\nand Tat-Seng Chua. 2018. NAIS: Neural Attentive Item Similarity Model for\nRecommendation. TKDE 30, 12 (2018), 2354\u20132366.\n[19] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In WWW. 173\u2013182.\n[20] Xiangnan He, Jinhui Tang, Xiaoyu Du, Richang Hong, Tongwei Ren, and Tat-Seng\nChua. 2019. Fast Matrix Factorization with Nonuniform Weights on Missing\nData. TNNLS (2019).\n[21] Santosh Kabbur, Xia Ning, and George Karypis. 2013. FISM: factored item\nsimilarity models for top-N recommender systems. In KDD. 659\u2013667.\n[22] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic\nOptimization. In ICLR.\n[23] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with\nGraph Convolutional Networks. In ICLR.\n[24] Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. 2019.\nPredict then propagate: Graph neural networks meet personalized pagerank.\nIn ICLR.\n[25] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted\ncollaborative filtering model. In KDD. 426\u2013434.\n[26] Yehuda Koren, Robert M. Bell, and Chris Volinsky. 2009. Matrix Factorization\nTechniques for Recommender Systems. IEEE Computer 42, 8 (2009), 30\u201337.\n[27] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper Insights Into Graph\nConvolutional Networks for Semi-Supervised Learning. In AAAI. 3538\u20133545.\n[28] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018.\nVariational Autoencoders for Collaborative Filtering. In WWW. 689\u2013698.\n[29] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. 2018.\nDeepInf: Social Influence Prediction with Deep Learning. In KDD. 2110\u20132119.\n[30] Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon. 2015.\nCollaborative filtering with graph information: Consistency and scalable methods.\nIn NIPS. 2107\u20132115.\n[31] Steffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning\nfor item recommendation from implicit feedback. In WSDM. 273\u2013282.\n[32] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI. 452\u2013\n461.\n[33] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.\n2011. Fast context-aware recommendations with factorization machines. InSIGIR.\n635\u2013644.\n[34] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Latent relational metric learning\nvia memory-based attention for collaborative ranking. In WWW. 729\u2013739.\n[35] Rianne van den Berg, Thomas N. Kipf, and Max Welling. 2018. Graph\nConvolutional Matrix Completion. In KDD Workshop on Deep Learning Day .\n[36] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.\n[37] Jun Wang, Arjen P. de Vries, and Marcel J. T. Reinders. 2006. Unifying User-based\nand Item-based Collaborative Filtering Approaches by Similarity Fusion. InSIGIR.\n501\u2013508.\n[38] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT:\nKnowledge Graph Attention Network for Recommendation. In KDD. 950\u2013958.\n[39] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR. 165\u2013174.\n[40] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and\nKilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. InICML.\n6861\u20136871.\n[41] Le Wu, Peijie Sun, Yanjie Fu, Richang Hong, Xiting Wang, and Meng Wang.\n2019. A Neural Influence Diffusion Model for Social Recommendation. In SIGIR.\n235\u2013244.\n[42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful\nare graph neural networks?. In ICLR.\n[43] Jheng-Hong Yang, Chih-Ming Chen, Chuan-Ju Wang, and Ming-Feng Tsai. 2018.\nHOP-rec: high-order proximity for implicit recommendation. InRecSys. 140\u2013144.\n[44] Yinwei Yin, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng\nChua. 2019. MMGCN: Multimodal Graph Convolution Network for Personalized\nRecommendation of Micro-video. In MM.\n[45] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,\nand Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale\nRecommender Systems. In KDD (Data Science track) . 974\u2013983.\n[46] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020.\nParameter-Efficient Transfer from Sequential Behaviors for User Modeling and\nRecommendation. In SIGIR.\n[47] Cheng Zhao, Chenliang Li, and Cong Fu. 2019. Cross-Domain Recommendation\nvia Preference Propagation GraphNet. In CIKM. 2165\u20132168.\n[48] Hongmin Zhu, Fuli Feng, Xiangnan He, Xiang Wang, Yan Li, Kai Zheng,\nand Yongdong Zhang. 2020. Bilinear Graph Neural Network with Neighbor\nInteractions. In IJCAI.",
          "translated": "[\ucc38\uace0\ubb38\ud5cc \uc0dd\ub7b5]"
        }
      ],
      "full_summary": "\uc774 \ub17c\ubb38\uc740 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638\uc791\uc6a9 \uadf8\ub798\ud504\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 \ub525\ub7ec\ub2dd \uae30\ubc18 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc778 NGCF\uc758 \uc131\ub2a5\uc744 \ubd84\uc11d\ud558\uace0 \uac1c\uc120\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4. \ud2b9\ud788, NGCF\uc758 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\uc778 \ud2b9\uc9d5 \ubcc0\ud658(feature transformation)\uacfc \ube44\uc120\ud615 \ud65c\uc131\ud654 \ud568\uc218(non-linear activation function)\uac00 \ucd94\ucc9c \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc2ec\uce35\uc801\uc73c\ub85c \uc870\uc0ac\ud569\ub2c8\ub2e4.\n\n\uc8fc\uc694 \ub0b4\uc6a9:\n- NGCF \ubd84\uc11d: NGCF\uc758 \uad6c\uc870\uc640 \uc791\ub3d9 \ubc29\uc2dd\uc744 \uc124\uba85\ud558\uace0, \ud2b9\uc9d5 \ubcc0\ud658\uacfc \ube44\uc120\ud615 \ud65c\uc131\ud654 \ud568\uc218\uac00 \ucd94\ucc9c \uc131\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud569\ub2c8\ub2e4.\n- Ablation Study: NGCF\uc758 \uc138 \uac00\uc9c0 \ubcc0\ud615 \ubaa8\ub378(NGCF-f, NGCF-n, NGCF-fn)\uc744 \uad6c\ucd95\ud558\uace0, \uac01\uac01\uc758 \uc131\ub2a5\uc744 \ube44\uad50 \ubd84\uc11d\ud569\ub2c8\ub2e4.\n- \uacb0\uacfc: \uc2e4\ud5d8 \uacb0\uacfc, \ud2b9\uc9d5 \ubcc0\ud658\uacfc \ube44\uc120\ud615 \ud65c\uc131\ud654 \ud568\uc218\uac00 \ucd94\ucc9c \uc131\ub2a5\uc744 \uc800\ud558\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \uc785\uc99d\ud569\ub2c8\ub2e4. \ud2b9\ud788, \ud2b9\uc9d5 \ubcc0\ud658\uc744 \uc81c\uac70\ud55c NGCF-fn \ubaa8\ub378\uc774 NGCF \ubaa8\ub378\ubcf4\ub2e4 \ud6e8\uc52c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.\n- \uacb0\ub860: \ucd94\ucc9c \uc2dc\uc2a4\ud15c \uc124\uacc4 \uc2dc, \ubd88\ud544\uc694\ud55c \ubcf5\uc7a1\uc131\uc744 \uc904\uc774\uace0, \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574 \ud2b9\uc9d5 \ubcc0\ud658\uacfc \ube44\uc120\ud615 \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc2e0\uc911\ud558\uac8c \uace0\ub824\ud574\uc57c \ud568\uc744 \uac15\uc870\ud569\ub2c8\ub2e4.\n\n\ud575\uc2ec \uae30\uc5ec:\n- NGCF\uc758 \uc131\ub2a5 \uc800\ud558 \uc6d0\uc778\uc744 \uba85\ud655\ud558\uac8c \ubc1d\ud600\ub0c4\uc73c\ub85c\uc368, \ub525\ub7ec\ub2dd \uae30\ubc18 \ucd94\ucc9c \uc2dc\uc2a4\ud15c \uc124\uacc4\uc5d0 \ub300\ud55c \uc911\uc694\ud55c \uc2dc\uc0ac\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n- \uc2e4\ud5d8\uc801\uc778 \ubd84\uc11d\uc744 \ud1b5\ud574, \ub525\ub7ec\ub2dd \ubaa8\ub378\uc758 \ubcf5\uc7a1\uc131\uc744 \uc904\uc774\uace0, \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.\n\n\uc774 \ub17c\ubb38\uc740 \ub525\ub7ec\ub2dd \uae30\ubc18 \ucd94\ucc9c \uc2dc\uc2a4\ud15c \uc124\uacc4\uc5d0 \ub300\ud55c \uc911\uc694\ud55c \uc5f0\uad6c \uacb0\uacfc\ub97c \uc81c\uacf5\ud558\uba70, \ud5a5\ud6c4 \uad00\ub828 \uc5f0\uad6c\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce60 \uac83\uc73c\ub85c \uae30\ub300\ub429\ub2c8\ub2e4."
    },
    {
      "id": "d51b6643-d2b5-431f-8df0-0b688e7d0f94",
      "title": "KGAT: Knowledge Graph Attention Network for Recommendation",
      "authors": [
        "Xiang Wang",
        "Xiangnan He",
        "Yixin Cao",
        "Meng Liu",
        "Tat-Seng Chua"
      ],
      "abstract": "To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
      "year": 2019,
      "arxiv_id": "1905.07854",
      "arxiv_url": "https://arxiv.org/abs/1905.07854",
      "conference": "KDD'19",
      "category": "recsys",
      "tags": [
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2019-05-20",
      "created_at": "2026-01-30T11:16:48.668808",
      "updated_at": "2026-01-30T12:03:45.416635",
      "summary": {
        "one_line": "KGAT introduces a Knowledge Graph Attention Network that leverages high-order item relationships to enhance recommendation accuracy and diversity.",
        "contribution": "This paper proposes KGAT, a novel method that incorporates knowledge graphs to model complex item relationships beyond traditional user-item interactions. Specifically, KGAT utilizes embedding propagation and an attention mechanism to capture high-order connections between items and attributes, addressing the limitations of existing methods.",
        "methodology": "KGAT recursively propagates embeddings from neighboring nodes (users, items, or attributes) within the knowledge graph. An attention mechanism is then applied to weight the importance of these neighbors during the embedding refinement process, allowing the model to prioritize relevant connections.",
        "results": "Experimental results on public benchmarks demonstrate that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet, showcasing the effectiveness of high-order relation modeling and the attention mechanism in recommendation."
      },
      "translation": {
        "title": "\uc81c\ubaa9: KGAT: \uc9c0\uc2dd \uadf8\ub798\ud504 \uc5b4\ud150\uc158 \ub124\ud2b8\uc6cc\ud06c \uae30\ubc18 \ucd94\ucc9c",
        "abstract": "\ucd08\ub85d: \ubcf4\ub2e4 \uc815\ud655\ud558\uace0, \ub2e4\uc591\ud558\uba70, \uc124\uba85 \uac00\ub2a5\ud55c \ucd94\ucc9c\uc744 \uc81c\uacf5\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638\uc791\uc6a9\uc744 \ubaa8\ub378\ub9c1\ud558\ub294 \uac83 \uc678\uc5d0\ub3c4 \ubd80\uac00 \uc815\ubcf4\ub97c \uace0\ub824\ud558\ub294 \uac83\uc774 \ud544\uc218\uc801\uc785\ub2c8\ub2e4.  \ud329\ud130\ub77c\uc774\uc81c\uc774\uc158 \uba38\uc2e0(FM)\uacfc \uac19\uc740 \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 \uc774\ub97c \uc9c0\ub3c4 \ud559\uc2b5 \ubb38\uc81c\ub85c \uac04\uc8fc\ud558\uba70, \uac01 \uc0c1\ud638\uc791\uc6a9\uc744 \ub3c5\ub9bd\uc801\uc778 \uc778\uc2a4\ud134\uc2a4\ub85c \uac00\uc815\ud558\uace0 \ubd80\uac00 \uc815\ubcf4\ub97c \uc778\ucf54\ub529\ud569\ub2c8\ub2e4.  \uc778\uc2a4\ud134\uc2a4 \ub610\ub294 \uc544\uc774\ud15c \uac04\uc758 \uad00\uacc4(\uc608: \uc601\ud654\uc758 \uac10\ub3c5\uc774 \ub2e4\ub978 \uc601\ud654\uc758 \ubc30\uc6b0)\ub97c \uac04\uacfc\ud558\ub294 \ub370\ub2e4, \uc774\ub7ec\ud55c \ubc29\ubc95\ub4e4\uc740 \uc0ac\uc6a9\uc790\ub4e4\uc758 \uc9d1\ub2e8 \ud589\ub3d9\uc5d0\uc11c \ud611\uc5c5 \uc2e0\ud638\ub97c \ucd94\ucd9c\ud558\ub294 \ub370 \ucda9\ubd84\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.  \ubcf8 \uc5f0\uad6c\uc5d0\uc11c\ub294 \uc9c0\uc2dd \uadf8\ub798\ud504(KG)\uc758 \uc720\uc6a9\uc131\uc744 \uc870\uc0ac\ud569\ub2c8\ub2e4. KG\ub294 \uc18d\uc131\uc744 \ud1b5\ud574 \uc544\uc774\ud15c\uc744 \uc5f0\uacb0\ud568\uc73c\ub85c\uc368 \ub3c5\ub9bd\uc801\uc778 \uc0c1\ud638\uc791\uc6a9 \uac00\uc815\uc5d0 \ub3c4\uc804\ud569\ub2c8\ub2e4.  KG\uc640 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uadf8\ub798\ud504\uc758 \uc774\ub7ec\ud55c \ud558\uc774\ube0c\ub9ac\ub4dc \uad6c\uc870\uc5d0\uc11c \ub450 \uc544\uc774\ud15c\uc744 \uc5f0\uacb0\ud558\ub294 \ud558\ub098 \uc774\uc0c1\uc758 \uc5f0\uacb0\ub41c \uc18d\uc131\uc744 \ud1b5\ud574 \ub098\ud0c0\ub098\ub294 \uace0\ucc28\uc6d0 \uad00\uacc4\ub294 \uc131\uacf5\uc801\uc778 \ucd94\ucc9c\uc5d0 \ud544\uc218\uc801\uc778 \uc694\uc18c\ub77c\ub294 \uc8fc\uc7a5\uc744 \ud3bc\uce69\ub2c8\ub2e4.  \uc800\ud76c\ub294 \uc9c0\uc2dd \uadf8\ub798\ud504 \uc5b4\ud150\uc158 \ub124\ud2b8\uc6cc\ud06c(KGAT)\ub77c\ub294 \uc0c8\ub85c\uc6b4 \ubc29\ubc95\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. KG \ub0b4\uc758 \uace0\ucc28\uc6d0 \uc5f0\uacb0\uc131\uc744 \uc804\uc774 \ud559\uc2b5 \ubc29\uc2dd\uc73c\ub85c \uba85\uc2dc\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4.  KG\uc758 \ub178\ub4dc \uc774\uc6c3(\uc0ac\uc6a9\uc790, \uc544\uc774\ud15c \ub610\ub294 \uc18d\uc131)\uc5d0\uc11c \uc784\ubca0\ub529\uc744 \uc7ac\uadc0\uc801\uc73c\ub85c \uc804\ud30c\ud558\uc5ec \ub178\ub4dc\uc758 \uc784\ubca0\ub529\uc744 \uac1c\uc120\ud558\uace0, \uc774\uc6c3\uc758 \uc911\uc694\ub3c4\ub97c \uad6c\ubcc4\ud558\uae30 \uc704\ud574 \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \uc800\ud76c\uc758 KGAT\ub294 \uae30\uc874 KG \uae30\ubc18 \ucd94\ucc9c \ubc29\ubc95\ubcf4\ub2e4 \uac1c\ub150\uc801\uc73c\ub85c \uc720\ub9ac\ud558\uba70, \uace0\ucc28\uc6d0 \uad00\uacc4\ub97c \ucd94\ucd9c\ud558\uc5ec \uacbd\ub85c\ub97c \ud65c\uc6a9\ud558\uac70\ub098 \uc815\uaddc\ud654\ub85c \uc554\uc2dc\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \ubc29\uc2dd\uc5d0 \uc758\uc874\ud569\ub2c8\ub2e4.  \uc138 \uac00\uc9c0 \uacf5\uac1c \ubca4\uce58\ub9c8\ud06c\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc\uc5d0 \ub530\ub974\uba74 KGAT\ub294 Neural FM \ubc0f RippleNet\uacfc \uac19\uc740 \ucd5c\ucca8\ub2e8 \ubc29\ubc95\ubcf4\ub2e4 \ud6e8\uc52c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.  \ucd94\uac00 \uc5f0\uad6c\ub294 \uace0\ucc28\uc6d0 \uad00\uacc4 \ubaa8\ub378\ub9c1\uc5d0 \ub300\ud55c \uc784\ubca0\ub529 \uc804\ud30c\uc758 \ud6a8\ub2a5\uacfc \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc73c\ub85c \uac00\uc838\uc624\ub294 \ud574\uc11d \uac00\ub2a5\uc131\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub97c \ud655\uc778\ud569\ub2c8\ub2e4."
      }
    },
    {
      "id": "559b0ff6-88d3-4b38-bc24-20c8722e9d16",
      "title": "Deep Delta Learning",
      "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Mengdi Wang",
        "Quanquan Gu"
      ],
      "abstract": "The effectiveness of deep residual networks hinges on the identity shortcut connection. While this mechanism alleviates the vanishing-gradient problem, it also has a strictly additive inductive bias on feature transformations, limiting the network's ability to model complex hidden state transitions. In this paper, we introduce \\textbf{Deep Delta Learning (DDL)}, which generalizes the shortcut from a fixed identity map to a learnable, state-dependent linear operator. The resulting Delta Operator is a rank-1 perturbation of the identity, $\\mathbf{A}(\\mathbf{X}) = \\mathbf{I}- \u03b2(\\mathbf{X})\\mathbf{k} (\\mathbf{X}) \\mathbf{k} (\\mathbf{X})^\\top$, parameterized by a unit direction $\\mathbf{k}(\\mathbf{X})$ and a scalar gate $\u03b2(\\mathbf{X})$. We provide a spectral analysis showing that $\u03b2(\\mathbf{X})$ continuously interpolates the shortcut between identity ($\u03b2=0$), orthogonal projection ($\u03b2=1$), and Householder reflection ($\u03b2=2$). Furthermore, we rewrite the residual update as a synchronized rank-1 delta write: $\u03b2$ scales both the removal of the current $\\mathbf{k}$-component and the injection of the new $\\mathbf{k}$-component. This unification enables explicit control of the shortcut spectrum along a data-dependent direction while retaining stable training behavior. Empirically, replacing Transformer residual additions with DDL improves validation loss and perplexity, as well as downstream evaluation accuracy on language modeling tasks, with larger gains in the expanded-state setting.",
      "year": 2026,
      "arxiv_id": "2601.00417",
      "arxiv_url": "https://arxiv.org/abs/2601.00417",
      "conference": null,
      "category": "ml",
      "tags": [
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        }
      ],
      "published_at": "2026-01-01",
      "created_at": "2026-01-30T17:01:16.738109",
      "updated_at": "2026-01-30T17:03:26.374854"
    },
    {
      "id": "cfdf54ff-64c8-4ba5-b705-87825d7a97df",
      "title": "Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation",
      "authors": [
        "Le Xiao",
        "Xiaolin Chen"
      ],
      "abstract": "News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. The experimental results show that the news summary generator is able to generate accurate and reliable news summaries with some generalization ability.",
      "year": 2023,
      "arxiv_id": "2307.02839",
      "arxiv_url": "https://arxiv.org/abs/2307.02839",
      "doi": null,
      "paper_url": null,
      "conference": null,
      "category": "nlp",
      "tags": [
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        }
      ],
      "published_at": "2023-07-06",
      "created_at": "2026-02-02T18:32:59.241228",
      "updated_at": "2026-02-02T18:32:59.241228"
    },
    {
      "id": "72278723-1cf4-44bf-85e3-7c05f9612d06",
      "title": "How AI Impacts Skill Formation",
      "authors": [
        "Judy Hanwen Shen",
        "Alex Tamkin"
      ],
      "abstract": "AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.",
      "year": 2026,
      "arxiv_id": "2601.20245",
      "arxiv_url": "https://arxiv.org/abs/2601.20245",
      "doi": null,
      "paper_url": null,
      "conference": null,
      "category": "other",
      "tags": [
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        }
      ],
      "published_at": "2026-01-28",
      "created_at": "2026-02-03T14:31:25.736856",
      "updated_at": "2026-02-03T17:48:42.012676",
      "translation": {
        "title": "\uc81c\ubaa9: AI\uac00 \uae30\uc220 \uc2b5\ub4dd\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5",
        "abstract": "\ucd08\ub85d: AI \uc9c0\uc6d0\uc740 \ud2b9\ud788 \ucd08\ubcf4 \uc791\uc5c5\uc790\uc758 \uacbd\uc6b0 \uc804\ubb38 \ubd84\uc57c\uc5d0\uc11c \uc0c1\ub2f9\ud55c \uc0dd\uc0b0\uc131 \ud5a5\uc0c1\uc744 \uac00\uc838\uc624\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 AI\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uac10\ub3c5\ud558\ub294 \ub370 \ud544\uc694\ud55c \uae30\uc220 \uac1c\ubc1c\uc5d0 \ub300\ud55c \uc774\ub7ec\ud55c \uc9c0\uc6d0\uc758 \uc601\ud5a5\uc740 \uc5ec\uc804\ud788 \ubd88\ubd84\uba85\ud569\ub2c8\ub2e4. AI\uc5d0 \uc758\uc874\ud558\uc5ec \uc775\uc219\ud558\uc9c0 \uc54a\uc740 \uc791\uc5c5\uc744 \uc644\ub8cc\ud558\ub294 \ucd08\ubcf4 \uc791\uc5c5\uc790\ub294 \uc774 \uacfc\uc815\uc5d0\uc11c \uc790\uc2e0\uc758 \uae30\uc220 \uc2b5\ub4dd\uc744 \uc800\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 AI\uc758 \ub3c4\uc6c0\uc744 \ubc1b\uc544 \uc0c8\ub85c\uc6b4 \ube44\ub3d9\uae30 \ud504\ub85c\uadf8\ub798\ubc0d \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \ub300\ud55c \uc219\ub2ec\uc744 \uc5bb\ub294 \uac1c\ubc1c\uc790\uac00 \uc5b4\ub5bb\uac8c \ub418\uc5c8\ub294\uc9c0 \uc870\uc0ac\ud558\uae30 \uc704\ud574 \ubb34\uc791\uc704 \uc2e4\ud5d8\uc744 \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc5f0\uad6c \uacb0\uacfc, AI \uc0ac\uc6a9\uc740 \uac1c\ub150\uc801 \uc774\ud574, \ucf54\ub4dc \uc77d\uae30, \ub514\ubc84\uae45 \ub2a5\ub825\uc5d0 \uc190\uc0c1\uc744 \uc8fc\uace0 \ud3c9\uade0\uc801\uc73c\ub85c \uc0c1\ub2f9\ud55c \ud6a8\uc728\uc131 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ucc38\uac00\uc790\ub4e4\uc774 \ucf54\ub529 \uc791\uc5c5\uc744 \uc644\uc804\ud788 \uc704\uc784\ud55c \uacbd\uc6b0 \uc0dd\uc0b0\uc131\uc774 \uc57d\uac04 \ud5a5\uc0c1\ub418\uc5c8\uc9c0\ub9cc \ub77c\uc774\ube0c\ub7ec\ub9ac \ud559\uc2b5\uc758 \ube44\uc6a9\uc774 \ubc1c\uc0dd\ud588\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc138 \uac00\uc9c0\uac00 AI\uc640\uc758 \uc778\uc9c0\uc801 \ucc38\uc5ec\ub97c \ud3ec\ud568\ud558\ub294 \uc5ec\uc12f \uac00\uc9c0 \uad6c\ubcc4\ub418\ub294 AI \uc0c1\ud638 \uc791\uc6a9 \ud328\ud134\uc744 \uc2dd\ubcc4\ud588\uc73c\uba70, \ucc38\uac00\uc790\uac00 AI \uc9c0\uc6d0\uc744 \ubc1b\ub354\ub77c\ub3c4 \ud559\uc2b5 \uacb0\uacfc\uac00 \uc720\uc9c0\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacb0\uacfc\ub294 AI \uac15\ud654\ub41c \uc0dd\uc0b0\uc131\uc774 \uc804\ubb38\uc131\uc744 \uc704\ud55c \ub2e8\ucd95 \ucf54\uc2a4\uac00 \uc544\ub2c8\uba70, \ud2b9\ud788 \uc548\uc804 \uad00\ub828 \ubd84\uc57c\uc5d0\uc11c \uae30\uc220 \uc2b5\ub4dd\uc744 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc6cc\ud06c\ud50c\ub85c\uc6b0\uc5d0 AI \uc9c0\uc6d0\uc744 \uc2e0\uc911\ud558\uac8c \ub3c4\uc785\ud574\uc57c \ud568\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4."
      }
    },
    {
      "id": "75e2f793-4eca-4dff-a15f-722fc453340d",
      "title": "Multi-Behavior Sequential Recommendation with Temporal Graph Transformer",
      "authors": [
        "Lianghao Xia",
        "Chao Huang",
        "Yong Xu",
        "Jian Pei"
      ],
      "abstract": "Modeling time-evolving preferences of users with their sequential item interactions, has attracted increasing attention in many online applications. Hence, sequential recommender systems have been developed to learn the dynamic user interests from the historical interactions for suggesting items. However, the interaction pattern encoding functions in most existing sequential recommender systems have focused on single type of user-item interactions. In many real-life online platforms, user-item interactive behaviors are often multi-typed (e.g., click, add-to-favorite, purchase) with complex cross-type behavior inter-dependencies. Learning from informative representations of users and items based on their multi-typed interaction data, is of great importance to accurately characterize the time-evolving user preference. In this work, we tackle the dynamic user-item relation learning with the awareness of multi-behavior interactive patterns. Towards this end, we propose a new Temporal Graph Transformer (TGT) recommendation framework to jointly capture dynamic short-term and long-range user-item interactive patterns, by exploring the evolving correlations across different types of behaviors. The new TGT method endows the sequential recommendation architecture to distill dedicated knowledge for type-specific behavior relational context and the implicit behavior dependencies. Experiments on the real-world datasets indicate that our method TGT consistently outperforms various state-of-the-art recommendation methods. Our model implementation codes are available at https://github.com/akaxlh/TGT.",
      "year": 2022,
      "arxiv_id": "2206.02687",
      "arxiv_url": "https://arxiv.org/abs/2206.02687",
      "conference": "IEEE Trans Knowl Data Eng'22",
      "category": "recsys",
      "tags": [
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2022-06-06",
      "created_at": "2026-02-03T17:48:25.509188",
      "updated_at": "2026-02-04T16:28:13.993014",
      "translation": {
        "title": "\uc81c\ubaa9: \ub2e4\uc911 \ud589\ub3d9 \uc21c\ucc28 \ucd94\ucc9c\uc744 \uc704\ud55c \uc2dc\uac04\uc801 \uadf8\ub798\ud504 \ud2b8\ub79c\uc2a4\ud3ec\uba38",
        "abstract": "\ucd08\ub85d: \uc0ac\uc6a9\uc790\uc758 \uc2dc\uac04 \ubcc0\ud654\ud558\ub294 \uc120\ud638\ub3c4\ub97c \uadf8\ub4e4\uc758 \uc21c\ucc28\uc801\uc778 \uc544\uc774\ud15c \uc0c1\ud638\uc791\uc6a9\uacfc \ud568\uaed8 \ubaa8\ub378\ub9c1\ud558\ub294 \uac83\uc740 \ub9ce\uc740 \uc628\ub77c\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uc810\uc810 \ub354 \ub9ce\uc740 \uad00\uc2ec\uc744 \ubc1b\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc0ac\uc6a9\uc790\uc758 \uacfc\uac70 \uc0c1\ud638\uc791\uc6a9\uc73c\ub85c\ubd80\ud130 \ub3d9\uc801 \uc0ac\uc6a9\uc790 \uad00\uc2ec\uc744 \ud559\uc2b5\ud558\uc5ec \uc544\uc774\ud15c\uc744 \uc81c\uc548\ud558\ub294 \uc21c\ucc28 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc774 \uac1c\ubc1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ub300\ubd80\ubd84\uc758 \uae30\uc874 \uc21c\ucc28 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \uc0c1\ud638\uc791\uc6a9 \ud328\ud134 \uc778\ucf54\ub529 \ud568\uc218\ub294 \ub2e8\uc77c \uc720\ud615\uc758 \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638\uc791\uc6a9\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uc5c8\uc2b5\ub2c8\ub2e4. \uc2e4\uc81c \uc628\ub77c\uc778 \ud50c\ub7ab\ud3fc\uc5d0\uc11c\ub294 \uc0ac\uc6a9\uc790\uc640 \uc544\uc774\ud15c \uac04\uc758 \uc0c1\ud638\uc791\uc6a9 \ud589\ub3d9\uc774 \uc885\uc885 \uc5ec\ub7ec \uc720\ud615(\uc608: \ud074\ub9ad, \uc7a5\ubc14\uad6c\ub2c8\uc5d0 \ucd94\uac00, \uad6c\ub9e4)\uc73c\ub85c \ubcf5\uc7a1\ud55c \uad50\ucc28 \uc720\ud615 \ud589\ub3d9 \uc0c1\ud638 \uc758\uc874\uc131\uc744 \uac16\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9\uc790\uc640 \uc544\uc774\ud15c\uc758 \ub2e4\uc911 \uc720\ud615 \uc0c1\ud638 \uc791\uc6a9 \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c \uc720\uc775\ud55c \ud45c\ud604\uc744 \ud559\uc2b5\ud558\ub294 \uac83\uc740 \uc2dc\uac04 \ubcc0\ud654\ud558\ub294 \uc0ac\uc6a9\uc790 \uc120\ud638\ub3c4\ub97c \uc815\ud655\ud558\uac8c \ud2b9\uc131\ud654\ud558\ub294 \ub370 \ub9e4\uc6b0 \uc911\uc694\ud569\ub2c8\ub2e4. \ubcf8 \uc5f0\uad6c\uc5d0\uc11c\ub294 \ub2e4\uc911 \ud589\ub3d9 \uc0c1\ud638 \uc791\uc6a9 \ud328\ud134\uc5d0 \ub300\ud55c \uc778\uc2dd\uc744 \ubc14\ud0d5\uc73c\ub85c \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uad00\uacc4 \ud559\uc2b5\uc744 \ud574\uacb0\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa9\ud45c\ub97c \ub2ec\uc131\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \uc11c\ub85c \ub2e4\ub978 \uc720\ud615\uc758 \ud589\ub3d9\uc744 \ud1b5\ud574 \ub3d9\uc801 \ub2e8\uae30 \ubc0f \uc7a5\uac70\ub9ac \uc0ac\uc6a9\uc790-\uc544\uc774\ud15c \uc0c1\ud638 \uc791\uc6a9 \ud328\ud134\uc744 \ud568\uaed8 \ud3ec\ucc29\ud558\uae30 \uc704\ud55c \uc0c8\ub85c\uc6b4 \uc2dc\uac04\uc801 \uadf8\ub798\ud504 \ud2b8\ub79c\uc2a4\ud3ec\uba38(TGT) \ucd94\ucc9c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 TGT \ubc29\ubc95\uc740 \uc21c\ucc28 \ucd94\ucc9c \uc544\ud0a4\ud14d\ucc98\uac00 \uc720\ud615\ubcc4 \ud589\ub3d9 \uad00\uacc4 \ucee8\ud14d\uc2a4\ud2b8 \ubc0f \uc554\ubb35\uc801\uc778 \ud589\ub3d9 \uc758\uc874\uc131\uc5d0 \ub300\ud55c \uc804\ub2f4 \uc9c0\uc2dd\uc744 \ucd94\ucd9c\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc2e4\uc81c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c \uc2e4\ud5d8 \uacb0\uacfc, \uc6b0\ub9ac\uc758 \ubc29\ubc95\uc778 TGT\uac00 \ub2e4\uc591\ud55c \ucd5c\ucca8\ub2e8 \ucd94\ucc9c \ubc29\ubc95\ubcf4\ub2e4 \uc77c\uad00\ub418\uac8c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ubaa8\ub378 \uad6c\ud604 \ucf54\ub4dc\ub294 https://github.com/akaxlh/TGT \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      }
    },
    {
      "id": "63b8e5a5-3f2b-4809-ac6d-a47f6e5252d6",
      "title": "A Survey on Multi-Behavior Sequential Recommendation",
      "authors": [
        "Xiaoqing Chen",
        "Zhitao Li",
        "Weike Pan",
        "Zhong Ming"
      ],
      "abstract": "Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.",
      "year": 2023,
      "arxiv_id": "2308.15701",
      "arxiv_url": "https://arxiv.org/abs/2308.15701",
      "conference": null,
      "category": "recsys",
      "tags": [
        {
          "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
          "name": "GCN"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2023-08-30",
      "created_at": "2026-02-04T14:55:20.499680",
      "updated_at": "2026-02-04T14:55:20.499680"
    },
    {
      "id": "14d9d150-68bd-44da-99ae-220683dbd56c",
      "title": "Multi-Behavior Generative Recommendation",
      "authors": [
        "Zihan Liu",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "abstract": "Multi-behavior sequential recommendation (MBSR) aims to incorporate behavior types of interactions for better recommendations. Existing approaches focus on the next-item prediction objective, neglecting the value of integrating the target behavior type into the learning objective. In this paper, we propose MBGen, a novel Multi-Behavior sequential Generative recommendation framework. We formulate the MBSR task into a consecutive two-step process: (1) given item sequences, MBGen first predicts the next behavior type to frame the user intention, (2) given item sequences and a target behavior type, MBGen then predicts the next items. To model such a two-step process, we tokenize both behaviors and items into tokens and construct one single token sequence with both behaviors and items placed interleaved. Furthermore, MBGen learns to autoregressively generate the next behavior and item tokens in a unified generative recommendation paradigm, naturally enabling a multi-task capability. Additionally, we exploit the heterogeneous nature of token sequences in the generative recommendation and propose a position-routed sparse architecture to efficiently and effectively scale up models. Extensive experiments on public datasets demonstrate that MBGen significantly outperforms existing MBSR models across multiple tasks.",
      "year": 2024,
      "arxiv_id": "2405.16871",
      "arxiv_url": "https://arxiv.org/abs/2405.16871",
      "conference": "CIKM'24",
      "category": "recsys",
      "tags": [
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        }
      ],
      "published_at": "2024-05-27",
      "created_at": "2026-02-04T15:45:44.609131",
      "updated_at": "2026-02-04T16:28:17.196022",
      "full_summary": "**\uc5f0\uad6c \ubc30\uacbd**\n\n\uae30\uc874\uc758 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc740 \uc0ac\uc6a9\uc790 \uc0c1\ud638\uc791\uc6a9 \uc720\ud615(\ud074\ub9ad, \uad6c\ub9e4, \uc7a5\ubc14\uad6c\ub2c8 \ucd94\uac00 \ub4f1)\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud558\uace0, \uc8fc\ub85c \ub2e4\uc74c \ud56d\ubaa9 \uc608\uce21\uc5d0 \uc9d1\uc911\ud588\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, Multi-Behavior Sequential Recommendation (MBSR)\ub294 \uc0ac\uc6a9\uc790 \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \ud1b5\ud569\ud558\uc5ec \ucd94\ucc9c \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\uc9c0\ub9cc, \uae30\uc874 \ubaa8\ub378\ub4e4\uc740 \uc608\uce21 \ubaa9\ud45c\uc5d0 \uc9d1\uc911\ud558\uc5ec \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \uc608\uce21\ud558\ub294 \uac83 \uc790\uccb4\uc758 \uac00\uce58(\uc608: \uc0ac\uc6a9\uc790\uc758 \uc758\ub3c4 \ud30c\uc545, \ub9de\ucda4\ud615 \uae30\ub2a5 \uc81c\uacf5)\ub97c \uac04\uacfc\ud588\uc2b5\ub2c8\ub2e4.\n\n**\ud575\uc2ec \uae30\uc5ec**\n\n- \ub450 \ub2e8\uacc4\uc758 \uc5f0\uc18d\uc801\uc778 \ubaa8\ub378\ub9c1: MBSR \uc791\uc5c5\uc744 \ub450 \ub2e8\uacc4\uc758 \uc5f0\uc18d\uc801\uc778 \ud504\ub85c\uc138\uc2a4\ub85c \ubd84\ud574\ud558\uc5ec \ubaa8\ub378\ub9c1\ud568\uc73c\ub85c\uc368, \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \uc608\uce21\ud558\uace0 \ub2e4\uc74c \ud56d\ubaa9\uc744 \uc608\uce21\ud558\ub294 \ub370 \ud544\uc694\ud55c \uc815\ubcf4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud569\ub2c8\ub2e4.\n- \ub370\uc774\ud130 \uc911\uc2ec\uc758 \uc0dd\uc131\uc801 \ubaa8\ub378: \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uacfc \ud56d\ubaa9\uc744 \ud1a0\ud070\ud654\ud558\uc5ec \ud558\ub098\uc758 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub97c \uc0dd\uc131\ud558\uace0, \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378\uc744 \ud1b5\ud574 \ub2e4\uc74c \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uacfc \ud56d\ubaa9\uc744 \uc790\ub3d9 \ud68c\uadc0\uc801\uc73c\ub85c \uc608\uce21\ud558\ub294 \ub370\uc774\ud130 \uc911\uc2ec\uc758 \ubaa8\ub378\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.\n- \ud6a8\uc728\uc801\uc778 \ubaa8\ub378 \ud655\uc7a5:  \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc758 \ubd88\uade0\ud615\ud55c \ud574\uacb0 \ubc29\uc548\uc73c\ub85c, \ud56d\ubaa9 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud574\uacb0 \ubc29\uc548\uc744 \uc81c\uc2dc\ud558\uace0, \uc704\uce58 \uae30\ubc18\uc758 \ud76c\uc18c \ub124\ud2b8\uc6cc\ud06c\ub97c \ud1b5\ud574 \ubaa8\ub378\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ud655\uc7a5\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.\n\n**\ubc29\ubc95\ub860**\n\nMBGen \ubaa8\ub378\uc740 \ub2e4\uc74c\uc758 \ud575\uc2ec \uc544\uc774\ub514\uc5b4\ub97c \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4. \uccab\uc9f8, MBSR \uc791\uc5c5\uc744 \ub450 \ub2e8\uacc4\uc758 \uc5f0\uc18d\uc801\uc778 \ud504\ub85c\uc138\uc2a4\ub85c \ubd84\ud574\ud569\ub2c8\ub2e4. (1) \uccab \ubc88\uc9f8 \ub2e8\uacc4\ub294 \uc0ac\uc6a9\uc790 \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \uc608\uce21\ud558\uace0, (2) \ub450 \ubc88\uc9f8 \ub2e8\uacc4\ub294 \uc608\uce21\ub41c \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \uae30\ubc18\uc73c\ub85c \ub2e4\uc74c \ud56d\ubaa9\uc744 \uc608\uce21\ud569\ub2c8\ub2e4.  \ub450 \ubc88\uc9f8, \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uacfc \ud56d\ubaa9\uc744 \ud1a0\ud070\ud654\ud558\uc5ec \ud558\ub098\uc758 \ud1a0\ud070 \uc2dc\ud000\uc2a4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.  \uc774 \uc2dc\ud000\uc2a4\ub294 \ud56d\ubaa9 \ud1a0\ud070\uacfc \uc0c1\ud638\uc791\uc6a9 \uc720\ud615 \ud1a0\ud070\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.  \uc138 \ubc88\uc9f8,  \uc704\uce58 \uae30\ubc18\uc758 \ud76c\uc18c \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ud655\uc7a5\ud569\ub2c8\ub2e4.  \uc774 \ub124\ud2b8\uc6cc\ud06c\ub294 \uac01 \uc785\ub825\uc774 \ub2e4\ub978 \uc804\ubb38\uac00 \ub124\ud2b8\uc6cc\ud06c\ub85c \ub77c\uc6b0\ud305\ub418\ub3c4\ub85d \uc124\uacc4\ub418\uc5b4 \ubaa8\ub378\uc758 \ud655\uc7a5\uc131\uc744 \ub192\uc774\uace0 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \ud559\uc2b5 \ubaa9\ud45c\ub97c \uc704\ud574 \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uba70, \ub2e4\uc74c \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uacfc \ud56d\ubaa9\uc744 \uc790\ub3d9 \ud68c\uadc0\uc801\uc73c\ub85c \uc608\uce21\ud569\ub2c8\ub2e4.  \ubaa8\ub378\uc740 \ud559\uc2b5 \ubaa9\ud45c\ub97c \uc704\ud574 \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uba70, \ub2e4\uc74c \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uacfc \ud56d\ubaa9\uc744 \uc790\ub3d9 \ud68c\uadc0\uc801\uc73c\ub85c \uc608\uce21\ud569\ub2c8\ub2e4.\n\n\uad00\ub828 \uc5f0\uad6c\n\n\uae30\uc874\uc758 \ucd94\ucc9c \uc2dc\uc2a4\ud15c, \uc0dd\uc131\uc801 \ucd94\ucc9c \ubaa8\ub378, Multi-Behavior Recommendation \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec, MBGen \ubaa8\ub378\uc774 \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \uc608\uce21\ud558\ub294 \ub370 \uc788\uc5b4 \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 \ub354 \ubbf8\uc138\ud55c \uc218\uc900\uc5d0\uc11c \uc0c1\ud638\uc791\uc6a9 \ud328\ud134\uc744 \ucea1\ucc98\ud558\uace0, Multi-Task Capability\ub97c \uc81c\uacf5\ud55c\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \ud2b9\ud788, \uae30\uc874 \ubaa8\ub378\ub4e4\uc774 \uc0c1\ud638\uc791\uc6a9 \uc720\ud615\uc744 \uc608\uce21\ud558\ub294 \ub370 \uc788\uc5b4 \ubbf8\uc138\ud55c \uc218\uc900\uc5d0\uc11c \uc0c1\ud638\uc791\uc6a9 \ud328\ud134\uc744 \ucea1\ucc98\ud558\uace0, Multi-Task Capability\ub97c \uc81c\uacf5\ud55c\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud569\ub2c8\ub2e4."
    }
  ],
  "tags": [
    {
      "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
      "name": "LLM"
    },
    {
      "id": "302859db-9e76-45a8-b984-61f43d889c96",
      "name": "Transformer"
    },
    {
      "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
      "name": "Industrial"
    },
    {
      "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
      "name": "Sequential"
    },
    {
      "id": "3e36ba36-2a7d-4cb5-9b57-83f4e1cee79a",
      "name": "VAE"
    },
    {
      "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
      "name": "GCN"
    },
    {
      "id": "ccd1c3cc-7252-4826-a6e5-0c5748a63ef1",
      "name": "CTR"
    }
  ]
}