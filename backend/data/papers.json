{
  "papers": [
    {
      "id": "6159a7d8-e767-49d6-a740-4fab9a545b57",
      "title": "FuXi-Î²: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model",
      "authors": [
        "Yufei Ye",
        "Wei Guo",
        "Hao Wang",
        "Hong Zhu",
        "Yuyang Ye",
        "Yong Liu",
        "Huifeng Guo",
        "Ruiming Tang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "Scaling laws for autoregressive generative recommenders reveal potential for larger, more versatile systems but mean greater latency and training costs. To accelerate training and inference, we investigated the recent generative recommendation models HSTU and FuXi-$Î±$, identifying two efficiency bottlenecks: the indexing operations in relative temporal attention bias and the computation of the query-key attention map. Additionally, we observed that relative attention bias in self-attention mechanisms can also serve as attention maps. Previous works like Synthesizer have shown that alternative forms of attention maps can achieve similar performance, naturally raising the question of whether some attention maps are redundant. Through empirical experiments, we discovered that using the query-key attention map might degrade the model's performance in recommendation tasks. To address these bottlenecks, we propose a new framework applicable to Transformer-like recommendation models. On one hand, we introduce Functional Relative Attention Bias, which avoids the time-consuming operations of the original relative attention bias, thereby accelerating the process. On the other hand, we remove the query-key attention map from the original self-attention layer and design a new Attention-Free Token Mixer module. Furthermore, by applying this framework to FuXi-$Î±$, we introduce a new model, FuXi-$Î²$. Experiments across multiple datasets demonstrate that FuXi-$Î²$ outperforms previous state-of-the-art models and achieves significant acceleration compared to FuXi-$Î±$, while also adhering to the scaling law. Notably, FuXi-$Î²$ shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale industrial datasets compared to FuXi-$Î±$. Our code is available in a public repository: https://github.com/USTC-StarTeam/FuXi-beta",
      "year": 2025,
      "arxiv_id": "2508.10615",
      "arxiv_url": "https://arxiv.org/abs/2508.10615",
      "category": "recsys",
      "tags": [
        {
          "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
          "name": "Industrial"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-08-14",
      "created_at": "2026-01-29T18:16:45.255432",
      "updated_at": "2026-01-29T18:16:45.255432",
      "conference": null
    },
    {
      "id": "0bc487da-337c-4aaa-a377-55b82880620a",
      "title": "FuXi-$Î±$: Scaling Recommendation Model with Feature Interaction Enhanced Transformer",
      "authors": [
        "Yufei Ye",
        "Wei Guo",
        "Jin Yao Chin",
        "Hao Wang",
        "Hong Zhu",
        "Xi Lin",
        "Yuyang Ye",
        "Yong Liu",
        "Ruiming Tang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "Inspired by scaling laws and large language models, research on large-scale recommendation models has gained significant attention. Recent advancements have shown that expanding sequential recommendation models to large-scale recommendation models can be an effective strategy. Current state-of-the-art sequential recommendation models primarily use self-attention mechanisms for explicit feature interactions among items, while implicit interactions are managed through Feed-Forward Networks (FFNs). However, these models often inadequately integrate temporal and positional information, either by adding them to attention weights or by blending them with latent representations, which limits their expressive power. A recent model, HSTU, further reduces the focus on implicit feature interactions, constraining its performance. We propose a new model called FuXi-$Î±$ to address these issues. This model introduces an Adaptive Multi-channel Self-attention mechanism that distinctly models temporal, positional, and semantic features, along with a Multi-stage FFN to enhance implicit feature interactions. Our offline experiments demonstrate that our model outperforms existing models, with its performance continuously improving as the model size increases. Additionally, we conducted an online A/B test within the Huawei Music app, which showed a $4.76\\%$ increase in the average number of songs played per user and a $5.10\\%$ increase in the average listening duration per user. Our code has been released at https://github.com/USTC-StarTeam/FuXi-alpha.",
      "year": 2025,
      "arxiv_id": "2502.03036",
      "arxiv_url": "https://arxiv.org/abs/2502.03036",
      "category": "recsys",
      "tags": [
        {
          "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
          "name": "Industrial"
        },
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-02-05",
      "created_at": "2026-01-29T18:17:13.221185",
      "updated_at": "2026-02-10T11:27:47.900481",
      "conference": "WWW'25",
      "summary": {
        "one_line": "FuXi-$Î±$ introduces an adaptive multi-channel self-attention mechanism to effectively scale recommendation models by integrating temporal, positional, and semantic features, leading to improved performance.",
        "contribution": "This paper proposes FuXi-$Î±$, a novel recommendation model that distinguishes between temporal, positional, and semantic features through an adaptive multi-channel self-attention mechanism. Furthermore, it utilizes a multi-stage FFN to enhance implicit feature interactions, addressing limitations in existing sequential recommendation models.",
        "methodology": "The model employs an adaptive multi-channel self-attention mechanism to capture diverse feature interactions, allowing for independent modeling of temporal, positional, and semantic information. A multi-stage FFN is integrated to refine implicit feature interactions, improving the model's ability to capture nuanced relationships between items.",
        "results": "Offline experiments demonstrate FuXi-$Î±$'s superior performance across increasing model sizes, while an online A/B test within Huawei Music resulted in significant improvements: a 4.76% increase in songs played and a 5.10% increase in listening duration."
      },
      "full_summary": "**ì—°êµ¬ ë°°ê²½**\n\nìµœê·¼ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ê³µì„ ë°”íƒ•ìœ¼ë¡œ, ì¶”ì²œ ëª¨ë¸ì˜ ê·œëª¨ë¥¼ í™•ì¥í•˜ëŠ” ê²ƒì€ íš¨ê³¼ì ì¸ ì „ëµìœ¼ë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ìˆœì°¨ ì¶”ì²œ ëª¨ë¸ì€ ì£¼ë¡œ ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ í•­ëª© ê°„ì˜ ëª…ì‹œì  íŠ¹ì§• ìƒí˜¸ ì‘ìš©ì„ ì²˜ë¦¬í•˜ê³ , í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(FFN)ë¥¼ í†µí•´ ì•”ì‹œì  ìƒí˜¸ ì‘ìš©ì„ ê´€ë¦¬í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ëª¨ë¸ì€ ì‹œê°„ì  ë° ìœ„ì¹˜ì  ì •ë³´ë¥¼ ì¶©ë¶„íˆ í™œìš©í•˜ì§€ ëª»í•˜ì—¬ í‘œí˜„ë ¥ì´ ì œí•œì ì´ë©°, HSTU ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì´ëŸ¬í•œ í•œê³„ë¡œ ì¸í•´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n**í•µì‹¬ ê¸°ì—¬**\n\n- ìƒˆë¡œìš´ ëª¨ë¸, FuXi-ğ›¼ë¥¼ ì œì•ˆí•˜ì—¬ ê·œëª¨ í™•ì¥ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n- Adaptive Multi-channel Self-attention (AMS) ë ˆì´ì–´ë¥¼ ë„ì…í•˜ì—¬ ì‹œê°„ì , ìœ„ì¹˜ì , ì˜ë¯¸ì  íŠ¹ì§•ì„ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì´ëŠ” ì‹œê°„ì  ë° ìœ„ì¹˜ì  ì •ë³´ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n- Multi-stage Feedforward Network (MFFN) ë ˆì´ì–´ë¥¼ í†µí•©í•˜ì—¬ ì•”ì‹œì  íŠ¹ì§• ìƒí˜¸ ì‘ìš©ì„ ê°•í™”í•©ë‹ˆë‹¤.\n\n**ë°©ë²•ë¡ **\n\nFuXi-ğ›¼ëŠ” ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì‹œê°„ì , ìœ„ì¹˜ì , ì˜ë¯¸ì  íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ AMS ë ˆì´ì–´ì™€ MFFN ë ˆì´ì–´ë¥¼ í†µí•©í•©ë‹ˆë‹¤. AMS ë ˆì´ì–´ëŠ” ê° íŠ¹ì§• ìœ í˜•ì— ëŒ€í•œ ë…ë¦½ì ì¸ attention ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ì—¬ ì‹œê°„ì , ìœ„ì¹˜ì , ì˜ë¯¸ì  íŠ¹ì§• ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ ë³´ë‹¤ ì •í™•í•˜ê²Œ ëª¨ë¸ë§í•©ë‹ˆë‹¤. MFFN ë ˆì´ì–´ëŠ” ì´ëŸ¬í•œ ìƒí˜¸ ì‘ìš©ì˜ ê²°ê³¼ë¥¼ ì¶”ê°€ë¡œ ì²˜ë¦¬í•˜ì—¬ ì•”ì‹œì  íŠ¹ì§• ìƒí˜¸ ì‘ìš©ì„ ê°•í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ í¬ê²Œ ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì²«ì§¸, ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ê° í•­ëª©ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ë‘˜ì§¸, AMS ë ˆì´ì–´ë¥¼ í†µí•´ ì‹œê°„ì , ìœ„ì¹˜ì , ì˜ë¯¸ì  íŠ¹ì§• ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì…‹ì§¸, MFFN ë ˆì´ì–´ë¥¼ í†µí•´ ëª¨ë¸ë§ëœ íŠ¹ì§• ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ ì¶”ê°€ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì˜ ê·œëª¨ í™•ì¥ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í•˜ê¸° ìœ„í•´ ëŒ€ê·œëª¨ ì‚°ì—… ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ëª¨ë¸ì˜ ê·œëª¨ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë˜ì—ˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
    },
    {
      "id": "e21a2e83-325b-4b34-bf15-d4acd105742b",
      "title": "Rethinking Overconfidence in VAEs: Can Label Smoothing Help?",
      "authors": [
        "Woo-Seong Yun",
        "Yeojun Choi",
        "Yoonsik Cho"
      ],
      "abstract": "By leveraging the expressive power of deep generative models, Variational Autoencoder (VAE)-based recommender models have demonstrated competitive performance. However, deep neural networks (DNNs) tend to exhibit overconfidence in their predictive distributions as training progresses. This issue is further exacerbated by two inherent characteristics of collaborative filtering (CF): (1) extreme data sparsity and (2) implicit feedback. Despite its importance, there has been a lack of systematic study into this problem. To fill the gap, this paper explores the above limitations with label smoothing (LS) from both theoretical and empirical aspects. Our extensive analysis demonstrates that overconfidence leads to embedding collapse, where latent representations collapse into a narrow subspace. Furthermore, we investigate the conditions under which LS helps recommendation, and observe that the optimal LS factor decreases proportionally with data sparsity. To the best of our knowledge, this is the first study in VAE-based CF that discovers the relationship between overconfidence and embedding collapse, and highlights the necessity of explicitly addressing them. Our code is available at https://github.com/yunwooseong/RethinkVAE.",
      "year": 2025,
      "arxiv_id": null,
      "arxiv_url": null,
      "doi": "10.1145/3705328.3748039",
      "paper_url": "https://dl.acm.org/doi/10.1145/3705328.3748039",
      "category": "recsys",
      "tags": [
        {
          "id": "5042e1cb-7e74-4fd0-8f81-f07d7cb24a9f",
          "name": "Deep Learning"
        },
        {
          "id": "3e36ba36-2a7d-4cb5-9b57-83f4e1cee79a",
          "name": "VAE"
        }
      ],
      "published_at": "2025-09-22",
      "created_at": "2026-01-29T18:17:24.423618",
      "updated_at": "2026-01-29T18:17:24.423618",
      "conference": "RecSys'25"
    },
    {
      "id": "3b59362b-ee9a-43db-8384-8ad5564f013a",
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": [
        "Shunyu Yao",
        "Dian Yu",
        "Jeffrey Zhao",
        "Izhak Shafran",
        "Thomas L. Griffiths",
        "Yuan Cao",
        "Karthik Narasimhan"
      ],
      "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
      "year": 2023,
      "arxiv_id": "2305.10601",
      "arxiv_url": "https://arxiv.org/abs/2305.10601",
      "category": "recsys",
      "tags": [
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2023-05-17",
      "created_at": "2026-01-29T18:17:44.088329",
      "updated_at": "2026-01-29T18:17:44.088329",
      "conference": "NeurIPS'23"
    },
    {
      "id": "1290e19a-7664-4a23-8abb-c9d7d0d26d2c",
      "title": "FuXi-$Î³$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism",
      "authors": [
        "Dezhi Yi",
        "Wei Guo",
        "Wenyang Cui",
        "Wenxuan He",
        "Huifeng Guo",
        "Yong Liu",
        "Zhenhua Dong",
        "Ye Lu"
      ],
      "abstract": "Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$Î³$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$Î³$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$Î³$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\\times$ and inference by up to 6.18$\\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.",
      "year": 2025,
      "arxiv_id": "2512.12740",
      "arxiv_url": "https://arxiv.org/abs/2512.12740",
      "category": "recsys",
      "tags": [
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-12-14",
      "created_at": "2026-01-29T18:20:45.141059",
      "updated_at": "2026-01-30T14:38:25.687459",
      "conference": null,
      "summary": {
        "one_line": "FuXi-$Î³$ presents a computationally efficient sequential recommendation framework leveraging an exponential-power temporal encoder and diagonal-sparse positional mechanism for improved performance and scalability.",
        "contribution": "This work introduces a novel architecture for sequential recommendation that addresses the computational bottlenecks of traditional Transformer models. Specifically, FuXi-$Î³$ employs an exponential-power temporal encoder to effectively model temporal dependencies and a diagonal-sparse positional mechanism to reduce computational complexity.",
        "methodology": "The framework utilizes a decoder-only Transformer structure with an exponential-power temporal encoder, which encodes relative temporal intervals using a tunable decay function. Furthermore, a diagonal-sparse positional mechanism prunes attention blocks via a diagonal-sliding strategy based on Toeplitz matrix persymmetry, optimizing memory access.",
        "results": "Experiments on multiple datasets demonstrate FuXi-$Î³$'s state-of-the-art recommendation quality, achieving significant speedups in training (up to 4.74x) and inference (up to 6.18x), highlighting its practical scalability."
      },
      "translation": {
        "title": "ì œëª©: FuXi-$Î³$: ì—‘ìŠ¤í¬ë„ŒíŠ¸-íŒŒì›Œ ì‹œê°„ ì¸ì½”ë”ì™€ ëŒ€ê°-í¬ì†Œ ìœ„ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì´ìš©í•œ íš¨ìœ¨ì ì¸ ìˆœì°¨ ì¶”ì²œ",
        "abstract": "ì´ˆë¡: ìˆœì°¨ ì¶”ì²œì€ ì‚¬ìš©ìì˜ ì—­ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í™”í•˜ëŠ” ì„ í˜¸ë„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ìµœê·¼ì˜ ë°œì „ì€ ì „ì—­ ì˜ì¡´ì„±ì„ í¬ì°©í•˜ê¸° ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ì€ ì‹œê°„ ì¸ì½”ë”©ì˜ ë¶ˆì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ì ‘ê·¼ê³¼ ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë¹½ë¹½í•œ ì–´í…ì…˜ìœ¼ë¡œ ì¸í•´ ë†’ì€ ê³„ì‚° ì˜¤ë²„í—¤ë“œë¥¼ ê²ªëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œ ì‚¬í•­ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì›ì¹™ì— ê¸°ë°˜í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ë¥¼ í†µí•´ íš¨ê³¼ì™€ íš¨ìœ¨ì„±ì„ ëª¨ë‘ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ìˆœì°¨ ì¶”ì²œ í”„ë ˆì„ì›Œí¬ì¸ FuXi-$Î³$ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. FuXi-$Î³$ëŠ” ë””ì½”ë”ë§Œ ì‚¬ìš©í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ë‘ ê°€ì§€ ì£¼ìš” í˜ì‹ ì„ ë„ì…í•©ë‹ˆë‹¤. (1) Ebbinghaus ë§ê° ê³¡ì„ ì—ì„œ ì˜ê°ì„ ë°›ì€ ì¡°ì • ê°€ëŠ¥í•œ ì§€ìˆ˜ ê°ì‡  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒëŒ€ì ì¸ ì‹œê°„ ê°„ê²©ì„ ì¸ì½”ë”©í•˜ëŠ” ì—‘ìŠ¤í¬ë„ŒíŠ¸-íŒŒì›Œ ì‹œê°„ ì¸ì½”ë”ì…ë‹ˆë‹¤. ì´ ì¸ì½”ë”ëŠ” ë†’ì€ íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë‹¨ê¸° ë° ì¥ê¸° ì„ í˜¸ë„ë¥¼ ìœ ì—°í•˜ê²Œ ëª¨ë¸ë§í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ëŠ” ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ì ‘ê·¼ê³¼ ìˆœìˆ˜í•œ í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (2) Toeplitz í–‰ë ¬ì˜ ëŒ€ì¹­ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚®ì€ ê¸°ì—¬ë„ë¥¼ ê°€ì§„ ì–´í…ì…˜ ë¸”ë¡ì„ ê°€ì§€ì¹˜ê¸°í•˜ëŠ” ëŒ€ê°-í¬ì†Œ ìœ„ì¹˜ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ë„¤ ê°œì˜ ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼, FuXi-$Î³$ëŠ” ì¶”ì²œ í’ˆì§ˆì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë™ì‹œì— í•™ìŠµ ì‹œê°„ì„ ìµœëŒ€ 4.74ë°° ë° ì¶”ë¡  ì‹œê°„ì„ ìµœëŒ€ 6.18ë°° ê°€ì†í™”í•˜ì—¬ ê¸´ ì‹œí€€ìŠ¤ ì¶”ì²œì— ëŒ€í•œ ì‹¤ìš©ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì½”ë“œëŠ” https://github.com/Yeedzhi/FuXi-gamma ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      },
      "full_summary": "## FuXi-ğ›¾: íš¨ìœ¨ì ì¸ ìˆœì°¨ ì¶”ì²œ í”„ë ˆì„ì›Œí¬ ìš”ì•½ (í•œêµ­ì–´)\n\n**1. ì—°êµ¬ ë°°ê²½ ë° ë™ê¸° (Introduction & Motivation)**\n\nFuXi-ğ›¾ëŠ” ì‚¬ìš©ìë“¤ì˜ ì§„í™”í•˜ëŠ” ì„ í˜¸ë„ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ìˆœì°¨ ì¶”ì²œ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ê³¼ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìˆœì°¨ ì¶”ì²œ ëª¨ë¸ë“¤ì€ ë³µì¡í•œ êµ¬ì¡°ì™€ ë†’ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ì¸í•´ í™•ì¥ì„±ì´ ë–¨ì–´ì§€ê³ , íŠ¹íˆ ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë¹„íš¨ìœ¨ì ì¸ ê²½ìš°ê°€ ë§ì•˜ìŠµë‹ˆë‹¤. FuXi-ğ›¾ëŠ” ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ê³ , ê³„ì‚° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. íŠ¹íˆ, ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , ì‹¤ìš©ì ì¸ ì ìš© ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n**2. í•µì‹¬ êµ¬ì„± ìš”ì†Œ**\n\nFuXi-ğ›¾ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n\n*   **ì‹œê°„ ì¸ì½”ë” (Temporal Encoder):** Ebbinghaus ë§ê° ê³¡ì„ ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì–´, ì‚¬ìš©ì ì„ í˜¸ë„ ë³€í™”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.  ì´ë¥¼ í†µí•´ ì‹œê°„ ì •ë³´ì— ë”°ë¥¸ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ì„ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ê³ ,  ë‹¤ì–‘í•œ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ì— ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n*   **ìœ„ì¹˜ ì¸ì½”ë” (Positional Encoder):**  ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬,  ê° í•­ëª© ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.  ì´ëŠ” íŠ¹íˆ,  ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ì—ì„œ í•­ëª© ê°„ì˜ ìˆœì„œì  ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n*   **Transformer ë¸”ë¡:**  ê¸°ì¡´ì˜ Transformer êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬,  ëª¨ë¸ì˜ í‘œí˜„ ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n\n**3. ì£¼ìš” ê¸°ì—¬ ë° ê²°ê³¼**\n\n*   **ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ ì„¤ê³„:** FuXi-ğ›¾ëŠ” ì‚¬ìš©ì ì„ í˜¸ë„ ëª¨ë¸ë§ê³¼ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë™ì‹œì— í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ìˆœì°¨ ì¶”ì²œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n*   **ì‹œê°„ ì¸ì½”ë”ì˜ í˜ì‹ ì ì¸ ì„¤ê³„:** Ebbinghaus ë§ê° ê³¡ì„ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹œê°„ ì¸ì½”ë”ëŠ” ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.\n*   **ìœ„ì¹˜ ì¸ì½”ë”ì˜ íš¨ìœ¨ì ì¸ í™œìš©:**  ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬,  ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n*   **ì‹¤í—˜ì  ê²€ì¦:**  ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´, FuXi-ğ›¾ê°€ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ,  ëŒ€ê·œëª¨ ìŒì•… ë°ì´í„°ì…‹ì—ì„œ FuXi-ğ›¾ëŠ” HR@10ì—ì„œ 25.06%ì˜ ê°œì„  íš¨ê³¼ì™€ NDCG@10ì—ì„œ 42.86%ì˜ ê°œì„  íš¨ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.\n\n**ìš”ì•½:** FuXi-ğ›¾ëŠ” íš¨ìœ¨ì ì¸ ìˆœì°¨ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ í˜ì‹ ì ì¸ í”„ë ˆì„ì›Œí¬ë¡œì„œ, ì‚¬ìš©ì ì„ í˜¸ë„ ëª¨ë¸ë§ì˜ ì •í™•ì„±ê³¼ ì‹¤ìš©ì„±ì„ ë™ì‹œì— í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤."
    },
    {
      "id": "7cd0cabf-95fb-449b-a16a-416cedab060d",
      "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
      "authors": [
        "Jiaqi Zhai",
        "Lucy Liao",
        "Xing Liu",
        "Yueming Wang",
        "Rui Li",
        "Xuan Cao",
        "Leon Gao",
        "Zhaojie Gong",
        "Fangda Gu",
        "Michael He",
        "Yinghai Lu",
        "Yu Shi"
      ],
      "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (\"Generative Recommenders\"), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.",
      "year": 2024,
      "arxiv_id": "2402.17152",
      "arxiv_url": "https://arxiv.org/abs/2402.17152",
      "conference": "ICML'24",
      "category": "recsys",
      "tags": [
        {
          "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
          "name": "Industrial"
        },
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        }
      ],
      "published_at": "2024-02-27",
      "created_at": "2026-01-29T18:36:45.020247",
      "updated_at": "2026-01-30T18:21:07.012366",
      "summary": {
        "one_line": "This paper introduces HSTU, a trillion-parameter generative recommender model leveraging sequential transduction to achieve state-of-the-art performance and scalability in recommendation systems.",
        "contribution": "The research reformulates recommendation as a sequential transduction task, enabling the development of Generative Recommenders. HSTU, a novel architecture with 1.5 trillion parameters, demonstrates improved recommendation performance and scalability compared to existing methods.",
        "methodology": "HSTU utilizes a Transformer-based architecture optimized for high-cardinality, streaming data, employing a sequential transduction approach. The model's performance scales with training compute, mirroring the scaling trends observed in large language models like GPT-3/LLaMa-2.",
        "results": "HSTU achieves up to 65.8% improvements in NDCG across synthetic and public datasets, while also demonstrating a 5.3x to 15.2x speedup over FlashAttention2-based Transformers.  Furthermore, the model's performance improved by 12.4% in online A/B tests, indicating practical applicability."
      }
    },
    {
      "id": "efb35aed-82e1-4303-a8d7-03afd89ef23f",
      "title": "LightKG: Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture",
      "authors": [
        "Yanhui Li",
        "Dongxia Wang",
        "Zhu Sun",
        "Haonan Zhang",
        "Huizhong Guo"
      ],
      "abstract": "Recently, Graph Neural Networks (GNNs) have become the dominant approach for Knowledge Graph-aware Recommender Systems (KGRSs) due to their proven effectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL) has been incorporated to address the sparity issue, leading to longer training time. However, through extensive experiments, we reveal that: (1)compared to other KGRSs, the existing GNN-based KGRSs fail to keep their superior performance under sparse interactions even with SSL. (2) More complex models tend to perform worse in sparse interaction scenarios and complex mechanisms, like attention mechanism, can be detrimental as they often increase learning difficulty. Inspired by these findings, we propose LightKG, a simple yet powerful GNN-based KGRS to address sparsity issues. LightKG includes a simplified GNN layer that encodes directed relations as scalar pairs rather than dense embeddings and employs a linear aggregation framework, greatly reducing the complexity of GNNs. Additionally, LightKG incorporates an efficient contrastive layer to implement SSL. It directly minimizes the node similarity in original graph, avoiding the time-consuming subgraph generation and comparison required in previous SSL methods. Experiments on four benchmark datasets show that LightKG outperforms 12 competitive KGRSs in both sparse and dense scenarios while significantly reducing training time. Specifically, it surpasses the best baselines by an average of 5.8% in recommendation accuracy and saves 84.3% of training time compared to KGRSs with SSL. Our code is available at https://github.com/1371149/LightKG.",
      "year": 2025,
      "arxiv_id": "2506.10347",
      "arxiv_url": "https://arxiv.org/abs/2506.10347",
      "doi": "10.1145/3711896.3737026",
      "paper_url": "https://dl.acm.org/doi/10.1145/3711896.3737026",
      "conference": "KDD'25",
      "category": "recsys",
      "tags": [
        {
          "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
          "name": "GCN"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2025-06-12",
      "created_at": "2026-01-29T18:45:57.419063",
      "updated_at": "2026-02-09T17:45:52.648267",
      "summary": {
        "one_line": "LightKG presents a simplified GNN-based Knowledge Graph Recommender System (KGRS) that achieves superior performance and efficiency in sparse interaction scenarios.",
        "contribution": "This paper demonstrates that complex GNN-based KGRSs fail to maintain performance under sparsity, highlighting the detrimental effects of intricate mechanisms like attention. LightKG introduces a simplified GNN layer and an efficient contrastive learning approach to directly minimize node similarity, addressing the limitations of existing methods.",
        "methodology": "LightKG utilizes a simplified GNN layer to encode directed relations as scalar pairs, reducing computational complexity. It employs a linear aggregation framework and incorporates a contrastive layer for self-supervised learning, directly minimizing node similarity within the original graph.",
        "results": "Experiments on benchmark datasets reveal that LightKG outperforms 12 competitive KGRSs in both sparse and dense scenarios, achieving a 5.8% average improvement in recommendation accuracy and reducing training time by 84.3% compared to SSL-based KGRSs."
      },
      "full_summary": "ìš”ì•½\n\nì´ ë…¼ë¬¸ì€ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸, LightKGë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë³µì¡í•œ ëª¨ë¸ë“¤ì´ í¬ì†Œí•œ ìƒí˜¸ì‘ìš© ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì´ëŠ” ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´, LightKGëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ì ì¸ íŠ¹ì§•ì„ ê°–ìŠµë‹ˆë‹¤.\n- ë‹¨ìˆœí™”ëœ ê´€ê³„ ì¸ì½”ë”©: ë³µì¡í•œ ê´€ê³„ ì¸ì½”ë”© ë°©ì‹ì„ ë‹¨ìˆœí™”í•˜ì—¬ ê³„ì‚° ë³µì¡ë„ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤.\n- ì„ í˜• ì •ë³´ ì§‘ê³„ í”„ë ˆì„ì›Œí¬: ì„ í˜• ì •ë³´ ì§‘ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.\n- íš¨ìœ¨ì ì¸ ëŒ€ë¹„ ë ˆì´ì–´: ëŒ€ë¹„ ë ˆì´ì–´ì˜ ìˆ˜ë¥¼ ì¤„ì—¬ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.\n\nLightKGëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹(Last.FM, ML-1M)ì—ì„œ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ í¬ì†Œí•œ ìƒí˜¸ì‘ìš© ë°ì´í„°ì—ì„œ ë›°ì–´ë‚œ íš¨ìœ¨ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.  ë˜í•œ, ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ attention ë©”ì»¤ë‹ˆì¦˜ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\ní•µì‹¬ ê²°ê³¼:\n- LightKGëŠ” ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ 1.4% ~ 11.7% ë” ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\n- LightKGëŠ” ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ 16.8% ~ 82.7% ë” ì ì€ í•™ìŠµ ì‹œê°„ì„ ì†Œìš”í–ˆìŠµë‹ˆë‹¤.\n- ê¸°ì¡´ì˜ ë³µì¡í•œ ëª¨ë¸ë“¤ì´ í¬ì†Œ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì´ëŠ” ë¬¸ì œì ì„ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\n\nì´ ë…¼ë¬¸ì€ ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì ì¸ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•˜ë©°, íŠ¹íˆ í¬ì†Œí•œ ìƒí˜¸ì‘ìš© ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ ê°œì„ ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤."
    },
    {
      "id": "ac2c1807-b63d-4510-af12-30d3d58d04e3",
      "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
      "authors": [
        "Xiangnan He",
        "Kuan Deng",
        "Xiang Wang",
        "Yan Li",
        "Yongdong Zhang",
        "Meng Wang"
      ],
      "abstract": "Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.   In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
      "year": 2020,
      "arxiv_id": "2002.02126",
      "arxiv_url": "https://arxiv.org/abs/2002.02126",
      "conference": "SIGIR'20",
      "category": "recsys",
      "tags": [
        {
          "id": "5042e1cb-7e74-4fd0-8f81-f07d7cb24a9f",
          "name": "Deep Learning"
        },
        {
          "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
          "name": "GCN"
        }
      ],
      "published_at": "2020-02-06",
      "created_at": "2026-01-30T11:16:47.921484",
      "updated_at": "2026-01-30T15:07:15.534327",
      "summary": {
        "one_line": "LightGCN simplifies GCN for recommendation by removing unnecessary components, achieving significant performance improvements through linear neighborhood aggregation.",
        "contribution": "This paper demonstrates that feature transformation and nonlinear activations in GCNs contribute minimally to collaborative filtering performance. LightGCN's core innovation is a linear neighborhood aggregation approach, resulting in a more concise and efficient model.",
        "methodology": "LightGCN learns user and item embeddings via linear propagation on the user-item graph, employing a weighted sum of layer embeddings as the final representation. This eliminates complex neural network operations, focusing solely on neighborhood aggregation for improved efficiency.",
        "results": "LightGCN achieves an average relative improvement of 16.0% over NGCF under identical experimental conditions, highlighting the effectiveness of its simplified design and linear propagation strategy."
      },
      "translation": {
        "title": "ì œëª©: LightGCN: ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¨ìˆœí™”í•˜ê³  ì¶”ì²œì„ ìœ„í•œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•",
        "abstract": "ì´ˆë¡: ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬(GCN)ëŠ” í˜‘ì—… í•„í„°ë§ ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê¸°ìˆ ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¶”ì²œì„ ìœ„í•œ ê·¸ íš¨ê³¼ì˜ ì´ìœ ëŠ” ì¶©ë¶„íˆ ì´í•´ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ GCNì„ ì¶”ì²œì— ì ìš©í•œ ì—°êµ¬ë“¤ì€ GCNì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì„ ëª…í™•í•˜ê²Œ ë¶„ì„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. GCNì€ ì›ë˜ ê·¸ë˜í”„ ë¶„ë¥˜ ì‘ì—…ì— ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ë§ì€ ì‹ ê²½ë§ ì—°ì‚°ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” GCNì˜ ê°€ì¥ í”í•œ ë‘ ê°€ì§€ ì„¤ê³„ â€“ íŠ¹ì§• ë³€í™˜ ë° ë¹„ì„ í˜• í™œì„±í™” â€“ ê°€ í˜‘ì—… í•„í„°ë§ì˜ ì„±ëŠ¥ì— í¬ê²Œ ê¸°ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ê²½í—˜ì ìœ¼ë¡œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë” ë‚˜ì•„ê°€ ì´ëŸ¬í•œ ìš”ì†Œë¥¼ í¬í•¨í•˜ë©´ í•™ìŠµì˜ ì–´ë ¤ì›€ì´ ì¦ê°€í•˜ê³  ì¶”ì²œ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” GCNì˜ ì„¤ê³„ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ë”ìš± ê°„ê²°í•˜ê³  ì¶”ì²œì— ì í•©í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” í˜‘ì—… í•„í„°ë§ì„ ìœ„í•´ GCNì˜ ê°€ì¥ í•„ìˆ˜ì ì¸ êµ¬ì„± ìš”ì†Œì¸ â€˜ì´ì›ƒ ì§‘ê³„(neighborhood aggregation)â€™ë§Œì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì¸ LightGCNì„ ì œì•ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, LightGCNì€ ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ ì‘ìš© ê·¸ë˜í”„ì—ì„œ ì„ í˜•ì ìœ¼ë¡œ ì‚¬ìš©ì ë° ì•„ì´í…œ ì„ë² ë”©ì„ í•™ìŠµí•˜ê³ , ëª¨ë“  ë ˆì´ì–´ì—ì„œ í•™ìŠµëœ ì„ë² ë”©ì˜ ê°€ì¤‘ í•©ì„ ìµœì¢… ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¨ìˆœí•˜ê³  ì„ í˜•ì ì´ë©° ê¹”ë”í•œ ëª¨ë¸ì€ Neural Graph Collaborative Filtering (NGCF) â€“ ìµœì²¨ë‹¨ GCN ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ â€“ ë³´ë‹¤ êµ¬í˜„ ë° í•™ìŠµì´ í›¨ì”¬ ì‰¬ìš°ë©°, ì •í™•íˆ ë™ì¼í•œ ì‹¤í—˜ ì„¤ì • í•˜ì—ì„œ í‰ê· ì ìœ¼ë¡œ ì•½ 16.0%ì˜ ìƒëŒ€ì  ê°œì„ ì„ ë³´ì…ë‹ˆë‹¤. LightGCNì˜ ë‹¨ìˆœì„±ì— ëŒ€í•œ ë¶„ì„ì  ë° ê²½í—˜ì  ê´€ì ì—ì„œ í•©ë¦¬ì„±ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì¶”ê°€ ë¶„ì„ì´ ì œê³µë©ë‹ˆë‹¤."
      },
      "full_summary": "ì´ ë…¼ë¬¸ì€ ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš© ê·¸ë˜í”„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œì¸ NGCFì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, NGCFì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ íŠ¹ì§• ë³€í™˜(feature transformation)ê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜(non-linear activation function)ê°€ ì¶”ì²œ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤.\n\nì£¼ìš” ë‚´ìš©:\n- NGCF ë¶„ì„: NGCFì˜ êµ¬ì¡°ì™€ ì‘ë™ ë°©ì‹ì„ ì„¤ëª…í•˜ê³ , íŠ¹ì§• ë³€í™˜ê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ê°€ ì¶”ì²œ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n- Ablation Study: NGCFì˜ ì„¸ ê°€ì§€ ë³€í˜• ëª¨ë¸(NGCF-f, NGCF-n, NGCF-fn)ì„ êµ¬ì¶•í•˜ê³ , ê°ê°ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n- ê²°ê³¼: ì‹¤í—˜ ê²°ê³¼, íŠ¹ì§• ë³€í™˜ê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ê°€ ì¶”ì²œ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤. íŠ¹íˆ, íŠ¹ì§• ë³€í™˜ì„ ì œê±°í•œ NGCF-fn ëª¨ë¸ì´ NGCF ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n- ê²°ë¡ : ì¶”ì²œ ì‹œìŠ¤í…œ ì„¤ê³„ ì‹œ, ë¶ˆí•„ìš”í•œ ë³µì¡ì„±ì„ ì¤„ì´ê³ , ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ íŠ¹ì§• ë³€í™˜ê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•´ì•¼ í•¨ì„ ê°•ì¡°í•©ë‹ˆë‹¤.\n\ní•µì‹¬ ê¸°ì—¬:\n- NGCFì˜ ì„±ëŠ¥ ì €í•˜ ì›ì¸ì„ ëª…í™•í•˜ê²Œ ë°í˜€ëƒ„ìœ¼ë¡œì¨, ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì„¤ê³„ì— ëŒ€í•œ ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.\n- ì‹¤í—˜ì ì¸ ë¶„ì„ì„ í†µí•´, ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê³ , ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.\n\nì´ ë…¼ë¬¸ì€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì„¤ê³„ì— ëŒ€í•œ ì¤‘ìš”í•œ ì—°êµ¬ ê²°ê³¼ë¥¼ ì œê³µí•˜ë©°, í–¥í›„ ê´€ë ¨ ì—°êµ¬ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤."
    },
    {
      "id": "d51b6643-d2b5-431f-8df0-0b688e7d0f94",
      "title": "KGAT: Knowledge Graph Attention Network for Recommendation",
      "authors": [
        "Xiang Wang",
        "Xiangnan He",
        "Yixin Cao",
        "Meng Liu",
        "Tat-Seng Chua"
      ],
      "abstract": "To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism.",
      "year": 2019,
      "arxiv_id": "1905.07854",
      "arxiv_url": "https://arxiv.org/abs/1905.07854",
      "conference": "KDD'19",
      "category": "recsys",
      "tags": [
        {
          "id": "5042e1cb-7e74-4fd0-8f81-f07d7cb24a9f",
          "name": "Deep Learning"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2019-05-20",
      "created_at": "2026-01-30T11:16:48.668808",
      "updated_at": "2026-01-30T12:03:45.416635",
      "summary": {
        "one_line": "KGAT introduces a Knowledge Graph Attention Network that leverages high-order item relationships to enhance recommendation accuracy and diversity.",
        "contribution": "This paper proposes KGAT, a novel method that incorporates knowledge graphs to model complex item relationships beyond traditional user-item interactions. Specifically, KGAT utilizes embedding propagation and an attention mechanism to capture high-order connections between items and attributes, addressing the limitations of existing methods.",
        "methodology": "KGAT recursively propagates embeddings from neighboring nodes (users, items, or attributes) within the knowledge graph. An attention mechanism is then applied to weight the importance of these neighbors during the embedding refinement process, allowing the model to prioritize relevant connections.",
        "results": "Experimental results on public benchmarks demonstrate that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet, showcasing the effectiveness of high-order relation modeling and the attention mechanism in recommendation."
      },
      "translation": {
        "title": "ì œëª©: KGAT: ì§€ì‹ ê·¸ë˜í”„ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ì¶”ì²œ",
        "abstract": "ì´ˆë¡: ë³´ë‹¤ ì •í™•í•˜ê³ , ë‹¤ì–‘í•˜ë©°, ì„¤ëª… ê°€ëŠ¥í•œ ì¶”ì²œì„ ì œê³µí•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ë¶€ê°€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.  íŒ©í„°ë¼ì´ì œì´ì…˜ ë¨¸ì‹ (FM)ê³¼ ê°™ì€ ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì´ë¥¼ ì§€ë„ í•™ìŠµ ë¬¸ì œë¡œ ê°„ì£¼í•˜ë©°, ê° ìƒí˜¸ì‘ìš©ì„ ë…ë¦½ì ì¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ ê°€ì •í•˜ê³  ë¶€ê°€ ì •ë³´ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.  ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” ì•„ì´í…œ ê°„ì˜ ê´€ê³„(ì˜ˆ: ì˜í™”ì˜ ê°ë…ì´ ë‹¤ë¥¸ ì˜í™”ì˜ ë°°ìš°)ë¥¼ ê°„ê³¼í•˜ëŠ” ë°ë‹¤, ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ì‚¬ìš©ìë“¤ì˜ ì§‘ë‹¨ í–‰ë™ì—ì„œ í˜‘ì—… ì‹ í˜¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  ë³¸ ì—°êµ¬ì—ì„œëŠ” ì§€ì‹ ê·¸ë˜í”„(KG)ì˜ ìœ ìš©ì„±ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. KGëŠ” ì†ì„±ì„ í†µí•´ ì•„ì´í…œì„ ì—°ê²°í•¨ìœ¼ë¡œì¨ ë…ë¦½ì ì¸ ìƒí˜¸ì‘ìš© ê°€ì •ì— ë„ì „í•©ë‹ˆë‹¤.  KGì™€ ì‚¬ìš©ì-ì•„ì´í…œ ê·¸ë˜í”„ì˜ ì´ëŸ¬í•œ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ì—ì„œ ë‘ ì•„ì´í…œì„ ì—°ê²°í•˜ëŠ” í•˜ë‚˜ ì´ìƒì˜ ì—°ê²°ëœ ì†ì„±ì„ í†µí•´ ë‚˜íƒ€ë‚˜ëŠ” ê³ ì°¨ì› ê´€ê³„ëŠ” ì„±ê³µì ì¸ ì¶”ì²œì— í•„ìˆ˜ì ì¸ ìš”ì†Œë¼ëŠ” ì£¼ì¥ì„ í¼ì¹©ë‹ˆë‹¤.  ì €í¬ëŠ” ì§€ì‹ ê·¸ë˜í”„ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬(KGAT)ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. KG ë‚´ì˜ ê³ ì°¨ì› ì—°ê²°ì„±ì„ ì „ì´ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.  KGì˜ ë…¸ë“œ ì´ì›ƒ(ì‚¬ìš©ì, ì•„ì´í…œ ë˜ëŠ” ì†ì„±)ì—ì„œ ì„ë² ë”©ì„ ì¬ê·€ì ìœ¼ë¡œ ì „íŒŒí•˜ì—¬ ë…¸ë“œì˜ ì„ë² ë”©ì„ ê°œì„ í•˜ê³ , ì´ì›ƒì˜ ì¤‘ìš”ë„ë¥¼ êµ¬ë³„í•˜ê¸° ìœ„í•´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  ì €í¬ì˜ KGATëŠ” ê¸°ì¡´ KG ê¸°ë°˜ ì¶”ì²œ ë°©ë²•ë³´ë‹¤ ê°œë…ì ìœ¼ë¡œ ìœ ë¦¬í•˜ë©°, ê³ ì°¨ì› ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²½ë¡œë¥¼ í™œìš©í•˜ê±°ë‚˜ ì •ê·œí™”ë¡œ ì•”ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ì— ì˜ì¡´í•©ë‹ˆë‹¤.  ì„¸ ê°€ì§€ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ì— ë”°ë¥´ë©´ KGATëŠ” Neural FM ë° RippleNetê³¼ ê°™ì€ ìµœì²¨ë‹¨ ë°©ë²•ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.  ì¶”ê°€ ì—°êµ¬ëŠ” ê³ ì°¨ì› ê´€ê³„ ëª¨ë¸ë§ì— ëŒ€í•œ ì„ë² ë”© ì „íŒŒì˜ íš¨ëŠ¥ê³¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” í•´ì„ ê°€ëŠ¥ì„±ì— ëŒ€í•œ íš¨ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
      }
    },
    {
      "id": "559b0ff6-88d3-4b38-bc24-20c8722e9d16",
      "title": "Deep Delta Learning",
      "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Mengdi Wang",
        "Quanquan Gu"
      ],
      "abstract": "The effectiveness of deep residual networks hinges on the identity shortcut connection. While this mechanism alleviates the vanishing-gradient problem, it also has a strictly additive inductive bias on feature transformations, limiting the network's ability to model complex hidden state transitions. In this paper, we introduce \\textbf{Deep Delta Learning (DDL)}, which generalizes the shortcut from a fixed identity map to a learnable, state-dependent linear operator. The resulting Delta Operator is a rank-1 perturbation of the identity, $\\mathbf{A}(\\mathbf{X}) = \\mathbf{I}- Î²(\\mathbf{X})\\mathbf{k} (\\mathbf{X}) \\mathbf{k} (\\mathbf{X})^\\top$, parameterized by a unit direction $\\mathbf{k}(\\mathbf{X})$ and a scalar gate $Î²(\\mathbf{X})$. We provide a spectral analysis showing that $Î²(\\mathbf{X})$ continuously interpolates the shortcut between identity ($Î²=0$), orthogonal projection ($Î²=1$), and Householder reflection ($Î²=2$). Furthermore, we rewrite the residual update as a synchronized rank-1 delta write: $Î²$ scales both the removal of the current $\\mathbf{k}$-component and the injection of the new $\\mathbf{k}$-component. This unification enables explicit control of the shortcut spectrum along a data-dependent direction while retaining stable training behavior. Empirically, replacing Transformer residual additions with DDL improves validation loss and perplexity, as well as downstream evaluation accuracy on language modeling tasks, with larger gains in the expanded-state setting.",
      "year": 2026,
      "arxiv_id": "2601.00417",
      "arxiv_url": "https://arxiv.org/abs/2601.00417",
      "conference": null,
      "category": "ml",
      "tags": [
        {
          "id": "ccd1c3cc-7252-4826-a6e5-0c5748a63ef1",
          "name": "CTR"
        },
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2026-01-01",
      "created_at": "2026-01-30T17:01:16.738109",
      "updated_at": "2026-01-30T17:03:26.374854"
    },
    {
      "id": "cfdf54ff-64c8-4ba5-b705-87825d7a97df",
      "title": "Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation",
      "authors": [
        "Le Xiao",
        "Xiaolin Chen"
      ],
      "abstract": "News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. The experimental results show that the news summary generator is able to generate accurate and reliable news summaries with some generalization ability.",
      "year": 2023,
      "arxiv_id": "2307.02839",
      "arxiv_url": "https://arxiv.org/abs/2307.02839",
      "doi": null,
      "paper_url": null,
      "conference": null,
      "category": "nlp",
      "tags": [
        {
          "id": "5042e1cb-7e74-4fd0-8f81-f07d7cb24a9f",
          "name": "Deep Learning"
        },
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        }
      ],
      "published_at": "2023-07-06",
      "created_at": "2026-02-02T18:32:59.241228",
      "updated_at": "2026-02-02T18:32:59.241228"
    },
    {
      "id": "72278723-1cf4-44bf-85e3-7c05f9612d06",
      "title": "How AI Impacts Skill Formation",
      "authors": [
        "Judy Hanwen Shen",
        "Alex Tamkin"
      ],
      "abstract": "AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.",
      "year": 2026,
      "arxiv_id": "2601.20245",
      "arxiv_url": "https://arxiv.org/abs/2601.20245",
      "doi": null,
      "paper_url": null,
      "conference": null,
      "category": "other",
      "tags": [
        {
          "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
          "name": "LLM"
        }
      ],
      "published_at": "2026-01-28",
      "created_at": "2026-02-03T14:31:25.736856",
      "updated_at": "2026-02-03T17:48:42.012676",
      "translation": {
        "title": "ì œëª©: AIê°€ ê¸°ìˆ  ìŠµë“ì— ë¯¸ì¹˜ëŠ” ì˜í–¥",
        "abstract": "ì´ˆë¡: AI ì§€ì›ì€ íŠ¹íˆ ì´ˆë³´ ì‘ì—…ìì˜ ê²½ìš° ì „ë¬¸ ë¶„ì•¼ì—ì„œ ìƒë‹¹í•œ ìƒì‚°ì„± í–¥ìƒì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ AIë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê°ë…í•˜ëŠ” ë° í•„ìš”í•œ ê¸°ìˆ  ê°œë°œì— ëŒ€í•œ ì´ëŸ¬í•œ ì§€ì›ì˜ ì˜í–¥ì€ ì—¬ì „íˆ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤. AIì— ì˜ì¡´í•˜ì—¬ ìµìˆ™í•˜ì§€ ì•Šì€ ì‘ì—…ì„ ì™„ë£Œí•˜ëŠ” ì´ˆë³´ ì‘ì—…ìëŠ” ì´ ê³¼ì •ì—ì„œ ìì‹ ì˜ ê¸°ìˆ  ìŠµë“ì„ ì €í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” AIì˜ ë„ì›€ì„ ë°›ì•„ ìƒˆë¡œìš´ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ìˆ™ë‹¬ì„ ì–»ëŠ” ê°œë°œìê°€ ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ì§€ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, AI ì‚¬ìš©ì€ ê°œë…ì  ì´í•´, ì½”ë“œ ì½ê¸°, ë””ë²„ê¹… ëŠ¥ë ¥ì— ì†ìƒì„ ì£¼ê³  í‰ê· ì ìœ¼ë¡œ ìƒë‹¹í•œ íš¨ìœ¨ì„± í–¥ìƒì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì°¸ê°€ìë“¤ì´ ì½”ë”© ì‘ì—…ì„ ì™„ì „íˆ ìœ„ì„í•œ ê²½ìš° ìƒì‚°ì„±ì´ ì•½ê°„ í–¥ìƒë˜ì—ˆì§€ë§Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•™ìŠµì˜ ë¹„ìš©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì„¸ ê°€ì§€ê°€ AIì™€ì˜ ì¸ì§€ì  ì°¸ì—¬ë¥¼ í¬í•¨í•˜ëŠ” ì—¬ì„¯ ê°€ì§€ êµ¬ë³„ë˜ëŠ” AI ìƒí˜¸ ì‘ìš© íŒ¨í„´ì„ ì‹ë³„í–ˆìœ¼ë©°, ì°¸ê°€ìê°€ AI ì§€ì›ì„ ë°›ë”ë¼ë„ í•™ìŠµ ê²°ê³¼ê°€ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” AI ê°•í™”ëœ ìƒì‚°ì„±ì´ ì „ë¬¸ì„±ì„ ìœ„í•œ ë‹¨ì¶• ì½”ìŠ¤ê°€ ì•„ë‹ˆë©°, íŠ¹íˆ ì•ˆì „ ê´€ë ¨ ë¶„ì•¼ì—ì„œ ê¸°ìˆ  ìŠµë“ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ì›Œí¬í”Œë¡œìš°ì— AI ì§€ì›ì„ ì‹ ì¤‘í•˜ê²Œ ë„ì…í•´ì•¼ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤."
      }
    },
    {
      "id": "75e2f793-4eca-4dff-a15f-722fc453340d",
      "title": "Multi-Behavior Sequential Recommendation with Temporal Graph Transformer",
      "authors": [
        "Lianghao Xia",
        "Chao Huang",
        "Yong Xu",
        "Jian Pei"
      ],
      "abstract": "Modeling time-evolving preferences of users with their sequential item interactions, has attracted increasing attention in many online applications. Hence, sequential recommender systems have been developed to learn the dynamic user interests from the historical interactions for suggesting items. However, the interaction pattern encoding functions in most existing sequential recommender systems have focused on single type of user-item interactions. In many real-life online platforms, user-item interactive behaviors are often multi-typed (e.g., click, add-to-favorite, purchase) with complex cross-type behavior inter-dependencies. Learning from informative representations of users and items based on their multi-typed interaction data, is of great importance to accurately characterize the time-evolving user preference. In this work, we tackle the dynamic user-item relation learning with the awareness of multi-behavior interactive patterns. Towards this end, we propose a new Temporal Graph Transformer (TGT) recommendation framework to jointly capture dynamic short-term and long-range user-item interactive patterns, by exploring the evolving correlations across different types of behaviors. The new TGT method endows the sequential recommendation architecture to distill dedicated knowledge for type-specific behavior relational context and the implicit behavior dependencies. Experiments on the real-world datasets indicate that our method TGT consistently outperforms various state-of-the-art recommendation methods. Our model implementation codes are available at https://github.com/akaxlh/TGT.",
      "year": 2022,
      "arxiv_id": "2206.02687",
      "arxiv_url": "https://arxiv.org/abs/2206.02687",
      "conference": "IEEE Trans Knowl Data Eng'22",
      "category": "recsys",
      "tags": [
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2022-06-06",
      "created_at": "2026-02-03T17:48:25.509188",
      "updated_at": "2026-02-04T16:28:13.993014",
      "translation": {
        "title": "ì œëª©: ë‹¤ì¤‘ í–‰ë™ ìˆœì°¨ ì¶”ì²œì„ ìœ„í•œ ì‹œê°„ì  ê·¸ë˜í”„ íŠ¸ëœìŠ¤í¬ë¨¸",
        "abstract": "ì´ˆë¡: ì‚¬ìš©ìì˜ ì‹œê°„ ë³€í™”í•˜ëŠ” ì„ í˜¸ë„ë¥¼ ê·¸ë“¤ì˜ ìˆœì°¨ì ì¸ ì•„ì´í…œ ìƒí˜¸ì‘ìš©ê³¼ í•¨ê»˜ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì€ ë§ì€ ì˜¨ë¼ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì ì  ë” ë§ì€ ê´€ì‹¬ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì‚¬ìš©ìì˜ ê³¼ê±° ìƒí˜¸ì‘ìš©ìœ¼ë¡œë¶€í„° ë™ì  ì‚¬ìš©ì ê´€ì‹¬ì„ í•™ìŠµí•˜ì—¬ ì•„ì´í…œì„ ì œì•ˆí•˜ëŠ” ìˆœì°¨ ì¶”ì²œ ì‹œìŠ¤í…œì´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ìˆœì°¨ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ìƒí˜¸ì‘ìš© íŒ¨í„´ ì¸ì½”ë”© í•¨ìˆ˜ëŠ” ë‹¨ì¼ ìœ í˜•ì˜ ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš©ì— ì´ˆì ì„ ë§ì¶”ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì˜¨ë¼ì¸ í”Œë«í¼ì—ì„œëŠ” ì‚¬ìš©ìì™€ ì•„ì´í…œ ê°„ì˜ ìƒí˜¸ì‘ìš© í–‰ë™ì´ ì¢…ì¢… ì—¬ëŸ¬ ìœ í˜•(ì˜ˆ: í´ë¦­, ì¥ë°”êµ¬ë‹ˆì— ì¶”ê°€, êµ¬ë§¤)ìœ¼ë¡œ ë³µì¡í•œ êµì°¨ ìœ í˜• í–‰ë™ ìƒí˜¸ ì˜ì¡´ì„±ì„ ê°–ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ ë‹¤ì¤‘ ìœ í˜• ìƒí˜¸ ì‘ìš© ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ìµí•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ì‹œê°„ ë³€í™”í•˜ëŠ” ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ì •í™•í•˜ê²Œ íŠ¹ì„±í™”í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‹¤ì¤‘ í–‰ë™ ìƒí˜¸ ì‘ìš© íŒ¨í„´ì— ëŒ€í•œ ì¸ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì-ì•„ì´í…œ ê´€ê³„ í•™ìŠµì„ í•´ê²°í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ í–‰ë™ì„ í†µí•´ ë™ì  ë‹¨ê¸° ë° ì¥ê±°ë¦¬ ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ ì‘ìš© íŒ¨í„´ì„ í•¨ê»˜ í¬ì°©í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì‹œê°„ì  ê·¸ë˜í”„ íŠ¸ëœìŠ¤í¬ë¨¸(TGT) ì¶”ì²œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìƒˆë¡œìš´ TGT ë°©ë²•ì€ ìˆœì°¨ ì¶”ì²œ ì•„í‚¤í…ì²˜ê°€ ìœ í˜•ë³„ í–‰ë™ ê´€ê³„ ì»¨í…ìŠ¤íŠ¸ ë° ì•”ë¬µì ì¸ í–‰ë™ ì˜ì¡´ì„±ì— ëŒ€í•œ ì „ë‹´ ì§€ì‹ì„ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼, ìš°ë¦¬ì˜ ë°©ë²•ì¸ TGTê°€ ë‹¤ì–‘í•œ ìµœì²¨ë‹¨ ì¶”ì²œ ë°©ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ëª¨ë¸ êµ¬í˜„ ì½”ë“œëŠ” https://github.com/akaxlh/TGT ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      }
    },
    {
      "id": "63b8e5a5-3f2b-4809-ac6d-a47f6e5252d6",
      "title": "A Survey on Multi-Behavior Sequential Recommendation",
      "authors": [
        "Xiaoqing Chen",
        "Zhitao Li",
        "Weike Pan",
        "Zhong Ming"
      ],
      "abstract": "Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.",
      "year": 2023,
      "arxiv_id": "2308.15701",
      "arxiv_url": "https://arxiv.org/abs/2308.15701",
      "conference": null,
      "category": "recsys",
      "tags": [
        {
          "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
          "name": "GCN"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2023-08-30",
      "created_at": "2026-02-04T14:55:20.499680",
      "updated_at": "2026-02-04T14:55:20.499680"
    },
    {
      "id": "14d9d150-68bd-44da-99ae-220683dbd56c",
      "title": "Multi-Behavior Generative Recommendation",
      "authors": [
        "Zihan Liu",
        "Yupeng Hou",
        "Julian McAuley"
      ],
      "abstract": "Multi-behavior sequential recommendation (MBSR) aims to incorporate behavior types of interactions for better recommendations. Existing approaches focus on the next-item prediction objective, neglecting the value of integrating the target behavior type into the learning objective. In this paper, we propose MBGen, a novel Multi-Behavior sequential Generative recommendation framework. We formulate the MBSR task into a consecutive two-step process: (1) given item sequences, MBGen first predicts the next behavior type to frame the user intention, (2) given item sequences and a target behavior type, MBGen then predicts the next items. To model such a two-step process, we tokenize both behaviors and items into tokens and construct one single token sequence with both behaviors and items placed interleaved. Furthermore, MBGen learns to autoregressively generate the next behavior and item tokens in a unified generative recommendation paradigm, naturally enabling a multi-task capability. Additionally, we exploit the heterogeneous nature of token sequences in the generative recommendation and propose a position-routed sparse architecture to efficiently and effectively scale up models. Extensive experiments on public datasets demonstrate that MBGen significantly outperforms existing MBSR models across multiple tasks.",
      "year": 2024,
      "arxiv_id": "2405.16871",
      "arxiv_url": "https://arxiv.org/abs/2405.16871",
      "conference": "CIKM'24",
      "category": "recsys",
      "tags": [
        {
          "id": "e433e5e6-273e-40bd-99cc-a5811a26496f",
          "name": "Recommendation"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        }
      ],
      "published_at": "2024-05-27",
      "created_at": "2026-02-04T15:45:44.609131",
      "updated_at": "2026-02-04T16:28:17.196022",
      "full_summary": "**ì—°êµ¬ ë°°ê²½**\n\nê¸°ì¡´ì˜ ì¶”ì²œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ìœ í˜•(í´ë¦­, êµ¬ë§¤, ì¥ë°”êµ¬ë‹ˆ ì¶”ê°€ ë“±)ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ëª»í•˜ê³ , ì£¼ë¡œ ë‹¤ìŒ í•­ëª© ì˜ˆì¸¡ì— ì§‘ì¤‘í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, Multi-Behavior Sequential Recommendation (MBSR)ëŠ” ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ìœ í˜•ì„ í†µí•©í•˜ì—¬ ì¶”ì²œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ì§€ë§Œ, ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ì˜ˆì¸¡ ëª©í‘œì— ì§‘ì¤‘í•˜ì—¬ ìƒí˜¸ì‘ìš© ìœ í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ìƒí˜¸ì‘ìš© ìœ í˜•ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ ìì²´ì˜ ê°€ì¹˜(ì˜ˆ: ì‚¬ìš©ìì˜ ì˜ë„ íŒŒì•…, ë§ì¶¤í˜• ê¸°ëŠ¥ ì œê³µ)ë¥¼ ê°„ê³¼í–ˆìŠµë‹ˆë‹¤.\n\n**í•µì‹¬ ê¸°ì—¬**\n\n- ë‘ ë‹¨ê³„ì˜ ì—°ì†ì ì¸ ëª¨ë¸ë§: MBSR ì‘ì—…ì„ ë‘ ë‹¨ê³„ì˜ ì—°ì†ì ì¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë¶„í•´í•˜ì—¬ ëª¨ë¸ë§í•¨ìœ¼ë¡œì¨, ìƒí˜¸ì‘ìš© ìœ í˜•ì„ ì˜ˆì¸¡í•˜ê³  ë‹¤ìŒ í•­ëª©ì„ ì˜ˆì¸¡í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.\n- ë°ì´í„° ì¤‘ì‹¬ì˜ ìƒì„±ì  ëª¨ë¸: ìƒí˜¸ì‘ìš© ìœ í˜•ê³¼ í•­ëª©ì„ í† í°í™”í•˜ì—¬ í•˜ë‚˜ì˜ í† í° ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ìƒì„±ì  ì¶”ì²œ ëª¨ë¸ì„ í†µí•´ ë‹¤ìŒ ìƒí˜¸ì‘ìš© ìœ í˜•ê³¼ í•­ëª©ì„ ìë™ íšŒê·€ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë°ì´í„° ì¤‘ì‹¬ì˜ ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤.\n- íš¨ìœ¨ì ì¸ ëª¨ë¸ í™•ì¥:  ìƒí˜¸ì‘ìš© ìœ í˜•ì˜ ë¶ˆê· í˜•í•œ í•´ê²° ë°©ì•ˆìœ¼ë¡œ, í•­ëª© í† í°ì„ ì‚¬ìš©í•˜ì—¬ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ê³ , ìœ„ì¹˜ ê¸°ë°˜ì˜ í¬ì†Œ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n\n**ë°©ë²•ë¡ **\n\nMBGen ëª¨ë¸ì€ ë‹¤ìŒì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì²«ì§¸, MBSR ì‘ì—…ì„ ë‘ ë‹¨ê³„ì˜ ì—°ì†ì ì¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë¶„í•´í•©ë‹ˆë‹¤. (1) ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ìœ í˜•ì„ ì˜ˆì¸¡í•˜ê³ , (2) ë‘ ë²ˆì§¸ ë‹¨ê³„ëŠ” ì˜ˆì¸¡ëœ ìƒí˜¸ì‘ìš© ìœ í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.  ë‘ ë²ˆì§¸, ìƒí˜¸ì‘ìš© ìœ í˜•ê³¼ í•­ëª©ì„ í† í°í™”í•˜ì—¬ í•˜ë‚˜ì˜ í† í° ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.  ì´ ì‹œí€€ìŠ¤ëŠ” í•­ëª© í† í°ê³¼ ìƒí˜¸ì‘ìš© ìœ í˜• í† í°ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.  ì„¸ ë²ˆì§¸,  ìœ„ì¹˜ ê¸°ë°˜ì˜ í¬ì†Œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤.  ì´ ë„¤íŠ¸ì›Œí¬ëŠ” ê° ì…ë ¥ì´ ë‹¤ë¥¸ ì „ë¬¸ê°€ ë„¤íŠ¸ì›Œí¬ë¡œ ë¼ìš°íŒ…ë˜ë„ë¡ ì„¤ê³„ë˜ì–´ ëª¨ë¸ì˜ í™•ì¥ì„±ì„ ë†’ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.  ëª¨ë¸ì€ í•™ìŠµ ëª©í‘œë¥¼ ìœ„í•´ ìƒì„±ì  ì¶”ì²œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ë‹¤ìŒ ìƒí˜¸ì‘ìš© ìœ í˜•ê³¼ í•­ëª©ì„ ìë™ íšŒê·€ì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.  ëª¨ë¸ì€ í•™ìŠµ ëª©í‘œë¥¼ ìœ„í•´ ìƒì„±ì  ì¶”ì²œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ë‹¤ìŒ ìƒí˜¸ì‘ìš© ìœ í˜•ê³¼ í•­ëª©ì„ ìë™ íšŒê·€ì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n\nê´€ë ¨ ì—°êµ¬\n\nê¸°ì¡´ì˜ ì¶”ì²œ ì‹œìŠ¤í…œ, ìƒì„±ì  ì¶”ì²œ ëª¨ë¸, Multi-Behavior Recommendation ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬, MBGen ëª¨ë¸ì´ ìƒí˜¸ì‘ìš© ìœ í˜•ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ìˆì–´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ë” ë¯¸ì„¸í•œ ìˆ˜ì¤€ì—ì„œ ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ ìº¡ì²˜í•˜ê³ , Multi-Task Capabilityë¥¼ ì œê³µí•œë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. íŠ¹íˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ì´ ìƒí˜¸ì‘ìš© ìœ í˜•ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ìˆì–´ ë¯¸ì„¸í•œ ìˆ˜ì¤€ì—ì„œ ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ ìº¡ì²˜í•˜ê³ , Multi-Task Capabilityë¥¼ ì œê³µí•œë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤."
    },
    {
      "id": "e2a563b0-fc93-42a3-9e19-aec38a08670b",
      "title": "Tail-Aware Data Augmentation for Long-Tail Sequential Recommendation",
      "authors": [
        "Yizhou Dang",
        "Zhifu Wei",
        "Minhan Huang",
        "Lianbo Ma",
        "Jianzhe Zhao",
        "Guibing Guo",
        "Xingwei Wang"
      ],
      "abstract": "Sequential recommendation (SR) learns user preferences based on their historical interaction sequences and provides personalized suggestions. In real-world scenarios, most users can only interact with a handful of items, while the majority of items are seldom consumed. This pervasive long-tail challenge limits the model's ability to learn user preferences. Despite previous efforts to enrich tail items/users with knowledge from head parts or improve tail learning through additional contextual information, they still face the following issues: 1) They struggle to improve the situation where interactions of tail users/items are scarce, leading to incomplete preferences learning for the tail parts. 2) Existing methods often degrade overall or head parts performance when improving accuracy for tail users/items, thereby harming the user experience. We propose Tail-Aware Data Augmentation (TADA) for long-tail sequential recommendation, which enhances the interaction frequency for tail items/users while maintaining head performance, thereby promoting the model's learning capabilities for the tail. Specifically, we first capture the co-occurrence and correlation among low-popularity items by a linear model. Building upon this, we design two tail-aware augmentation operators, T-Substitute and T-Insert. The former replaces the head item with a relevant item, while the latter utilizes co-occurrence relationships to extend the original sequence by incorporating both head and tail items. The augmented and original sequences are mixed at the representation level to preserve preference knowledge. We further extend the mix operation across different tail-user sequences and augmented sequences to generate richer augmented samples, thereby improving tail performance. Comprehensive experiments demonstrate the superiority of our method. The codes are provided at https://github.com/KingGugu/TADA.",
      "year": 2026,
      "arxiv_id": "2601.10933",
      "arxiv_url": "https://arxiv.org/abs/2601.10933",
      "conference": null,
      "category": "recsys",
      "tags": [
        {
          "id": "e433e5e6-273e-40bd-99cc-a5811a26496f",
          "name": "Recommendation"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        }
      ],
      "published_at": "2026-01-16",
      "created_at": "2026-02-06T13:56:24.855831",
      "updated_at": "2026-02-06T13:56:24.855831"
    },
    {
      "id": "56676162-fc40-423d-bc8a-7ddb3fac7294",
      "title": "Meta Graph Learning for Long-tail Recommendation",
      "authors": [
        "Chunyu Wei",
        "Jian Liang",
        "Di Liu",
        "Zehui Dai",
        "Man-Chun Li",
        "Fei Wang"
      ],
      "abstract": "Highly skewed long-tail item distribution commonly hurts model performance on tail items in recommendation systems, especially for graph-based recommendation models. We propose a novel idea to learn relations among items as an auxiliary graph to enhance the graph-based representation learning and make recommendations collectively in a coupled framework. This raises two challenges, 1) the long-tail downstream information may also bias the auxiliary graph learning, and 2) the learned auxiliary graph may cause negative transfer to the original user-item bipartite graph. We innovatively propose a novel Meta Graph Learning framework for long-tail recommendation (MGL) for solving both challenges. The meta-learning strategy is introduced to the learning of an edge generator, which is first tuned to reconstruct a debiased item co-occurrence matrix, and then virtually evaluated on generating item relations for recommendation. Moreover, we propose a popularity-aware contrastive learning strategy to prevent negative transfer by aligning the confident head item representations with those of the learned auxiliary graph. Experiments on public datasets demonstrate that our proposed model significantly outperforms strong baselines for tail items without compromising the overall performance.",
      "year": 2023,
      "arxiv_id": null,
      "arxiv_url": null,
      "doi": "10.1145/3580305.3599428",
      "paper_url": "https://www.semanticscholar.org/paper/8d768f804f9c16e7ac8dc1afb2e2747fcbe8e7a2",
      "conference": "KDD'23",
      "category": "recsys",
      "tags": [
        {
          "id": "5042e1cb-7e74-4fd0-8f81-f07d7cb24a9f",
          "name": "Deep Learning"
        },
        {
          "id": "e433e5e6-273e-40bd-99cc-a5811a26496f",
          "name": "Recommendation"
        }
      ],
      "published_at": "2023-08-04",
      "created_at": "2026-02-09T11:18:01.830280",
      "updated_at": "2026-02-09T11:18:01.830280"
    },
    {
      "id": "9fc78cb9-ea06-4d7c-a0ee-0edb23b59a25",
      "title": "A Generic Behavior-Aware Data Augmentation Framework for Sequential Recommendation",
      "authors": [
        "Jing Xiao",
        "Weike Pan",
        "Zhong Ming"
      ],
      "abstract": "Multi-behavior sequential recommendation (MBSR), which models multi-behavior sequentiality and heterogeneity to better learn users' multifaceted intentions has achieved remarkable success. Though effective, the performance of these approaches may be limited due to the sparsity inherent in a real-world data. Existing data augmentation methods in recommender systems focus solely on a single type of behavior, overlooking the variations in expressing user preferences via different types of behaviors. During the augmentation of samples, it is easy to introduce excessive disturbance or noise, which may mislead the next-item recommendation. To address this limitation, we propose a novel generic framework called multi-behavior data augmentation for sequential recommendation (MBASR). Specifically, we design three behavior-aware data augmentation operations to construct rich training samples. Each augmentation operation takes into account the correlations between behaviors and aligns with the users' behavior patterns. In addition, we introduce a position-based sampling strategy that can effectively reduce the perturbation brought by the augmentation operations to the original data. Note that our model is data-oriented and can thus be embedded in different downstream MBSR models, so the overall framework is generic. Extensive experiments on three real-world datasets demonstrate the effectiveness of our MBASR and its applicability to a wide variety of mainstream MBSR models. Our source code is available at https://github.com/XiaoJing-C/MBASR.",
      "year": 2024,
      "arxiv_id": null,
      "arxiv_url": null,
      "doi": "10.1145/3626772.3657682",
      "paper_url": "https://www.semanticscholar.org/paper/f0a62b7b5f7cb9f1c262786c495d107de21650e0",
      "conference": "SIGIR'24",
      "category": "recsys",
      "tags": [
        {
          "id": "e433e5e6-273e-40bd-99cc-a5811a26496f",
          "name": "Recommendation"
        },
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        }
      ],
      "published_at": "2024-07-10",
      "created_at": "2026-02-09T12:07:30.099328",
      "updated_at": "2026-02-09T12:07:30.099328"
    },
    {
      "id": "046540b7-296c-4f2b-94e2-42fecf899193",
      "title": "Recommender Systems with Generative Retrieval",
      "authors": [
        "Shashank Rajput",
        "Nikhil Mehta",
        "Anima Singh",
        "Raghunandan H. Keshavan",
        "Trung Vu",
        "Lukasz Heldt",
        "Lichan Hong",
        "Yi Tay",
        "Vinh Q. Tran",
        "Jonah Samost",
        "Maciej Kula",
        "Ed H. Chi",
        "Maheswaran Sathiamoorthy"
      ],
      "abstract": "Modern recommender systems perform large-scale retrieval by first embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. To the best of our knowledge, this is the first Semantic ID-based generative model for recommendation tasks. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history.",
      "year": 2023,
      "arxiv_id": "2305.05065",
      "arxiv_url": "https://arxiv.org/abs/2305.05065",
      "conference": "NIPS'23",
      "category": "recsys",
      "tags": [
        {
          "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
          "name": "Sequential"
        },
        {
          "id": "302859db-9e76-45a8-b984-61f43d889c96",
          "name": "Transformer"
        }
      ],
      "published_at": "2023-05-08",
      "created_at": "2026-02-10T10:33:15.639267",
      "updated_at": "2026-02-10T10:36:44.745925",
      "full_summary": "**ì—°êµ¬ ë°°ê²½**\n\nìµœê·¼ ì¶”ì²œ ì‹œìŠ¤í…œì€ ì¿¼ë¦¬ì™€ í›„ë³´ ì•„ì´í…œì„ ê³µí†µëœ ê³µê°„ì— ì„ë² ë”©í•˜ì—¬ ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰ì„ í†µí•´ ìƒìœ„ í›„ë³´ë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëŒ€ê·œëª¨ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ì´ëŸ¬í•œ ê¸°ì¡´ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ìƒì„±ì  ê²€ìƒ‰ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ëŒ€ìƒ í›„ë³´ì˜ ì‹ë³„ìë¥¼ ìë™ìœ¼ë¡œ ë””ì½”ë”©í•˜ì—¬ ì¶”ì²œ ëª¨ë¸ì´ í›„ë³´ ì•„ì´í…œì„ ì˜ˆì¸¡í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n\n**í•µì‹¬ ê¸°ì—¬**\n\n- Transformer ê¸°ë°˜ì˜ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì„¸ì…˜ì˜ Semantic IDë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ì¸ TIGERë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.\n- Semantic IDë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í†µí•´ ê¸°ì¡´ì˜ ì›ì‹œ ì•„ì´í…œ IDë³´ë‹¤ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , íŠ¹íˆ ìƒí˜¸ ì‘ìš© ì´ë ¥ì´ ì—†ëŠ” ìƒˆë¡œìš´ ì•„ì´í…œì— ëŒ€í•œ ì¶”ì²œ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.\n- TIGER í”„ë ˆì„ì›Œí¬ê°€ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ ëª¨ë¸(SOTA)ë³´ë‹¤ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ë­í‚¹ ì •í™•ë„(Recall, NDCG) ì¸¡ë©´ì—ì„œ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.\n\n**ë°©ë²•ë¡ **\n\nì´ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë¡ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¨¼ì €, ê° ì•„ì´í…œì˜ ì½˜í…ì¸  íŠ¹ì§•ì„ ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ì½˜í…ì¸  ì¸ì½”ë”ë¥¼ í†µí•´ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ ì„ë² ë”© ë²¡í„°ëŠ” Residual Quantized Variational Autoencoder (RQ-V AE)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¶ì— ë§¤í•‘ë˜ì–´ Semantic IDë¼ëŠ” ì½”ë“œ ë²¡í„° íŠœí”Œë¡œ ìƒì„±ë©ë‹ˆë‹¤. ìƒì„±ëœ Semantic ID íŠœí”Œì€ Transformer ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, ì´ ëª¨ë¸ì€ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ì‚¬ìš©ì ì„¸ì…˜ì˜ Semantic ID ì‹œí€€ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ Semantic IDë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.  íŠ¹íˆ, RQ-V AEëŠ” ë‹¤ë‹¨ê³„ ëŸ‰ìí™” ê³¼ì •ì„ í†µí•´ ì•„ì´í…œì˜ ì˜ë¯¸ì  íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•˜ê³ , ì´ë¥¼ í†µí•´ ìœ ì‚¬í•œ ì•„ì´í…œ ê°„ì˜ Semantic IDê°€ ê²¹ì¹˜ëŠ” í˜„ìƒì„ ìœ ë„í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n\nìš”ì•½:"
    }
  ],
  "tags": [
    {
      "id": "86afe8f0-5c9f-4b1b-bda9-ae95aa1fa5be",
      "name": "LLM"
    },
    {
      "id": "302859db-9e76-45a8-b984-61f43d889c96",
      "name": "Transformer"
    },
    {
      "id": "e3f8c350-ed68-44e0-ba52-f32be9e17659",
      "name": "Industrial"
    },
    {
      "id": "3aaf48f3-7ba1-4a61-98f0-8baaad8024ed",
      "name": "Sequential"
    },
    {
      "id": "3e36ba36-2a7d-4cb5-9b57-83f4e1cee79a",
      "name": "VAE"
    },
    {
      "id": "1c8b90dd-a65d-420b-9bf0-527b53b70e23",
      "name": "GCN"
    },
    {
      "id": "ccd1c3cc-7252-4826-a6e5-0c5748a63ef1",
      "name": "CTR"
    },
    {
      "id": "5042e1cb-7e74-4fd0-8f81-f07d7cb24a9f",
      "name": "Deep Learning"
    },
    {
      "id": "e433e5e6-273e-40bd-99cc-a5811a26496f",
      "name": "Recommendation"
    },
    {
      "id": "391a99c7-5568-4fa6-b6ff-da32e60c44c9",
      "name": "Graph"
    }
  ]
}